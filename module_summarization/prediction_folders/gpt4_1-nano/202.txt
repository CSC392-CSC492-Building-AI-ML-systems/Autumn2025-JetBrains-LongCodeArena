Permutation Feature Importance
==============================

Permutation feature importance is a model-agnostic method to evaluate the importance of features in a predictive model. It measures the increase in the model's prediction error after permuting the feature's values, which breaks the relationship between the feature and the target. The underlying assumption is that if a feature is important, shuffling its values will significantly decrease the model's performance.

How It Works
------------

1. **Baseline Score**: Compute the model's performance on the original dataset.
2. **Permutation**: For each feature, shuffle its values across all samples, effectively destroying any relationship with the target.
3. **Re-evaluate**: Measure the model's performance with the permuted feature.
4. **Importance Calculation**: The importance of a feature is quantified as the difference between the baseline score and the score after permutation. Larger differences indicate more important features.

Implementation Details
----------------------

- The method supports multiple repeats to obtain a distribution of importance scores, providing a measure of variability.
- It is compatible with various estimators, including those from ensemble methods.
- Supports parallel computation to speed up the process.
- Handles datasets with different formats, including NumPy arrays and pandas DataFrames.

Usage
-----

To compute permutation feature importance, provide a trained estimator, the dataset, and optionally specify the scoring metric, number of repeats, and other parameters:

```python
from sklearn.inspection import permutation_importance

result = permutation_importance(
    estimator,
    X,
    y,
    scoring='accuracy',
    n_repeats=10,
    n_jobs=-1,
    random_state=42
)
```

Output
------

The result is a `Bunch` object containing:

- `importances_mean`: The mean importance scores across repeats for each feature.
- `importances_std`: The standard deviation of importance scores across repeats.
- `importances`: The raw importance scores for each feature and each repeat.

Interpretation
--------------

Features with higher importance scores are more influential in the model's predictions. The standard deviation provides insight into the stability of the importance estimate across repeats. This method helps identify which features contribute most to the predictive performance and can guide feature selection and model interpretation.

Limitations
-----------

- Permutation importance can be biased if features are correlated.
- It assumes the model's performance metric is appropriate for the task.
- Computationally intensive for large datasets or complex models, especially with many repeats.

References
----------

- Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.
- Altmann, A., et al. (2010). Permutation importance: a corrected feature importance measure. Bioinformatics, 26(10), 1340-1347.

For more details, see the official scikit-learn documentation on permutation importance.