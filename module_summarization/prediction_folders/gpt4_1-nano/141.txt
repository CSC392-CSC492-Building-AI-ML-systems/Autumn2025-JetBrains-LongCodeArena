Connect to Remote Data
========================

The library provides robust functionality to connect to and read data from remote storage systems such as S3, HDFS, and other cloud or distributed filesystems. This enables efficient processing of large datasets stored remotely without the need to download entire files locally.

Reading Bytes from Remote Files
------------------------------
The primary function for accessing remote data is `read_bytes`. This function allows you to read data from one or multiple files specified by a URL path or glob pattern. It supports various protocols, including `s3://`, `hdfs://`, and others, provided the necessary libraries are installed.

Features:
- Supports reading from multiple files using glob patterns or lists of paths.
- Handles different storage protocols seamlessly.
- Breaks data into manageable blocks based on specified block size.
- Supports reading compressed files with compatible compression formats.
- Can include file paths with the data blocks for easier identification.
- Provides an initial sample of the data for inspection.

Usage:
```python
sample, blocks = read_bytes('s3://bucket/data-*.csv', delimiter=b'\n')
```

Parameters:
- `urlpath`: Path or list of paths to the remote files.
- `delimiter`: Byte sequence to split data into blocks (e.g., newline).
- `not_zero`: Skip the header by seeking after the start of the file.
- `blocksize`: Size of each data block (e.g., "128 MiB").
- `compression`: Compression format if files are compressed (e.g., 'gzip', 'xz').
- `sample`: Size of the header sample to read.
- `include_path`: Whether to include file paths with data blocks.
- `kwargs`: Additional storage-specific options such as host, port, credentials.

Example:
```python
sample, blocks, paths = read_bytes('hdfs://namenode:8020/data/*.csv', delimiter=b'\n', include_path=True)
```

Reading Data Blocks
-------------------
The function `read_block_from_file` is used internally to read specific blocks of data from remote files, handling offsets and block sizes efficiently. It ensures that large files are read in chunks, optimizing memory usage and performance.

Compression Support
-------------------
The library includes utilities for handling compressed data, such as `zip_compress`, which writes data into a zip archive and returns the bytes. Support for other compression formats like gzip, bz2, and lzma is integrated, allowing seamless reading of compressed remote files.

Summary
-------
Connecting to remote data sources is streamlined with functions that handle various protocols, compression formats, and large datasets efficiently. This setup enables scalable data processing workflows directly from remote storage systems, facilitating big data analytics and distributed computing tasks.