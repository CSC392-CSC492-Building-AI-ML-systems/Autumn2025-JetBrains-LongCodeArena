Support Vector Machines (SVMs)
=============================

Introduction
------------
Support Vector Machines (SVMs) are powerful supervised learning models used for classification and regression tasks. They aim to find the optimal hyperplane that separates classes with the maximum margin, providing robust performance even in high-dimensional spaces.

Key Concepts
------------
- **Hyperplane**: A decision boundary that separates different classes.
- **Support Vectors**: Data points that are closest to the hyperplane and influence its position and orientation.
- **Margin**: The distance between the hyperplane and the support vectors; SVMs maximize this margin to improve generalization.

Types of SVMs
------------
- **Linear SVMs**: Suitable for linearly separable data.
- **Kernel SVMs**: Use kernel functions to handle non-linear data by implicitly mapping inputs into higher-dimensional spaces.

Implementation Details
----------------------
SVMs can be implemented using various libraries. In this context, the implementation leverages the liblinear library for linear SVMs, which offers flexibility in penalty and loss functions and scales efficiently to large datasets.

Linear Support Vector Classifier
-------------------------------
The `LinearSVC` class provides a linear support vector classifier with the following features:

- **Penalty**: Supports `'l1'` and `'l2'` norms for regularization.
- **Loss Function**: Supports `'hinge'` and `'squared_hinge'`.
- **Dual Formulation**: Automatically selects between primal and dual optimization based on data characteristics (`dual='auto'`).
- **Multiclass Strategy**: Supports `'ovr'` (one-vs-rest) and `'crammer_singer'`.
- **Intercept Handling**: Optionally fits an intercept term with `fit_intercept`.
- **Regularization Parameter**: Controlled via `C`, where larger values specify less regularization.
- **Tolerance**: `tol` parameter defines the stopping criterion for optimization.

Usage
-----
To use the `LinearSVC`, instantiate the class with desired parameters and fit it to your data:

```python
from sklearn.svm import LinearSVC

clf = LinearSVC(penalty='l2', loss='squared_hinge', dual='auto', C=1.0, tol=1e-4)
clf.fit(X, y)
```

The trained model can then be used to predict class labels for new data points.

Additional Notes
----------------
- The `dual` parameter defaults to `'auto'`, which chooses the optimization formulation based on the dataset size.
- The `penalty` and `loss` parameters should be chosen based on the problem specifics and data characteristics.
- For multiclass classification, `LinearSVC` employs a one-vs-rest scheme by default.

References
----------
For more detailed information, refer to the [scikit-learn user guide on SVMs](https://scikit-learn.org/stable/modules/svm.html).