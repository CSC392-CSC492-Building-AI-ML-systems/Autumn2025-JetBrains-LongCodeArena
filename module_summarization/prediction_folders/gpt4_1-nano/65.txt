Calibration of Predicted Probabilities
========================================

Introduction
------------
Probability calibration is a crucial step in many machine learning applications where the predicted probabilities need to reflect true likelihoods accurately. Proper calibration ensures that, for example, among all instances assigned a probability of 0.8, approximately 80% are positive cases. This tutorial provides an overview of probability calibration techniques, focusing on methods implemented in scikit-learn, such as isotonic regression and Platt scaling.

What is Probability Calibration?
-------------------------------
Probability calibration adjusts the output probabilities of a classifier to better match the true likelihood of events. Raw classifier scores or decision function outputs often do not represent true probabilities, especially in models like Support Vector Machines. Calibration methods transform these scores into well-calibrated probabilities, improving decision-making processes that depend on probability estimates.

Calibration Methods
-------------------
1. Sigmoid Calibration (Platt Scaling)
   - Uses logistic regression to fit a sigmoid function to the classifier scores.
   - Suitable for models where the decision function is well-behaved.
   - Implemented via the `'sigmoid'` method.

2. Isotonic Regression
   - A non-parametric approach that fits a monotonically increasing function.
   - Can model more complex calibration curves.
   - Suitable when sufficient calibration data is available (recommended >1000 samples).
   - Implemented via the `'isotonic'` method.

Using CalibratedClassifierCV
----------------------------
The `CalibratedClassifierCV` class in scikit-learn provides an easy-to-use interface for probability calibration with cross-validation. It can calibrate any classifier that provides a `decision_function` or `predict_proba` method.

Key Parameters:
- `estimator`: The base classifier to calibrate.
- `method`: Calibration method (`'sigmoid'` or `'isotonic'`).
- `cv`: Cross-validation strategy or "prefit" if the estimator is already fitted.
- `ensemble`: If `True`, combines predictions from multiple calibrated classifiers for improved stability.
- `n_jobs`: Number of parallel jobs for fitting.

Workflow:
1. Instantiate `CalibratedClassifierCV` with desired parameters.
2. Fit the calibrator on training data.
3. Use the calibrated classifier to predict probabilities with improved calibration.

Example:
```python
from sklearn.svm import SVC
from sklearn.calibration import CalibratedClassifierCV

# Initialize base classifier
svm_clf = SVC(probability=True)

# Initialize calibration with isotonic method
calibrator = CalibratedClassifierCV(svm_clf, method='isotonic', cv=5)

# Fit on training data
calibrator.fit(X_train, y_train)

# Predict calibrated probabilities
probabilities = calibrator.predict_proba(X_test)
```

Considerations
--------------
- Isotonic regression may overfit with small calibration datasets; use with caution when data is limited.
- Sigmoid calibration is generally more robust with smaller datasets.
- Calibration is most effective when the classifier's raw probabilities are poorly calibrated.

Summary
-------
Probability calibration enhances the interpretability and reliability of predicted probabilities. By choosing appropriate methods and parameters, practitioners can significantly improve the quality of probabilistic outputs, leading to better decision-making in classification tasks.

Further Reading
--------------
- scikit-learn documentation on [calibration](https://scikit-learn.org/stable/modules/calibration.html)
- Platt, J. (1999). Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.
- Niculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning.

This concludes the tutorial on probability calibration.