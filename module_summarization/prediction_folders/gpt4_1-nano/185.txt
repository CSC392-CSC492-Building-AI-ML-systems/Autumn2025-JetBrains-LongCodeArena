Tuning the Hyper-parameters of an Estimator
============================================

Introduction
------------
Hyper-parameter tuning is a crucial step in optimizing the performance of machine learning models. It involves searching for the best combination of hyper-parameters that yields the highest model accuracy or other relevant metrics. Scikit-learn provides tools such as `GridSearchCV` and `RandomizedSearchCV` to facilitate systematic hyper-parameter tuning.

Grid Search
-----------
Grid Search exhaustively considers all parameter combinations in a specified grid. It is useful when the hyper-parameter space is small and a thorough search is feasible.

Example:
```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    estimator=RandomForestClassifier(),
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,
    verbose=1
)

grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print("Best hyper-parameters:", best_params)
```

Randomized Search
-----------------
Randomized Search samples a fixed number of hyper-parameter settings from specified distributions. It is more efficient than Grid Search when the hyper-parameter space is large.

Example:
```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': randint(2, 11)
}

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(),
    param_distributions=param_dist,
    n_iter=100,
    scoring='accuracy',
    cv=5,
    verbose=1,
    random_state=42
)

random_search.fit(X_train, y_train)
best_params = random_search.best_params_
print("Best hyper-parameters:", best_params)
```

Evaluating Hyper-parameter Tuning
----------------------------------
After performing hyper-parameter tuning, it is essential to evaluate the best model on a validation or test set to estimate its generalization performance.

Visualization
-------------
Visual tools such as learning curves and validation curves can help understand the effect of hyper-parameters on model performance.

For example, `ValidationCurveDisplay` can be used to visualize how changing a hyper-parameter affects the training and validation scores:

```python
from sklearn.model_selection import validation_curve
from sklearn.model_selection import ValidationCurveDisplay

param_range = [1, 10, 20, 30]
train_scores, test_scores = validation_curve(
    estimator=RandomForestClassifier(),
    X=X_train,
    y=y_train,
    param_name='max_depth',
    param_range=param_range,
    scoring='accuracy',
    cv=5
)

ValidationCurveDisplay(
    validation_curve=(train_scores, test_scores),
    param_name='max_depth',
    param_range=param_range
).plot()
```

Summary
-------
Hyper-parameter tuning is an iterative process that involves selecting a search strategy, defining a parameter grid or distribution, and evaluating model performance. Proper visualization and validation ensure that the selected hyper-parameters lead to a robust and well-performing model.