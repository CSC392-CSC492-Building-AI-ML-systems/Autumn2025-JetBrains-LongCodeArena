Schedulers Module
=================

Overview
--------
The `schedulers` module provides a collection of stateless learning rate schedulers designed to enhance flexibility and control over learning rate adjustments during training. Unlike traditional PyTorch schedulers, these schedulers interface directly with Composer's `~composer.core.time` abstraction, allowing configuration using arbitrary and explicit time units.

Key Concepts
------------
- Stateless Schedulers: Functions or callable classes that determine the learning rate multiplier based on the current training state.
- Time Abstraction: Integration with Composer's `Time` and `TimeUnit` classes enables scheduling based on epochs, batches, or custom time units.
- Multiplicative Effects: Multiple schedulers can be combined, with their effects stacking multiplicatively to produce the final learning rate multiplier.

Main Components
---------------

### ComposerScheduler Protocol
The `ComposerScheduler` protocol defines the interface for all stateless schedulers. Any function or callable class implementing this protocol must define the `__call__` method, which computes a learning rate multiplier based on the current `State`.

#### Usage
A scheduler can be a simple function:
```python
def decay_after_10_epochs(state: State) -> float:
    if state.timestamp.epoch < 10:
        return 1.0
    return 0.5
```

Or a callable class:
```python
class EpochDecayScheduler:
    def __init__(self, num_epochs: int):
        self.num_epochs = num_epochs

    def __call__(self, state: State) -> float:
        if state.timestamp.epoch < self.num_epochs:
            return 1.0
        return 0.5
```

### __call__ Method
The core method of a `ComposerScheduler`, which calculates the learning rate multiplier (`alpha`) given the current `State` and an optional scale schedule ratio (`ssr`). The multiplier is a pure function, enabling flexible and composable scheduling strategies.

### _convert_time Function
Converts a specified `Time` object (string, `Time[int]`, or `Time[float]`) into a `Time[int]` object aligned with the current training `State`. This conversion accounts for different time units and the maximum duration of training, facilitating precise scheduling.

### compile_composer_scheduler Function
Transforms a stateless `ComposerScheduler` into a PyTorch-compatible scheduler object with a `.step()` interface. This allows integration with PyTorch training loops while maintaining the stateless scheduling logic.

Available Schedulers
--------------------
The module exports several predefined schedulers, including:
- `StepScheduler`
- `MultiStepScheduler`
- `ConstantScheduler`
- `LinearScheduler`
- `ExponentialScheduler`
- `CosineAnnealingScheduler`
- `CosineAnnealingWarmRestartsScheduler`
- `PolynomialScheduler`
- Schedulers with warmup phases:
  - `MultiStepWithWarmupScheduler`
  - `ConstantWithWarmupScheduler`
  - `LinearWithWarmupScheduler`
  - `CosineAnnealingWithWarmupScheduler`
  - `PolynomialWithWarmupScheduler`

These schedulers can be combined and customized to suit various training regimes, providing fine-grained control over learning rate schedules in a stateless, flexible manner.

Usage in Training
-----------------
Schedulers are typically used by passing them to the `Trainer`'s `schedulers` argument. They influence the learning rate dynamically based on the current training state, enabling strategies like warmup, decay, cosine annealing, and polynomial decay, all configured with explicit time units.

Summary
-------
The `schedulers` module offers a powerful, flexible framework for defining and applying learning rate schedules in a stateless manner, tightly integrated with Composer's training abstractions. This design promotes clarity, composability, and ease of customization in training workflows.