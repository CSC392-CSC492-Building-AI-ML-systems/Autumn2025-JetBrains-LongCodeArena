Naive Bayes Methods
===================

Overview
--------
The :mod:`sklearn.naive_bayes` module provides implementations of several Naive Bayes algorithms, which are supervised learning methods based on applying Bayes' theorem with strong (naive) feature independence assumptions. These methods are particularly effective for high-dimensional data and are widely used for text classification, spam detection, and other applications.

Naive Bayes Classifiers
-----------------------
The module includes the following Naive Bayes classifiers:

- **BernoulliNB**: Suitable for binary/boolean features.
- **GaussianNB**: Assumes features follow a Gaussian distribution.
- **MultinomialNB**: Designed for discrete features such as word counts.
- **ComplementNB**: An improved version of MultinomialNB for imbalanced data.
- **CategoricalNB**: Handles categorical features with multiple categories.

Common Features
---------------
All Naive Bayes classifiers in scikit-learn share a common interface and provide the following methods:

- **fit(X, y, sample_weight=None)**: Fit the model according to the given training data.
- **predict(X)**: Perform classification on an array of test vectors.
- **predict_proba(X)**: Return probability estimates for the test vectors.
- **predict_log_proba(X)**: Return log-probability estimates for the test vectors.
- **predict_joint_log_proba(X)**: Return joint log probability estimates, combining class prior and feature likelihoods.

Model Attributes
----------------
Naive Bayes classifiers maintain several attributes after fitting:

- **classes_**: Array of class labels known to the classifier.
- **class_count_**: Number of training samples observed in each class.
- **class_prior_**: Estimated prior probabilities of each class.
- **feature_log_prob_**: Log probabilities of features given each class.
- **theta_**: Mean of each feature per class (GaussianNB).
- **var_**: Variance of each feature per class (GaussianNB).
- **n_features_in_**: Number of features seen during fit.
- **feature_names_in_**: Names of features if provided with feature names.

Implementation Details
----------------------
Naive Bayes classifiers assume feature independence given the class label, which simplifies the computation of class probabilities. The models estimate the parameters (e.g., means, variances, feature probabilities) from the training data and use these estimates to perform classification on new data.

Gaussian Naive Bayes
--------------------
The `GaussianNB` class assumes that features follow a Gaussian distribution within each class. It can perform online updates to model parameters via the `partial_fit` method, making it suitable for incremental learning scenarios.

Key parameters include:
- **priors**: Optional prior probabilities for each class.
- **var_smoothing**: Portion of the largest variance added to variances for numerical stability.

Use Cases
---------
Naive Bayes methods are particularly effective for:
- Text classification
- Spam filtering
- Document categorization
- Situations with high-dimensional feature spaces

Limitations
-----------
The naive independence assumption may not hold in all cases, which can affect the accuracy of the models. Nonetheless, Naive Bayes classifiers are often competitive and computationally efficient.

See Also
--------
- :class:`BernoulliNB`
- :class:`MultinomialNB`
- :class:`ComplementNB`
- :class:`CategoricalNB`

For more details, refer to the user guide on [Naive Bayes classifiers](https://scikit-learn.org/stable/modules/naive_bayes.html).