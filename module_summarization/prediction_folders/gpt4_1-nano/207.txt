Decision Trees
==============

Introduction
------------
Decision trees are a popular and versatile machine learning method used for classification and regression tasks. They work by recursively partitioning the feature space into regions with similar target values, creating a tree-like model of decisions.

Impurity Criteria
-----------------
At the core of decision trees is the concept of impurity criteria, which measure how mixed or pure a node is. The Criterion class provides an interface for various impurity metrics, enabling the evaluation of split quality.

Key Methods
-----------
- `init`: Initializes the criterion with target values, sample weights, and sample indices for a node.
- `init_missing`: Handles missing feature values by initializing missing value statistics.
- `reset` / `reverse_reset`: Resets the criterion at the start or end of a node's data segment.
- `update`: Updates the criterion's statistics when moving samples from one child node to another.
- `node_impurity`: Calculates the impurity of the current node.
- `children_impurity`: Evaluates the impurity of the left and right child nodes resulting from a split.
- `node_value`: Computes the value (e.g., class probabilities or regression value) of the current node.
- `clip_node_value`: Clips the node value within specified bounds.
- `middle_value`: Computes the middle value of a split, useful for monotonicity constraints.
- `proxy_impurity_improvement`: Provides a fast approximation of the impurity reduction for candidate splits.
- `impurity_improvement`: Calculates the actual impurity decrease achieved by a split.

Impurity Calculation
--------------------
The impurity of a node reflects how homogeneous the target values are within that node. The goal of the decision tree algorithm is to find splits that maximize the impurity improvement, leading to purer child nodes.

Split Evaluation
----------------
Splits are evaluated based on their impurity reduction. The process involves:
- Computing the impurity of the parent node.
- Assessing the impurity of potential child nodes.
- Calculating the impurity improvement to select the best split.

Handling Missing Values
-----------------------
The framework supports missing feature values by initializing and managing missing value statistics, ensuring robust split decisions even with incomplete data.

Node Values and Constraints
---------------------------
The decision tree can compute and store node values, which are used for predictions. Monotonicity constraints can be enforced by calculating the middle value of splits.

Performance Optimization
------------------------
Proxy impurity improvements are used to speed up the search for the best split by approximating the potential gain without full calculations.

Conclusion
----------
Decision trees are a fundamental component of ensemble methods like Random Forests and Gradient Boosting Machines. Their ability to model complex relationships, handle various data types, and provide interpretable models makes them a powerful tool in machine learning.