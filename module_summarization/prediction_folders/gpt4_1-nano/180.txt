Cross-validation: Evaluating Estimator Performance
====================================================

Introduction
------------
Cross-validation is a robust statistical method used to evaluate the performance of machine learning estimators. It involves partitioning the data into multiple subsets, training the model on some subsets, and validating it on the remaining ones. This process helps in assessing how well the model generalizes to unseen data and in tuning hyperparameters.

Types of Cross-validation
-------------------------
- **K-Fold Cross-validation**: Divides the data into *k* equal parts or folds. The model is trained on *k-1* folds and validated on the remaining fold. This process repeats *k* times, with each fold used once as the validation set.
- **Stratified K-Fold**: Similar to K-Fold but preserves the class distribution in each fold, suitable for classification tasks with imbalanced classes.
- **Leave-One-Out (LOO)**: A special case of K-Fold where *k* equals the number of data points. Each data point is used once as a validation set.
- **ShuffleSplit**: Randomly splits the data into training and test sets multiple times, providing a flexible way to perform cross-validation.

Evaluating Estimator Performance
--------------------------------
The core idea of cross-validation is to estimate the performance of an estimator by averaging scores obtained on different validation sets. This process reduces the variance associated with a single train-test split and provides a more reliable estimate of model performance.

Key Components
--------------
- **Training Scores**: Scores obtained on the training subsets during each fold.
- **Test Scores**: Scores obtained on the validation subsets during each fold.
- **Score Metrics**: Quantitative measures such as accuracy, precision, recall, F1-score, or custom scoring functions to evaluate model performance.

Visualization
-------------
Visual tools like learning curves and validation curves help interpret the results of cross-validation:
- **Learning Curves**: Show how the estimator's performance varies with the size of the training set, indicating whether the model benefits from more data or is overfitting.
- **Validation Curves**: Illustrate the effect of a hyperparameter on model performance, aiding in hyperparameter tuning.

Plotting Utilities
------------------
The `sklearn.model_selection` module provides classes such as `LearningCurveDisplay` to visualize learning curves. These classes include methods like `from_estimator` to generate plots that display training and validation scores across different training set sizes or hyperparameter values.

Example Usage
-------------
```python
from sklearn.model_selection import learning_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Load data
X, y = load_iris(return_X_y=True)

# Define estimator
estimator = RandomForestClassifier()

# Compute learning curve
train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5)

# Plot learning curve
from sklearn.model_selection import LearningCurveDisplay
disp = LearningCurveDisplay(
    learning_curve=learning_curve(estimator, X, y, cv=5),
)
disp.plot()
plt.show()
```

Summary
-------
Cross-validation is an essential technique for assessing the generalization ability of machine learning models. By systematically partitioning data and evaluating performance across multiple folds, it provides a comprehensive understanding of model robustness and helps guide model selection and hyperparameter tuning.