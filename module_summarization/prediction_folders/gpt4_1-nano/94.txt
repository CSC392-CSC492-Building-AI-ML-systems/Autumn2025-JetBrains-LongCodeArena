# Scale Schedule Module Documentation

## Overview

The scale schedule module provides a framework for defining and utilizing stateless learning rate schedulers within the Composer training environment. Unlike traditional PyTorch schedulers, these schedulers are designed to interface directly with Composer's `~composer.core.time` abstraction, allowing for flexible and explicit time unit configurations. This module enables users to create custom scheduling strategies that can be seamlessly integrated into training workflows, supporting complex and dynamic learning rate adjustments.

## Key Components

### ComposerScheduler Protocol

The `ComposerScheduler` protocol defines the interface for stateless scheduler functions. Any callable that matches this interface can serve as a scheduler, whether implemented as a simple function or a callable class.

#### Main Method

- `__call__(self, state: State, ssr: float = 1.0) -> float`

  Calculates the current learning rate multiplier (`Î±`) based on the trainer's state and an optional scale schedule ratio (SSR). The multiplier is a pure function that determines how the base learning rate should be scaled at any given point in training.

  **Parameters:**
  - `state` (State): The current state of the Composer trainer, providing temporal and progress information.
  - `ssr` (float): The scale schedule ratio, used to stretch or compress the schedule temporally.

  **Returns:**
  - `alpha` (float): A multiplier to be applied to the optimizer's initial learning rate.

### Utility Functions

- `_convert_time(time: Union[str, Time[int], Time[float]], state: State, ssr: float = 1.0) -> Time[int]`

  Converts a specified time (string, epoch, or batch) into an absolute batch count, considering the current trainer state and SSR. This function ensures that time specifications are correctly interpreted regardless of their original units.

- `compile_composer_scheduler(scheduler: ComposerScheduler, state: State, ssr: float = 1.0) -> PyTorchScheduler`

  Converts a stateless `ComposerScheduler` into a PyTorch-compatible scheduler object. This allows the scheduler to be used with PyTorch's training loop, providing a `.step()` interface while maintaining access to the current trainer state.

## Usage Examples

### Defining a Simple Scheduler Function

```python
def ten_epoch_decay_scheduler(state: State) -> float:
    if state.timestamp.epoch < 10:
        return 1.0
    return 0.5
```

This function halves the learning rate after 10 epochs and can be used directly as a `ComposerScheduler`.

### Implementing a Callable Class

```python
class VariableEpochDecayScheduler:
    def __init__(self, num_epochs: int):
        self.num_epochs = num_epochs

    def __call__(self, state: State) -> float:
        if state.timestamp.epoch < self.num_epochs:
            return 1.0
        return 0.5
```

This class-based scheduler offers more configurability and can be instantiated with parameters.

## Integration

Schedulers can be combined multiplicatively to create complex scheduling behaviors. They are typically passed to the `Trainer` as part of the `schedulers` list, allowing for flexible and dynamic learning rate adjustments throughout training.

## Summary

This module provides a robust and flexible framework for defining stateless learning rate schedulers that are tightly integrated with Composer's training abstractions. By supporting both functional and class-based implementations, and providing tools for conversion to PyTorch schedulers, it enables advanced scheduling strategies tailored to complex training scenarios.