Optimization Algorithms for Parameter Estimation
===============================================

This section describes three main types of algorithms used for the estimation of model parameters through optimization techniques. These algorithms are implemented within a flexible framework that allows for various solver methods, each suitable for different types of problems and convergence criteria.

1. Newton-Raphson Method
-------------------------
The Newton-Raphson method is an iterative technique that uses the first and second derivatives (gradient and Hessian) of the objective function to find the parameter values that maximize or minimize the function. It is suitable for problems where the Hessian is available and well-behaved. The convergence criterion is based on the relative error in parameters (`tol`).

Parameters:
- `tol`: Relative error in parameters acceptable for convergence.

2. Gradient-Based Methods
-------------------------
These methods rely on the gradient information to navigate the parameter space efficiently. Common algorithms include:

- Nelder-Mead (`nm`): A derivative-free method that uses simplex reflections and contractions.
- Broyden-Fletcher-Goldfarb-Shanno (BFGS) (`bfgs`): Uses an approximation to the Hessian to guide the search.
- Limited-memory BFGS (`lbfgs`): Suitable for large-scale problems, with parameters like `m` controlling memory.
- Conjugate Gradient (`cg`): Uses gradient information to accelerate convergence in large problems.
- Newton-Conjugate Gradient (`ncg`): Combines Newton's method with conjugate gradient techniques for efficient optimization.

Key parameters include:
- `gtol`: Gradient norm tolerance for stopping.
- `maxfun`: Maximum number of function evaluations.
- `epsilon`: Step size for numerical gradient approximation.
- `approx_grad`: Whether to approximate the gradient numerically.

3. Global and Stochastic Methods
--------------------------------
These algorithms are designed to escape local minima and explore the parameter space more broadly:

- Powell (`powell`): Uses line searches along conjugate directions.
- Basin-Hopping (`basinhopping`): Combines local minimization with stochastic jumps to find global minima.
  - Parameters include:
    - `niter`: Number of basin-hopping iterations.
    - `T`: Temperature parameter controlling acceptance probability.
    - `stepsize`: Step size for random displacements.
    - `interval`: Frequency of stepsize updates.
    - `minimizer`: Additional arguments for the local minimizer.

Notes:
- The `basinhopping` algorithm ignores certain parameters like `maxiter`, `retall`, and `full_output`.
- The choice of algorithm depends on the problem's nature, such as smoothness, convexity, and the presence of multiple minima.

This flexible framework allows users to select the most appropriate optimization method for their specific model fitting task, with configurable parameters to control convergence and performance.