Handling Large Datasets
=========================
Managing and processing large datasets efficiently is crucial in astronomical data analysis, especially when working with spectral cubes that can be very large in size. This section discusses strategies and methods implemented in the codebase to handle large datasets effectively.

Memory-Efficient Moment Map Computation
---------------------------------------
The code provides multiple approaches to compute moment maps, which are statistical summaries of spectral data along specified axes. These methods are designed to optimize for speed and memory usage depending on the dataset size and available resources.

1. Slicewise Computation
------------------------
The `moment_slicewise` function calculates moments by processing the data slice-by-slice. This approach minimizes memory usage by loading only one slice into memory at a time, making it suitable for very large datasets that cannot fit entirely into RAM.

2. Raywise Computation
----------------------
The `moment_raywise` function computes moments along individual rays (lines of sight). This method is particularly useful when the dataset is sparse or when focusing on specific regions, as it processes data incrementally and can skip over empty or masked regions efficiently.

3. Cubewise Computation
------------------------
The `moment_cubewise` function operates on the entire dataset at once, leveraging optimized array operations. While this method is faster for smaller datasets, it requires sufficient memory to hold the entire data cube in RAM.

Strategy Selection
------------------
The `moment_auto` function intelligently selects the most appropriate computation strategy based on the dataset's characteristics and available memory. It uses the `iterator_strategy` to determine whether to process data slice-by-slice, ray-by-ray, or all at once, balancing between computational speed and memory constraints.

Memory Optimization Techniques
-----------------------------
- Use of `np.nan_to_num` and masking to handle invalid or missing data efficiently.
- Reshaping and broadcasting arrays to minimize temporary memory allocations.
- Incremental processing to avoid loading entire datasets into memory simultaneously.

Additional Considerations
-------------------------
- Data Masking: Masks are used to exclude invalid or irrelevant data points, reducing unnecessary computations.
- Data Compatibility: Compatibility functions like `allbadtonan` ensure that operations handle datasets with large numbers of invalid entries gracefully.
- WCS and Masking Utilities: Utilities for World Coordinate System (WCS) handling and masking facilitate targeted data processing, further optimizing resource usage.

By employing these strategies and methods, the codebase enables efficient handling of large spectral datasets, ensuring that analyses remain feasible and performant even with limited computational resources.
