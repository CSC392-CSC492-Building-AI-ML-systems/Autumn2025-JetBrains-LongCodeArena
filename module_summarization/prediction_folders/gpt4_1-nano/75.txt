Working with Text Data
========================

Introduction
------------
This tutorial provides an overview of how to work with text data using scikit-learn's feature extraction utilities. It covers preprocessing, tokenization, n-gram generation, and stop words handling, enabling you to convert raw text into numerical feature vectors suitable for machine learning models.

Preprocessing Text Data
-----------------------
Preprocessing is a crucial step in preparing text data. The `_preprocess` function allows you to normalize text by converting it to lowercase and applying accent removal functions.

```python
def _preprocess(doc, accent_function=None, lower=False):
    """
    Chain together an optional series of text preprocessing steps to
    apply to a document.
    """
    if lower:
        doc = doc.lower()
    if accent_function is not None:
        doc = accent_function(doc)
    return doc
```

You can specify whether to convert text to lowercase and choose an accent removal strategy, such as `strip_accents_unicode` or `strip_accents_ascii`.

Accent Removal
--------------
Handling accented characters is often necessary for normalization. Two functions are provided:

- `strip_accents_unicode(s)`: Converts accented unicode characters into their simple counterparts, suitable for languages with diverse accents.
- `strip_accents_ascii(s)`: Transliterates accented characters into ASCII equivalents, best for languages with direct transliteration.

```python
def strip_accents_unicode(s):
    """Transform accentuated unicode symbols into their simple counterpart."""
    try:
        s.encode("ASCII", errors="strict")
        return s
    except UnicodeEncodeError:
        normalized = unicodedata.normalize("NFKD", s)
        return "".join([c for c in normalized if not unicodedata.combining(c)])

def strip_accents_ascii(s):
    """Transform accentuated unicode symbols into ASCII or nothing."""
    nkfd_form = unicodedata.normalize("NFKD", s)
    return nkfd_form.encode("ASCII", "ignore").decode("ASCII")
```

Removing HTML/XML tags
----------------------
The `strip_tags` function removes HTML or XML tags from text, which is useful for cleaning web-scraped data.

```python
def strip_tags(s):
    """Basic regexp based HTML / XML tag stripper function."""
    return re.compile(r"<([^>]+)>", flags=re.UNICODE).sub(" ", s)
```

Tokenization and N-gram Generation
----------------------------------
Tokenization is the process of splitting text into individual tokens (words or symbols). The `_analyze` function orchestrates tokenization, n-gram creation, and stop words removal.

```python
def _analyze(
    doc,
    analyzer=None,
    tokenizer=None,
    ngrams=None,
    preprocessor=None,
    decoder=None,
    stop_words=None,
):
    """
    Chain together an optional series of text processing steps to go from
    a single document to ngrams, with or without tokenizing or preprocessing.
    """
    if decoder is not None:
        doc = decoder(doc)
    if analyzer is not None:
        doc = analyzer(doc)
    else:
        if preprocessor is not None:
            doc = preprocessor(doc)
        if tokenizer is not None:
            doc = tokenizer(doc)
        if ngrams is not None:
            if stop_words is not None:
                doc = ngrams(doc, stop_words)
            else:
                doc = ngrams(doc)
    return doc
```

The `analyzer` parameter can be a custom function that combines tokenization and n-gram generation, or you can specify individual `tokenizer` and `ngrams` functions.

Stop Words Handling
-------------------
Stop words are common words that are often removed to reduce noise in text analysis. The `_check_stop_list` function manages stop words lists, supporting built-in options like `"english"` or custom collections.

```python
def _check_stop_list(stop):
    if stop == "english":
        return ENGLISH_STOP_WORDS
    elif isinstance(stop, str):
        raise ValueError("not a built-in stop list: %s" % stop)
    elif stop is None:
        return None
    else:
        return frozenset(stop)
```

Usage in Vectorizers
--------------------
These preprocessing and tokenization utilities are integrated into scikit-learn's vectorizers such as `CountVectorizer` and `TfidfVectorizer`. They enable transforming raw text into feature vectors by:

- Decoding input data
- Applying preprocessing steps
- Tokenizing text
- Generating n-grams
- Removing stop words

This pipeline facilitates effective feature extraction from text data for machine learning tasks.

Summary
-------
By leveraging these functions, you can customize text preprocessing workflows to suit your specific data and analysis needs. Proper preprocessing enhances model performance by ensuring that the input features accurately represent the underlying textual information.