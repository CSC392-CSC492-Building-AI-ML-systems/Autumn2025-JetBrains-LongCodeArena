Probability Calibration
=========================
Probability calibration is a technique used to improve the accuracy of predicted probabilities generated by classifiers. Well-calibrated probabilities are crucial for decision-making processes that depend on reliable probability estimates, such as risk assessment, medical diagnosis, and financial forecasting.

Overview
--------
Calibration adjusts the output probabilities of a classifier to better reflect the true likelihood of events. This process is especially important when the raw classifier outputs are poorly calibrated, meaning the predicted probabilities do not correspond well to the actual observed frequencies.

Methods
-------
The `CalibratedClassifierCV` class provides two primary methods for probability calibration:

- Sigmoid Calibration ('sigmoid'):
  Based on Platt's scaling, this method fits a logistic regression model to the classifier's decision function outputs. It is effective for many classifiers but may overfit with small calibration datasets.

- Isotonic Regression ('isotonic'):
  A non-parametric approach that fits a monotonically increasing function to the data. It can model more complex calibration curves but may overfit with limited data (less than approximately 1000 samples).

Usage
-----
The calibration process involves cross-validation to prevent overfitting and to produce unbiased probability estimates. The class can be used in different modes:

- Ensemble Mode (`ensemble=True`):
  For each fold in cross-validation, the base estimator is trained on the training subset and calibrated on the testing subset. The final predicted probabilities are averaged across all calibrated models.

- Non-Ensemble Mode (`ensemble=False`):
  Cross-validation predictions are used solely for calibration, and the final classifier is trained on the entire dataset for prediction.

- Pre-Fitted Estimators (`cv="prefit"`):
  If the estimator is already fitted, calibration can be performed directly without cross-validation, assuming the user ensures disjoint training and calibration data.

Parameters
----------
- `estimator`: The classifier to be calibrated. Defaults to `LinearSVC`.
- `method`: Calibration method, either `'sigmoid'` or `'isotonic'`.
- `cv`: Cross-validation strategy, which can be an integer, a CV splitter, an iterable, or `'prefit'`.
- `n_jobs`: Number of parallel jobs for fitting.
- `ensemble`: Whether to use ensemble averaging across CV folds.

Attributes
----------
- `classes_`: Array of class labels.
- `n_features_in_`: Number of features seen during fitting.
- `feature_names_in_`: Names of features seen during fitting.
- `calibrated_classifiers_`: List of calibrated classifiers, one per CV fold or a single classifier if `cv="prefit"`.

Advantages
----------
- Improves the reliability of probability estimates.
- Flexible calibration methods to suit different data and classifier types.
- Supports cross-validation to prevent overfitting.

Considerations
--------------
- Isotonic calibration may overfit with small datasets.
- Proper cross-validation setup is essential for unbiased calibration.
- Calibration is most effective when the base classifier's decision function is well-behaved.

Read more in the User Guide <calibration>.
