# Stochastic Gradient Descent (SGD)

Stochastic Gradient Descent (SGD) is an iterative optimization algorithm used to minimize a loss function, particularly in machine learning models such as linear regression, logistic regression, and neural networks. Unlike traditional gradient descent, which computes the gradient using the entire dataset, SGD updates the model parameters using only a single or a small batch of training samples at each iteration. This approach significantly reduces computation time and allows the model to scale to large datasets.

## Key Concepts

- **Gradient Update**: In each iteration, the model parameters are adjusted in the direction opposite to the gradient of the loss function with respect to the parameters.
- **Stochastic Approximation**: The gradient is estimated using a randomly selected subset of data, introducing noise that can help escape local minima.
- **Learning Rate**: A hyperparameter that controls the size of the update steps. Proper tuning of the learning rate is crucial for convergence.

## Implementation Details

The implementation of SGD involves the following steps:

1. Initialize model parameters (weights and biases).
2. Shuffle the training data to ensure randomness.
3. For each epoch:
   - For each sample or mini-batch:
     - Compute the prediction.
     - Calculate the gradient of the loss function with respect to the parameters.
     - Update the parameters using the gradient and the learning rate.

## Loss Functions

SGD can be used with various loss functions, including:

- **Squared Loss**: Suitable for regression tasks.
- **Log Loss (Logistic Loss)**: Used for classification tasks, especially binary and multinomial logistic regression.

## Advantages

- Efficient for large datasets.
- Can escape local minima due to its stochastic nature.
- Suitable for online learning where data arrives sequentially.

## Considerations

- Choice of learning rate and its decay schedule.
- Number of epochs and convergence criteria.
- Variance reduction techniques like momentum or adaptive learning rates (e.g., Adam, RMSProp).

## Usage in Code

The provided code includes optimized implementations of loss functions and their gradients for multinomial logistic regression, supporting different data types (`float` and `double`). These implementations are designed to be used within a larger SGD framework, enabling efficient training of models with high-dimensional data.

---

This documentation provides an overview of Stochastic Gradient Descent, its principles, and its implementation considerations, serving as a guide for understanding and applying SGD in machine learning workflows.