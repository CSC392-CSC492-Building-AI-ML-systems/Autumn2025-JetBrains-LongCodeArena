Neural Network Models (Supervised)
===================================

Overview
--------
Supervised neural network models are a class of machine learning algorithms designed to learn mappings from input data to output labels. These models are trained using labeled datasets, where the goal is to minimize the discrepancy between the predicted outputs and the true labels. They are widely used for tasks such as classification and regression.

Activation Functions
--------------------
Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. The following activation functions are supported:

- **Identity**: Leaves the input unchanged.
- **Tanh**: Hyperbolic tangent function, outputs values between -1 and 1.
- **Logistic (Sigmoid)**: Outputs values between 0 and 1, suitable for probability estimation.
- **ReLU (Rectified Linear Unit)**: Outputs zero for negative inputs and the input itself for positive inputs.
- **Softmax**: Converts input vectors into probability distributions over classes.

These functions can be applied inplace for efficiency during training.

Derivative Functions
--------------------
For backpropagation, derivatives of activation functions are essential to compute gradients:

- **Identity Derivative**: No change.
- **Logistic Derivative**: Exploits the output value for efficient computation.
- **Tanh Derivative**: Uses the output value to compute the gradient.
- **ReLU Derivative**: Zeroes out gradients where the input was zero.

Loss Functions
--------------
Loss functions quantify the difference between predicted outputs and true labels, guiding the training process:

- **Squared Loss**: Suitable for regression tasks, measures mean squared error.
- **Log Loss (Cross-Entropy Loss)**: Used for classification, measures the divergence between predicted probabilities and true labels.
- **Binary Log Loss**: Specialized for binary classification and multilabel problems with a single output node.

Implementation details include numerical stability measures such as clipping predicted probabilities to avoid log(0) errors.

Usage
-----
These utilities form the backbone of supervised neural network training, providing essential functions for activation, derivatives, and loss calculations. They can be integrated into custom neural network implementations or used to extend existing frameworks for specialized tasks.

Note
----
This module is designed to be efficient, supporting inplace operations to reduce memory overhead during training. Proper selection of activation functions and loss metrics is crucial for model performance depending on the specific problem domain.