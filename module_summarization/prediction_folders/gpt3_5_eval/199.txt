Neural Network Models (Supervised)
====================================

This module provides utilities for neural network models with supervised learning. It includes functions for various activation functions, their derivatives, loss functions for regression and classification tasks, and more.

Activation Functions
--------------------

1. **Identity Function**
    - Function: Simply leaves the input array unchanged.
    - Parameters:
        - `X`: Data array with shape (n_samples, n_features).

2. **Logistic Function**
    - Function: Computes the logistic function inplace.
    - Parameters:
        - `X`: Input data array with shape (n_samples, n_features).

3. **Hyperbolic Tan Function**
    - Function: Computes the hyperbolic tangent function inplace.
    - Parameters:
        - `X`: Input data array with shape (n_samples, n_features).

4. **Rectified Linear Unit (ReLU) Function**
    - Function: Computes the rectified linear unit function inplace.
    - Parameters:
        - `X`: Input data array with shape (n_samples, n_features).

5. **Softmax Function**
    - Function: Computes the K-way softmax function inplace.
    - Parameters:
        - `X`: Input data array with shape (n_samples, n_features).

Loss Functions
--------------

1. **Squared Loss**
    - Function: Computes the squared loss for regression.
    - Parameters:
        - `y_true`: Ground truth values.
        - `y_pred`: Predicted values.

2. **Logistic Loss**
    - Function: Computes the logistic loss for classification.
    - Parameters:
        - `y_true`: Ground truth labels.
        - `y_prob`: Predicted probabilities.

3. **Binary Logistic Loss**
    - Function: Computes binary logistic loss for classification.
    - Parameters:
        - `y_true`: Ground truth labels.
        - `y_prob`: Predicted probabilities.

Activation Function Derivatives
-------------------------------

The module also provides derivatives for the activation functions to be used during backpropagation.

For detailed usage and examples, please refer to the function documentation within the code.