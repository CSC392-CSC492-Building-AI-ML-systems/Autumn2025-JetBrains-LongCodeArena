# Generate documentation for this module

The module `scale_schedule.py` provides stateless learning rate schedulers that aim to address some of the limitations of PyTorch's built-in schedulers. These schedulers are designed to work seamlessly with Composer's `composer.core.time` abstraction, allowing for configuration using explicit time units.

## ComposerScheduler Protocol

The `ComposerScheduler` protocol defines the specification for a stateless scheduler function. It can be implemented as a function or a callable class, as long as it adheres to the signature of the `__call__` method. The scheduler function should return a multiplier that will be applied to the optimizer's learning rate based on the trainer's current state and an optional "scale schedule ratio" (SSR).

## Supported Schedulers

The module provides the following stateless schedulers:

- `StepScheduler`
- `MultiStepScheduler`
- `ConstantScheduler`
- `LinearScheduler`
- `ExponentialScheduler`
- `CosineAnnealingScheduler`
- `CosineAnnealingWarmRestartsScheduler`
- `PolynomialScheduler`
- `MultiStepWithWarmupScheduler`
- `ConstantWithWarmupScheduler`
- `LinearWithWarmupScheduler`
- `CosineAnnealingWithWarmupScheduler`
- `PolynomialWithWarmupScheduler`

These schedulers offer various strategies for adjusting the learning rate over time, allowing for flexibility in training configurations.

## Converting Schedulers

The `compile_composer_scheduler` function can be used to convert a stateless scheduler into a PyTorch scheduler object. This conversion enables the stateless scheduler to be used within the PyTorch training loop, providing a familiar `.step()` interface while maintaining a reference to the current `composer.core.State`.

By leveraging the capabilities of the `ComposerScheduler` protocol and the provided schedulers, users can easily define custom learning rate schedules tailored to their specific training requirements.