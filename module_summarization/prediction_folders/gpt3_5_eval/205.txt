# Stochastic Gradient Descent (SGD)

Stochastic Gradient Descent (SGD) is a popular optimization algorithm used in machine learning for training models. It is particularly well-suited for large datasets and online learning scenarios where the entire dataset cannot fit into memory at once.

## Algorithm Overview

SGD works by updating the model parameters iteratively based on the gradient of the loss function with respect to the parameters. Instead of computing the gradient using the entire dataset (as in batch gradient descent), SGD computes the gradient using a single data point or a small subset of data points (mini-batch) at each iteration.

The key steps of the SGD algorithm are as follows:

1. Initialize the model parameters.
2. Shuffle the training data.
3. For each data point or mini-batch:
   - Compute the gradient of the loss function with respect to the parameters.
   - Update the parameters using the gradient and a learning rate.
4. Repeat the process for a specified number of iterations or until convergence.

## Multinomial Logistic Regression Loss

In the context of classification tasks, SGD can be used to optimize the parameters of a multinomial logistic regression model. The MultinomialLogLoss class provides the implementation of the multinomial logistic regression loss function for different data types (float and double).

The loss function computes the multinomial logistic loss for a single sample, taking into account the prediction, the correct class index, the number of classes, and the sample weight. It is based on the concept of log-sum-exp to ensure numerical stability.

## Multinomial Logistic Regression Gradient

The gradient of the multinomial logistic loss with respect to the model parameters is computed using the dloss method in the MultinomialLogLoss class. This gradient is used to update the model parameters during the optimization process.

## Soft Thresholding Function

The _soft_thresholding function provides a way to apply soft thresholding to a given value, which is commonly used in regularization techniques such as L1 regularization (Lasso).

## Authors

- Danny Sullivan <dbsullivan23@gmail.com>
- Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
- Arthur Mensch <arthur.mensch@m4x.org>
- Arthur Imbert <arthurimbert05@gmail.com>
- Joan Massich <mailsik@gmail.com>

## License

This implementation of SGD is provided under the BSD 3-Clause License.