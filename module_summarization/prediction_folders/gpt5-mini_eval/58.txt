optimization module
===================

General-purpose optimization helpers for model fitting. These utilities are
designed to be independent of any particular likelihood model implementation
and wrap common solvers from scipy.optimize to provide a unified interface
for fitting objective functions (for example, negative log-likelihoods).

Summary of contents
-------------------
- _check_method: Validate that a requested optimization method is supported.
- Optimizer: Lightweight class providing a flexible _fit(...) method that
  dispatches to multiple scipy.optimize solvers and collects solver settings
  and return information.

Functions
---------

_check_method(method, methods)
    Validate that `method` is present in the allowed `methods` sequence.

    Parameters
    ----------
    method : str
        Name of the chosen optimization method.
    methods : iterable
        Iterable of allowed method names.

    Raises
    ------
    ValueError
        If `method` is not in `methods`.

Classes
-------

Optimizer
    Reusable optimizer wrapper with a single high-level fitting routine.

    Methods
    -------

    _fit(objective, gradient, start_params, fargs, kwargs,
         hessian=None, method='newton', maxiter=100, full_output=True,
         disp=True, callback=None, retall=False)
        Fit an objective function using a chosen solver from scipy.optimize.

        This routine standardizes inputs and outputs across different
        underlying solvers (Newton-Raphson, Nelder-Mead, BFGS, Powell,
        Conjugate Gradient, Newton-conjugate-gradient, basinhopping, or the
        generic scipy.optimize.minimize wrapper). It returns the optimized
        parameter vector and, optionally, a dictionary of solver-specific
        return values and settings.

        Parameters
        ----------
        objective : callable
            Function to minimize (for example, negative log-likelihood).
            Signature: objective(params, *fargs, **kwargs) -> float
        gradient : callable or None
            Gradient (score) function with signature gradient(params, *fargs,
            **kwargs) -> ndarray. If None, a numerical approximation may be
            used by the underlying solver.
        start_params : array_like
            Initial guess for the parameters.
        fargs : tuple
            Extra positional arguments passed to `objective` and `gradient`.
        kwargs : dict
            Extra keyword arguments passed to `objective` and `gradient`.
        hessian : callable or None, optional
            Hessian function (or function returning Hessian-vector products
            for some solvers). Used when available for second-order methods.
        method : str, optional
            Optimization method to use. Supported examples include:
            'newton', 'nm' (Nelder-Mead), 'bfgs', 'powell', 'cg', 'ncg',
            'basinhopping', and the generic 'minimize' wrapper. The choice
            determines which scipy solver is invoked and which solver-specific
            options apply.
        maxiter : int, optional
            Maximum number of iterations to perform (solver-dependent).
        full_output : bool, optional
            If True, return all available solver outputs in a dictionary.
        disp : bool, optional
            If True, display convergence messages produced by the solver.
        callback : callable or None, optional
            Function called after each iteration as callback(xk), where xk
            is the current parameter vector.
        retall : bool, optional
            If True, request the solver to return the list of solutions at
            each iteration (when supported).

        Returns
        -------
        xopt : ndarray
            Parameter vector at the solution found by the optimizer.
        retvals : dict or None
            If `full_output` is True, a dictionary containing solver-specific
            return values (e.g., convergence status, function value,
            number of iterations, message). If `full_output` is False,
            this is None.
        optim_settings : dict
            Dictionary of settings passed to the solver, including the chosen
            method and any explicit solver options.

        Notes
        -----
        - The wrapper forwards arguments to the underlying scipy solvers and
          therefore supports solver-specific optional parameters (tol, gtol,
          ftol, xtol, maxfun, epsilon, etc.). Those options are recorded in
          optim_settings and returned as part of the solver metadata.
        - The 'basinhopping' global optimizer ignores some of the high-level
          explicit flags (for example, `maxiter`, `retall`, `full_output`)
          and exposes its own control arguments (niter, T, stepsize, etc.).
        - When analytic gradient/hessian functions are unavailable, many
          scipy solvers will use numerical approximations; options such as
          `epsilon` control the step sizes used for finite-difference
          approximations.
        - Behavior and exact returned fields in retvals depend on the chosen
          scipy.optimize solver. See scipy.optimize documentation for solver-
          specific details.

See also
--------
scipy.optimize
    For details and options for individual optimization algorithms.