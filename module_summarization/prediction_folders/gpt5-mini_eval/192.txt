Manifold Learning — Non‑linear Dimensionality Reduction
======================================================

Overview
--------
Manifold learning refers to a family of methods for non‑linear dimensionality reduction that seek a low‑dimensional embedding of high‑dimensional data while preserving geometric or topological structure. These methods assume that the high‑dimensional data lie on or near a low‑dimensional manifold and attempt to recover meaningful coordinates on that manifold for visualization, preprocessing, or downstream tasks.

Common approaches include:
- Multidimensional Scaling (MDS) — metric and nonmetric variants.
- Isomap, Locally Linear Embedding (LLE), Hessian LLE, and other neighborhood‑preserving methods.
- Spectral embedding methods (Laplacian Eigenmaps).
- Stochastic techniques such as t‑SNE and UMAP (for visualization-focused embeddings).

Multidimensional Scaling (MDS)
-----------------------------
MDS embeds items in a low‑dimensional Euclidean space so that pairwise distances in the embedding match given dissimilarities as closely as possible.

Key concepts
- Dissimilarity matrix: a symmetric n×n matrix Δ = (δ_ij) of pairwise dissimilarities between n points. For metric MDS the matrix is taken as given; for nonmetric MDS the method seeks a monotonic transformation of the dissimilarities to best match embedding distances.
- Stress (raw): the objective function typically minimized by MDS. A common form is
  stress = 1/2 Σ_ij (d_ij − δ̂_ij)^2
  where d_ij are distances in the embedding and δ̂_ij are either the original dissimilarities (metric MDS) or disparities (nonmetric MDS).
- Normalized stress (Stress‑1): a scale‑invariant version of stress, often used for comparing solutions across problems. One common normalization is
  Stress‑1 = sqrt( stress / (1/2 Σ_ij δ̂_ij^2) ).

Metric vs Nonmetric MDS
- Metric MDS: attempts to preserve the numerical values of dissimilarities directly.
- Nonmetric MDS: preserves only the rank order (monotonic relationship) of dissimilarities. Nonmetric MDS fits a monotonic transformation (disparities) of the dissimilarities to match embedding distances; isotonic regression is typically used for this step. In many implementations, certain sentinel values (e.g., zeros) may be treated as missing dissimilarities.

SMACOF algorithm (Scaling by MAjorizing a COmplicated Function)
---------------------------------------------------------------
SMACOF is an iterative majorization algorithm commonly used to solve the MDS stress minimization problem. Main elements:
- Initialization: choose an initial configuration X (random or provided).
- Majorization steps:
  1. Given current embedding X, compute pairwise embedding distances d_ij.
  2. For nonmetric MDS, compute disparities δ̂_ij by isotonic regression of δ_ij onto d_ij (only using observed dissimilarities).
  3. Form a weight/ratio matrix from disparities and distances and construct the Guttman transform (matrix B).
  4. Update the embedding: X ← (1/n) B X (a linear update).
- Convergence: iterate until stress change is below a tolerance (relative or absolute) or a maximum number of iterations is reached.

Practical parameters and behavior
- n_components: target embedding dimensionality (usually 2 or 3 for visualization).
- init: starting configuration (array of shape n_samples × n_components). If not provided, a random initialization is used.
- max_iter: maximum number of SMACOF iterations per run.
- eps (tolerance): relative tolerance for declaring convergence in stress.
- n_init: (when available) number of independent runs with different initializations; choose the solution with best stress.
- random_state: seed or RNG for reproducible random initialization.
- normalized_stress: option to return normalized stress (Stress‑1). Note: normalization only makes sense when disparities are available (commonly used with nonmetric MDS).
- Handling missing dissimilarities: in nonmetric MDS implementations, specific values (e.g., zeros) may be treated as missing and ignored during isotonic regression and stress computation.
- Verbosity and diagnostics: iterative algorithms commonly provide the final stress value, the number of iterations used, and optional per‑iteration logging.

Interpreting stress values
- Stress measures goodness of fit. Lower stress indicates a better fit between dissimilarities and embedding distances.
- Heuristic interpretation (typical guidelines):
  - 0.000–0.025: excellent
  - 0.025–0.050: good
  - 0.050–0.100: fair
  - 0.100–0.200: poor
These are rules of thumb; interpretation depends on problem scale and normalization.

Practical tips
- Scale dissimilarities appropriately before embedding; extreme scales may affect convergence or interpretation.
- Use multiple random initializations (n_init) for robustness against local minima.
- For nonmetric MDS, ensure that missing or sentinel values are handled consistently and that monotonic fits are meaningful for the data.
- Compare MDS results with other manifold methods (Isomap, LLE, spectral embeddings, t‑SNE/UMAP) depending on whether global geometry or local neighborhoods are most important.

References
- Kruskal, J. (1964). Nonmetric multidimensional scaling: a numerical method. Psychometrika, 29.
- Kruskal, J. (1964). Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29.
- Borg, I., & Groenen, P. (1997). Modern Multidimensional Scaling: Theory and Applications. Springer Series in Statistics.