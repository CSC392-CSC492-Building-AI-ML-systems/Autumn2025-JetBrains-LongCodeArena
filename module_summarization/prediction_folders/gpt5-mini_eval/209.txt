Analyzing a collection of text documents
======================================

This module provides utilities to transform a collection of text documents into
numeric feature vectors for use with machine learning estimators. The most
commonly used classes are:

- CountVectorizer — Convert a collection of text documents to a matrix of token
  counts.
- TfidfTransformer — Transform a count matrix to a normalized TF or TF–IDF
  representation.
- TfidfVectorizer — Equivalent to CountVectorizer followed by
  TfidfTransformer.
- HashingVectorizer — Convert a collection of text documents to a matrix of
  token occurrences using a hashing function (stateless, no vocabulary).

Core concepts
-------------

Preprocessing pipeline
~~~~~~~~~~~~~~~~~~~~~~

Documents are processed through a simple, composable pipeline. The main steps
are:

1. decoding (if the input is bytes, filename, or file-like object),
2. preprocessing (e.g., lowercasing, accent stripping),
3. tokenization,
4. formation of n‑grams,
5. optional stop word filtering.

The order and composition of these steps can be customized by providing a
preprocessor, tokenizer, analyzer or decoder. If an analyzer callable is
provided, it replaces the preprocessor + tokenizer + ngram steps and receives
the (decoded) document directly.

Key utility functions
~~~~~~~~~~~~~~~~~~~~~

strip_accents_unicode(s)
    Normalize and remove combining diacritics using Unicode normalization.
    Slower but generic.

strip_accents_ascii(s)
    Normalize and transliterate to ASCII where possible (fast but language
    dependent).

strip_tags(s)
    Very small regexp-based HTML/XML tag stripper. For robust HTML parsing use
    an external library (lxml, BeautifulSoup).

_check_stop_list(stop)
    Resolve the stop words argument. The string 'english' is mapped to the
    builtin ENGLISH_STOP_WORDS set. Passing None leaves stop words disabled.
    Any other iterable is converted to a frozenset.

Analysis primitives
~~~~~~~~~~~~~~~~~~~

_analyze(doc, analyzer=None, tokenizer=None, ngrams=None,
         preprocessor=None, decoder=None, stop_words=None)
    Apply the configured sequence of steps to a single document and return a
    sequence of tokens or n‑grams. If decoder is provided it is applied first,
    then either the user-supplied analyzer (which replaces the remaining
    steps) or the preprocessor -> tokenizer -> ngrams chain. If stop words are
    provided they may be applied by the ngrams step or by the token filtering
    performed prior to forming n‑grams.

_preprocess(doc, accent_function=None, lower=False)
    Convenience helper to apply lowercasing and an accent function (one of
    strip_accents_unicode or strip_accents_ascii) to a single document.

_VectorizerMixin
~~~~~~~~~~~~~~~~

Shared logic used by vectorizer implementations:

- decode(doc) accepts bytes, filename, file-like objects or unicode strings and
  returns a unicode string using the configured encoding and error handling.
  Special input options include input="filename" and input="file".
- _word_ngrams(tokens, stop_words=None) filters tokens by stop words and
  optionally produces token n‑grams.

Typical usage
-------------

Basic CountVectorizer usage:

:: 
    from sklearn.feature_extraction.text import CountVectorizer
    docs = [
        "The quick brown fox jumps over the lazy dog",
        "Never jump over the lazy dog quickly",
    ]
    vec = CountVectorizer(lowercase=True, strip_accents='unicode',
                          stop_words='english', ngram_range=(1,2))
    X = vec.fit_transform(docs)
    # X is a sparse matrix of token (and token-pair) counts
    vocab = vec.vocabulary_  # mapping token -> feature index

TF–IDF pipeline:

::
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1))
    X_tfidf = tfidf.fit_transform(docs)

HashingVectorizer (stateless):

::
    from sklearn.feature_extraction.text import HashingVectorizer
    hasher = HashingVectorizer(n_features=2**20, alternate_sign=False)
    X_hashed = hasher.transform(docs)
    # No .vocabulary_: features are determined by the hash function

Custom tokenization or analysis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can provide a custom tokenizer or analyzer to fully control tokenization
and normalization. Examples:

::
    # custom tokenizer
    def my_tokenizer(doc):
        # return an iterable of tokens
        return doc.split("-")

    cv = CountVectorizer(tokenizer=my_tokenizer, lowercase=False)

::
    # custom analyzer (skips preprocessor and tokenizer)
    def my_analyzer(doc):
        # return an iterable of tokens or n-grams
        return (token.strip() for token in doc.split())

    cv = CountVectorizer(analyzer=my_analyzer)

Notes and caveats
-----------------

- Use strip_accents='unicode' or strip_accents='ascii' (or supply an
  appropriate function) when your data contains accented characters and you
  want a deterministic mapping to base characters.
- stop_words='english' uses the builtin ENGLISH_STOP_WORDS constant. You may
  also pass a custom iterable of stop words.
- HashingVectorizer is stateless: it does not provide a vocabulary mapping,
  and collisions are possible due to hashing.
- For robust HTML preprocessing use a proper HTML parser rather than
  strip_tags.
- Vectorizers typically return scipy sparse matrices (CSR) of dtype int64 or
  float64 depending on the transformer (counts vs TF–IDF).

See also
--------

- ENGLISH_STOP_WORDS — builtin English stop word set.
- FeatureHasher — lower-level hashing utility used by HashingVectorizer.