Decision Trees
==============

Overview
--------
Decision trees are a family of predictive models that partition the feature space into axis-aligned regions and assign a constant prediction to each region. Trees are built recursively by selecting at each node a feature and a threshold that maximize some measure of improvement in a chosen impurity (or loss) criterion. The implementation described here exposes a low-level Criterion interface used to compute node and children impurities, node prediction values, and to evaluate the quality of candidate splits efficiently (including support for sample weights and missing values).

Criterion interface
-------------------
A Criterion encapsulates how to evaluate the quality of a node and of a split. It provides methods to:

- initialize state for a given node:
  - y (n_samples × n_outputs), sample_weight, weighted_n_samples, sample_indices, start, end
  - initialize missing-value bookkeeping when feature values are missing (init_missing)
- reset state to the leftmost or rightmost position (reset, reverse_reset)
- update statistics when samples are moved between children (update)
- compute the impurity of the current node (node_impurity)
- compute the combined impurities of left and right children for the current split position (children_impurity)
- compute the node prediction value (node_value) and a clipped node value (clip_node_value)
- compute a middle value for splits used when enforcing monotonicity constraints (middle_value)
- compute a proxy score used to quickly rank candidate splits (proxy_impurity_improvement)
- compute the final weighted impurity improvement for a chosen split (impurity_improvement)

Key inputs and data organization
-------------------------------
- y: target array (as Cython memoryview / contiguous buffer) possibly storing multiple outputs.
- sample_weight: per-sample weights (double precision).
- weighted_n_samples: total weight of samples in the node (double).
- sample_indices: an index buffer describing which samples are present in the node; samples for missing values can be placed at the end of this buffer for a node and initialized via init_missing.
- start / end: indices into sample_indices specifying the current node's samples.

Impurity and improvement
------------------------
Impurity measures how “pure” a node is (lower is better). Different criteria are used depending on the learning task:

- Classification: commonly Gini impurity or entropy (log-loss).
- Regression: mean squared error (MSE) or absolute error (MAE) variants.
- Count-based tasks: Poisson or other likelihood-based criteria (note: numerical stability constants such as EPSILON are used internally for Poisson).

For a parent node t with weighted impurity I_parent and a candidate split producing left and right children with weighted impurities I_left and I_right, the weighted impurity improvement is computed as:

  improvement = (N_t / N) * (I_parent - (N_t_R / N_t) * I_right - (N_t_L / N_t) * I_left)

where:
- N is the total number of samples in the dataset (constant for tree building),
- N_t is the (weighted) number of samples at node t,
- N_t_L and N_t_R are the (weighted) samples in left and right children respectively.

Proxy criterion for fast search
-------------------------------
To accelerate the search for the best split, a proxy score is computed that neglects additive constants common across splits and focuses on the terms that change with the split. The proxy is proportional to:

  proxy = - N_t_R * I_right - N_t_L * I_left

Maximizing this proxy selects the same split as maximizing the exact impurity improvement while avoiding repeated computation of invariant terms. The exact weighted improvement is computed once the best candidate split has been selected.

Node values and prediction
--------------------------
A Criterion computes the prediction value for a node (node_value), i.e., the constant label or regression value to be returned for samples that fall in the node. For some criteria and applications, a clipped node value is useful to enforce bounds on predictions (clip_node_value). When monotonic constraints are used, the middle_value method provides the appropriate split midpoint to preserve monotonicity.

Sample weights and missing values
-------------------------------
Decision tree criteria support per-sample weights. The Criterion implementation stores and updates weighted counts and weighted impurities as samples move between children during split evaluation.

Missing feature values are supported by grouping missing samples in the node's sample_indices buffer and initializing missing-specific accumulators using init_missing. This allows splits to be evaluated while accounting for absent data points.

Monotonic constraints
---------------------
When monotonicity constraints are required, split selection and node values can use specialized computations (middle_value, clip_node_value) to ensure that predictions respect specified monotonic relationships between features and the target.

Implementation notes
--------------------
- The Criterion interface is implemented in Cython for speed, using typed memoryviews and C-level buffers to incrementally update sufficient statistics for splits without Python overhead.
- Numerical stability considerations (e.g., small EPSILON values) are applied in likelihood-based criteria (such as Poisson).
- Utility components such as a WeightedMedianCalculator and numerically-stable log helpers (xlogy) are used to implement quantile-based objectives and entropy/log-loss computations efficiently.

References
----------
- L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen (1984). Classification and Regression Trees.
- Standard impurity measures: Gini, entropy (log-loss), mean squared error.