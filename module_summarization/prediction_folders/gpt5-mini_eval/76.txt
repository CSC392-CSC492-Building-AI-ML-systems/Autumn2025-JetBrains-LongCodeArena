Feature extraction
==================

Overview
--------
This module provides utilities to extract intermediate features (activations) from a torch.nn.Module by tracing its forward graph and returning the outputs of selected nodes. It is useful for inspection, feature-based losses, perceptual metrics, and transfer learning.

Key behaviors
- You select nodes in the model's FX graph to return; the extractor returns an OrderedDict mapping user-chosen output names to the corresponding tensors.
- Node names are dot-separated paths walking the module hierarchy (the top-level module name is omitted). For example: "layer1.0.relu".
- Duplicate names are disambiguated with a numeric suffix (_1, _2, ...), computed relative to the local parent scope.
- The tracer records node names for both training and eval graphs and will warn if they differ; in such cases you may need to pick outputs separately for train/eval modes.
- The tracer infrastructure supports treating specified module classes as "leaf modules" so tracing does not go inside them (returning a single call_module node for that module).

Public API (summary)
- create_feature_extractor(model, return_nodes)
  - model: the nn.Module to trace.
  - return_nodes: dict mapping node names (as returned by get_graph_node_names) to the desired output names (strings).
  - Returns: a GraphModule (callable) that, when invoked with inputs, returns an OrderedDict[str, Tensor] of requested intermediate outputs.

- get_graph_node_names(model)
  - model: the nn.Module to inspect.
  - Returns: list of node-qualified names (strings) present in the model's forward graph, suitable for use as keys in return_nodes.

Usage
-----

1) Inspect the graph to find node names
   Use get_graph_node_names to list candidate nodes to extract.

   Example:
   ::
       >>> from torchvision.models import resnet50
       >>> from torchvision.models.feature_extraction import get_graph_node_names
       >>> model = resnet50(pretrained=True)
       >>> node_names = get_graph_node_names(model)
       >>> for n in node_names[:20]:
       ...     print(n)

2) Create a feature extractor
   Construct a mapping from graph node names to the output keys you want, then call create_feature_extractor.

   Basic example:
   ::
       >>> from torchvision.models import resnet50
       >>> from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names
       >>> import torch
       >>>
       >>> model = resnet50(pretrained=True)
       >>> # Inspect and pick nodes (names will depend on model and tracing)
       >>> nodes = get_graph_node_names(model)
       >>> # Suppose we want outputs of 'layer1.0.relu' and 'layer3.5.relu'
       >>> return_nodes = {
       ...     "layer1.0.relu": "feat1",
       ...     "layer3.5.relu": "feat2",
       ... }
       >>> extractor = create_feature_extractor(model, return_nodes)
       >>> extractor.eval()
       >>> x = torch.randn(1, 3, 224, 224)
       >>> feats = extractor(x)
       >>> # feats is an OrderedDict with keys "feat1", "feat2"
       >>> print(feats["feat1"].shape, feats["feat2"].shape)

Notes and tips
- Always use get_graph_node_names to discover the canonical names for nodes in your model; layer/module attribute names and FX node names can differ in subtle ways.
- If you see warnings about graph differences between train and eval traces, the set/order of nodes differs between modes; you may need to choose nodes that are present in both or handle train/eval separately.
- To avoid tracing inside certain modules (for instance, custom submodules you want treated as atomic), use the tracer's support for leaf modules (the tracer used internally accepts a leaf_modules set). This keeps the graph shallower and yields a single node for each leaf module.
- Returned outputs preserve execution order as discovered during tracing (OrderedDict).

Example: transfer-style feature loss
-----------------------------------
Use a feature extractor to compute perceptual losses between input and target images:

::
    >>> from torchvision.models import vgg16
    >>> from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names
    >>> import torch.nn.functional as F
    >>> model = vgg16(pretrained=True).features
    >>> nodes = get_graph_node_names(model)
    >>> # choose layers commonly used for perceptual loss
    >>> return_nodes = {
    ...     "3": "relu1_2",   # layer index strings for sequential modules
    ...     "8": "relu2_2",
    ...     "15": "relu3_3",
    ... }
    >>> extractor = create_feature_extractor(model, return_nodes)
    >>> extractor.eval()
    >>> x = torch.randn(1, 3, 224, 224)
    >>> y = torch.randn(1, 3, 224, 224)
    >>> feats_x = extractor(x)
    >>> feats_y = extractor(y)
    >>> loss = 0.0
    >>> for k in feats_x:
    ...     loss += F.mse_loss(feats_x[k], feats_y[k])

See also
--------
- torch.fx: FX tracing utilities used under the hood to construct the feature extractor graph.
- torchvision model definitions: node naming conventions differ by model implementation; consult the graph node listing for each model with get_graph_node_names.

License and attribution
-----------------------
This feature extraction utility is built on torch.fx and designed to integrate with torchvision models.