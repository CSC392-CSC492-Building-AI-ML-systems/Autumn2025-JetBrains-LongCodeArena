Neural network models (supervised)
=================================

This module provides small utilities used by supervised neural network models:
inplace activation functions and their corresponding inplace derivatives, and
common loss functions for regression and classification.

Overview
--------
Functions operate inplace on Numpy arrays (or array-like objects with Numpy
semantics). Most activation functions accept an array `X` with shape
(n_samples, n_features) and replace its contents with the activation outputs.
Derivative functions accept the activation output `Z` from the forward pass
and an already allocated `delta` array (same shape) containing the
backpropagated error; they modify `delta` inplace to apply the elementwise
derivative of the activation.

Activation functions
--------------------
Provided inplace activation functions (available via the ACTIVATIONS mapping):

- identity
  - Function: inplace_identity(X)
  - Behaviour: leaves X unchanged.

- logistic (sigmoid)
  - Function: inplace_logistic(X)
  - Behaviour: applies the logistic sigmoid elementwise using a stable
    implementation.

- tanh
  - Function: inplace_tanh(X)
  - Behaviour: applies hyperbolic tangent elementwise.

- relu
  - Function: inplace_relu(X)
  - Behaviour: applies rectified linear unit: X <- max(X, 0).

- softmax
  - Function: inplace_softmax(X)
  - Behaviour: computes row-wise K-way softmax in-place in a numerically
    stable fashion:
      1. subtracts the row max,
      2. exponentiates,
      3. normalizes by row sums.

Access: the ACTIVATIONS dictionary maps the activation name (string) to the
corresponding inplace function.

Activation derivatives (inplace)
-------------------------------
Derivative functions modify the backpropagated error signal `delta` in-place
based on the forward-pass activation output `Z`. They are available via the
DERIVATIVES mapping.

- identity
  - Function: inplace_identity_derivative(Z, delta)
  - Behaviour: no change to delta.

- logistic
  - Function: inplace_logistic_derivative(Z, delta)
  - Behaviour: multiplies delta by Z * (1 - Z) (Z is sigmoid output).

- tanh
  - Function: inplace_tanh_derivative(Z, delta)
  - Behaviour: multiplies delta by (1 - Z**2).

- relu
  - Function: inplace_relu_derivative(Z, delta)
  - Behaviour: sets delta entries to zero where Z == 0 (i.e., where ReLU is
    inactive). Note that implementation treats exact zeros in Z as inactive.

Access: the DERIVATIVES dictionary maps the activation name to its derivative
function.

Loss functions
--------------
Loss functions operate on ground-truth arrays and predictions (or predicted
probabilities) and return a scalar loss averaged over samples.

- Squared loss (regression)
  - Function: squared_loss(y_true, y_pred)
  - Behaviour: returns mean squared error divided by 2:
      loss = mean( (y_true - y_pred)**2 ) / 2

- Multiclass / multiclass-probability logistic loss
  - Function: log_loss(y_true, y_prob)
  - Behaviour: computes the negative log-likelihood for label indicator
    matrices and probability predictions. Probabilities are clipped to machine
    epsilon for numerical stability. Works for binary and multi-class cases
    (internally converts single-column probabilities and labels to two-column
    form when necessary).

- Binary logistic loss (for binary / multilabel)
  - Function: binary_log_loss(y_true, y_prob)
  - Behaviour: computes the binary cross-entropy per sample using stable
    clipping; useful in binary and multilabel contexts where `y_prob` is a
    single-column probability array.

Usage notes
-----------
- Arrays are expected to follow shape convention (n_samples, n_features) for
  activations and derivatives; for classification losses `y_prob` typically
  has shape (n_samples, n_classes) (or (n_samples, 1) for binary).
- All modifications in activation and derivative functions are performed
  inplace to minimize allocations; callers should ensure arrays are writable
  and properly shaped.
- Loss functions return the mean loss per sample (suitable for logging,
  optimization objectives after scaling, or comparison across models).

Mappings
--------
- ACTIVATIONS: dict mapping string names to inplace activation callables.
- DERIVATIVES: dict mapping string names to inplace derivative callables.

See the function docstrings for precise parameter descriptions and expected
array shapes.