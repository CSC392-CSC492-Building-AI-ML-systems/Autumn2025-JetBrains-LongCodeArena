Tensor Attributes
=================

Overview
--------
Tensor attributes describe static metadata attached to OneFlow MLIR values and operations. They capture information about the tensor's element type, shape, memory layout, device placement and other properties that are necessary for verification, optimization and code generation. Attributes are immutable values embedded in the IR and are intended to reflect compile-time-known properties; for runtime-varying properties use runtime values instead.

Common attribute categories
---------------------------
- dtype (data type): element type of tensor values (e.g. float32, float16, int32, int64, bool).
- shape: dimensionality information (static sizes and dynamic / unknown dimensions).
- strides: optional explicit memory strides per dimension.
- device: device placement (e.g. "cpu", "cuda:0").
- memory_format: layout shorthand (e.g. "contiguous", "channels_last").
- requires_grad: boolean flag indicating whether gradients should be accumulated.
- named_dims: optional list of names for each dimension.
- storage_offset: offset (in elements) into underlying storage when tensor is a view.
- pin_memory, is_contiguous, is_lazy, is_distributed: additional boolean or tag attributes used by specific passes/operators.

Representation and semantics
----------------------------
- dtype
  - Encoded as an enumerated attribute or named string (implementation-specific).
  - Common element types: float16, float32, float64, int8, int16, int32, int64, bool.
  - May include quantization metadata for quantized tensors when applicable.

- shape
  - Represented as a ranked list of integer sizes.
  - Static dimension: non-negative integer size (e.g. 64).
  - Dynamic / unknown dimension: represented by -1 (or an explicit dynamic token) to indicate that the size is not known at compile time.
  - Zero-rank (scalar) tensors are represented by an empty list [].
  - When shape is absent, the tensor is unranked.

- strides
  - Optional list of integer stride values, one per dimension, indicating the number of elements to step in memory to move to the next index along that dimension.
  - If absent, a canonical row-major (C-contiguous) layout is assumed for storage and layout reasoning.
  - Strides may include negative values for views that represent reversed or transposed memory access.

- device
  - Describes the target device and optional ordinal: examples "cpu", "cuda:0", "cuda:1".
  - Device affinity is a compile-time hint for lowering and placement passes.

- memory_format
  - A lightweight tag describing a layout family such as "contiguous" (default row-major) or "channels_last".
  - Used by passes that optimize layout-sensitive kernels or perform layout conversions.

- requires_grad
  - Boolean attribute that indicates whether gradient tracking and accumulation should be enabled.
  - Typically used by autodiff/gradient-related passes and optimizers.

- named_dims
  - Optional ordered list of strings or placeholders (e.g. ["N", "C", "H", "W"]) mapping to shape dimensions.
  - Unnamed dimensions can be represented by an explicit null/place-holder in the list.

- storage_offset
  - Integer offset (in elements) from the underlying storage base; nonzero for views/subtensors.

Attribute composition and dictionaries
--------------------------------------
Tensor attributes are frequently grouped in attribute dictionaries attached to operations. A compact representation for a tensor value might include multiple attributes:

Example (illustrative):
{
  dtype = "float32",
  shape = [N, 3, H, W],
  device = "cuda:0",
  memory_format = "contiguous",
  requires_grad = false
}

Notes:
- Attributes are immutable in the IR. If a pass changes an attribute (e.g., refines a dynamic dimension to a static size) it must produce a new attribute and update the IR accordingly.
- Missing attributes typically imply default semantics: missing dtype is an error for typed tensor ops; missing shape can mean unranked or unknown-shape tensor; missing strides implies canonical contiguous layout.

Static vs. dynamic reasoning
----------------------------
- Static attributes enable compile-time verification and certain optimizations (layout folding, kernel selection).
- Dynamic or unknown attributes are handled by runtime checks or by deferring transformations until shapes/types are resolved.
- Passes should conservatively propagate and refine attributes where correct: e.g., an operation that concatenates two tensors with static sizes can produce a static output shape when inputs allow it.

Examples
--------
1) Static shape, default contiguous layout:
{
  dtype = "float32",
  shape = [8, 3, 224, 224],
  device = "cpu",
  requires_grad = true
}

2) Dynamic batch dimension:
{
  dtype = "float16",
  shape = [-1, 512],
  device = "cuda:0",
  memory_format = "contiguous"
}

3) Explicit strides for a transposed view:
{
  dtype = "float32",
  shape = [64, 128],
  strides = [1, 64],   // column-major view example
  storage_offset = 0
}

Best practices
--------------
- Prefer expressing only the attributes that are known and necessary. Over-constraining with incorrect attributes can inhibit optimization or lead to verification errors.
- Use dynamic (-1) dimensions when sizes are not known at compile time rather than inventing sentinel numeric sizes.
- When producing views, provide updated shape, strides and storage_offset attributes to preserve correct semantics.
- Keep dtype and device accurate; they are critical for lowering and kernel selection.

API notes (conceptual)
----------------------
- Construction: attributes are typically created via dialect-specific builders or factory utilities in the host API (C++/Python) and attached to ops as named attributes.
- Inspection: transformation and optimization passes query attributes to decide legality of rewrites and to refine representations.
- Refinement: passes can replace an attribute with a stronger/refined attribute (e.g., replace shape [-1, 128] with [32, 128]) by updating the op attribute dictionary.

Compatibility and extension
---------------------------
- The set of tensor attributes is extensible. New attributes should be documented and added conservatively so that older passes tolerate unknown attributes (treat as opaque).
- If an attribute affects semantics (not only optimization), be explicit about its meaning and interaction with existing attributes to avoid ambiguity.

See also
--------
- Dialect-specific operator documentation for how tensor attributes are interpreted by particular ops.
- Shape inference and verification pass documentation for how attributes are used to validate and propagate tensor metadata.