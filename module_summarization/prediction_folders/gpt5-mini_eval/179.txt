Cross decomposition
===================

Cross decomposition methods relate two sets of variables by extracting latent
components that capture the covariance (or correlation) structure between them.
This module provides Partial Least Squares (PLS) and related cross-decomposition
estimators for regression, canonical correlation analysis and SVD-based
decompositions.

Implemented estimators
----------------------

PLSRegression
^^^^^^^^^^^^^

PLS regression finds a linear model by projecting X and Y to a new space of
latent variables (scores) and performing regression in that space. The latent
variables are computed to maximize the covariance between X- and Y-scores,
and the resulting model can be used for prediction of new Y given X.

Key points
- Simultaneously decomposes X and Y into scores and loadings.
- Useful when predictors are many and highly collinear.
- Returns regression coefficients relating original X to Y.

PLSCanonical
^^^^^^^^^^^^

PLS canonical implements a symmetric decomposition of X and Y aimed at
maximizing the correlation between X- and Y-projections. In practice this is
close to canonical correlation analysis (CCA); certain algorithmic ``mode``
choices correspond to the CCA formulation and may involve pseudo-inverses of
covariance matrices.

Key points
- Produces canonical weights and scores for both X and Y.
- Useful for exploring shared structure between two multivariate datasets.

PLSSVD
^^^^^^^

PLS via SVD (PLSSVD) performs a cross-decomposition by computing a singular
value decomposition of the empirical cross-covariance (or cross-correlation)
matrix between X and Y. It is typically equivalent to extracting pairs of
singular vectors of X'Y and can be used as a fast, numerically stable
alternative when the SVD of the cross-covariance is affordable.

Core concepts and outputs
-------------------------

All estimators in this module follow the typical scikit-learn estimator
interface (fit, transform, fit_transform, predict where applicable) and
provide access to the following common attributes after fitting:

- x_mean_, y_mean_ : feature-wise means used for centering.
- x_std_, y_std_ : feature-wise standard deviations used for scaling
  (if scaling is enabled).
- x_weights_, y_weights_ : weight vectors (loadings) used to form scores.
- x_loadings_, y_loadings_ : loadings of original variables on latent
  components.
- x_scores_, y_scores_ : latent scores (projections) of X and Y.
- coef_ : regression coefficients mapping original X to predicted Y
  (available for regression estimators).
- n_components_ : number of extracted components.
- n_iter_ : number of iterations used by iterative algorithms (if iterative
  solvers are used).

Centering and scaling
---------------------

Input matrices X and Y are centered (column-wise). Optionally, each column can
be scaled to unit variance prior to decomposition. When scaling is disabled,
std attributes are set to ones and only centering is applied.

Algorithms and numerical considerations
---------------------------------------

- SVD-based methods compute an SVD of the cross-covariance matrix X^T Y and
  extract the leading singular vectors. This is robust when the cross-covariance
  matrix is not excessively large.
- Power-method / NIPALS-style iterative algorithms can be used as an
  alternative to compute only the first (or a few) singular vector pairs
  without forming the entire SVD. Iterative procedures expose a maximum
  iteration limit and a convergence tolerance and may raise a
  ConvergenceWarning if the maximum number of iterations is reached.
- Certain formulations (for example a CCA-like mode) require pseudo-inverses
  of X^T X or Y^T Y; these can be unstable when the number of features or
  targets exceeds the number of samples. Appropriate regularization or
  dimensionality reduction is recommended in such cases.

Usage notes
-----------

- Choose the number of components according to cross-validation, explained
  variance, or application-specific criteria.
- Use scaling if variables are on different scales or when comparing
  correlation-based relationships is desired; disable if the original
  variable scales are meaningful and should be preserved.
- When working with a single-target Y (univariate response), many PLS
  algorithms converge quickly and computations are simplified.

References
----------

- T. Wold, "PLS regression" — overview of partial least squares concepts and
  applications.
- Wegelin, J., "A survey of partial least squares (PLS) methods, with
  emphasis on the NIPALS algorithm" — discussion of algorithmic modes and
  numerical properties.

Examples
--------

Fit a PLS regression to data and predict:

    >>> from sklearn.cross_decomposition import PLSRegression
    >>> pls = PLSRegression(n_components=2)
    >>> pls.fit(X_train, Y_train)
    >>> Y_pred = pls.predict(X_test)

Transform X (and Y) to latent component space:

    >>> X_scores = pls.transform(X_new)  # projects X to the latent space
    >>> X_scores, Y_scores = pls.transform(X_new), pls.y_scores_

See also
--------

- sklearn.cross_decomposition.PLSRegression
- sklearn.cross_decomposition.PLSCanonical
- sklearn.cross_decomposition.PLSSVD