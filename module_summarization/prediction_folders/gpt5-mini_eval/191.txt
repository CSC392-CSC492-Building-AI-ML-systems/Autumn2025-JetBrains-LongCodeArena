Validation curves: plotting scores to evaluate models
===================================================

A validation curve shows how the training and validation scores of an estimator vary
with a model hyperparameter. It is useful to diagnose whether a model is
underfitting or overfitting and to select good values for hyperparameters.

Compute validation scores
-------------------------
Use :func:`sklearn.model_selection.validation_curve` to compute the scores for a
range of hyperparameter values:

.. code-block:: python

    from sklearn.model_selection import validation_curve
    from sklearn.svm import SVC
    import numpy as np

    param_range = np.logspace(-3, 2, 6)
    train_scores, test_scores = validation_curve(
        SVC(),
        X, y,
        param_name="gamma",
        param_range=param_range,
        cv=5,
        scoring="accuracy",
        n_jobs=1,
    )

The function returns two arrays of shape (n_param_values, n_cv_folds):
train_scores and test_scores. Use their means and standard deviations to plot
central tendencies and uncertainty.

Plotting the validation curve
-----------------------------
A simple matplotlib plot shows the mean score with a band (or error bars)
representing the standard deviation:

.. code-block:: python

    import matplotlib.pyplot as plt

    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    test_mean = test_scores.mean(axis=1)
    test_std = test_scores.std(axis=1)

    plt.figure()
    plt.plot(param_range, train_mean, label="Training score")
    plt.plot(param_range, test_mean, label="Cross-validation score")
    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.3)
    plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.3)
    plt.xscale("log")  # or "linear", depending on the parameter scale
    plt.xlabel("gamma")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.show()

Using a helper display (API note)
---------------------------------
Higher-level visualization helpers follow the scikit-learn visualization API
and:

- accept train_scores and test_scores and plot the mean and uncertainty
  (either with ``fill_between`` or ``errorbar``).
- allow selecting which scores to show (training, test, or both).
- infer an appropriate x-axis scale (log vs linear) when the parameter values
  span several orders of magnitude; you can override this with
  ``ax.set_xscale(...)`` on the returned axes.

Interpreting the curve
----------------------
- Overfitting (high variance): Training score is significantly better than
  validation score. Consider stronger regularization, more training data, or a
  simpler model.
- Underfitting (high bias): Both training and validation scores are low and
  similar. Consider a more complex model or improved features.
- Good model: Training and validation scores are both high and close together.

Scoring name and sign
---------------------
If the scoring function is a loss that is negated (e.g. ``neg_mean_squared_error``),
you may want to negate the scores when plotting so that higher is better. When
reporting labels, replace underscores by spaces and capitalize words to produce
a readable y-axis label (for example, ``neg_mean_squared_error`` -> ``Negative mean squared error``).

Best practices
--------------
- Plot both mean and variability (std or confidence interval) to assess
  stability across folds.
- Plot on a log-scale when the parameter values span multiple orders of
  magnitude.
- Use fine-grained parameter ranges around regions where the validation score
  changes quickly to better locate optimal values.

See also
--------
:func:`sklearn.model_selection.validation_curve`
:func:`sklearn.model_selection.learning_curve`