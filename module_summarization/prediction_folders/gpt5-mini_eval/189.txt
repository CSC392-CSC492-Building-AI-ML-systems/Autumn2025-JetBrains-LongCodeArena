Kernel ridge regression
=======================

Kernel ridge regression (KRR) combines ridge regression (linear least
squares with L2-norm regularization) with the kernel trick. It learns a
linear function in the space induced by the chosen kernel and the data; for
non-linear kernels this corresponds to a non-linear function in the
original feature space.

The model form of KRR is identical to that of support vector regression
(SVR), but KRR uses the squared error loss while SVR uses the
epsilon-insensitive loss. Unlike SVR, KRR admits a closed-form solution
and is typically faster to fit for medium-sized datasets. The resulting
model is non-sparse, so prediction can be slower than sparse SVR models.
KRR supports multi-output regression (y can be a 2D array of shape
[n_samples, n_targets]).

See the User Guide section :ref:`kernel_ridge` for more background.

Parameters
----------

alpha : float or array-like of shape (n_targets,), default=1.0
    Regularization strength; must be a non-negative float. Larger values
    increase regularization (reduce variance). Alpha corresponds to
    ``1 / (2C)`` in some other linear models (e.g.,
    :class:`~sklearn.linear_model.LogisticRegression`, :class:`~sklearn.svm.LinearSVC`).
    If an array is provided, each value is interpreted as the penalty for
    the corresponding target.

kernel : str or callable, default="linear"
    Kernel mapping used internally. Passed directly to
    :func:`~sklearn.metrics.pairwise.pairwise_kernels`. If a string, it
    must be one of the available kernels in
    ``pairwise.PAIRWISE_KERNEL_FUNCTIONS`` or ``"precomputed"``.
    If ``"precomputed"``, X is assumed to be a precomputed kernel matrix
    (shape [n_samples, n_samples]). If a callable is supplied, it should
    accept two 1-D arrays (individual samples) and return a single scalar
    kernel value. Note: callables from :mod:`sklearn.metrics.pairwise`
    operate on matrices and are not suitable here — use the kernel string
    identifiers instead.

gamma : float, default=None
    Kernel coefficient for kernels such as RBF, laplacian, polynomial,
    exponential chi2 and sigmoid. Default behavior is kernel-specific;
    ignored by kernels that do not use gamma.

degree : int, default=3
    Degree for the polynomial kernel. Ignored by other kernels.

coef0 : float, default=1
    Independent term in polynomial and sigmoid kernels. Ignored by other
    kernels.

kernel_params : dict, default=None
    Additional keyword arguments passed to the kernel callable when
    ``kernel`` is callable.

Attributes
----------

dual_coef_ : ndarray of shape (n_samples,) or (n_samples, n_targets)
    Representation of the learned weights in kernel (dual) space. These
    are the coefficients used with the kernel function to make predictions.

X_fit_ : ndarray or sparse matrix of shape (n_samples, n_features)
    Training data required for prediction. If ``kernel == "precomputed"``
    this is the precomputed training kernel matrix of shape
    (n_samples, n_samples).

n_features_in_ : int
    Number of features seen during fit.

feature_names_in_ : ndarray of shape (n_features_in_,)
    Names of features seen during fit, defined only when X has feature
    names that are all strings.

Notes
-----

- KRR fitting reduces to solving a linear system involving the kernel
  (Gram) matrix plus a multiple of the identity matrix (ridge term).
- For large datasets the O(n^3) complexity of solving the system may be
  prohibitive; consider approximate kernel methods or other scalable
  approaches for very large n.
- Multi-target regression is supported by allowing y to be 2D.

See Also
--------

:class:`~sklearn.gaussian_process.GaussianProcessRegressor`
    Gaussian Process regressor providing automatic kernel hyperparameter
    tuning and predictive uncertainty.

:class:`~sklearn.linear_model.Ridge`
    Linear ridge regression (no kernel).

:class:`~sklearn.linear_model.RidgeCV`
    Ridge regression with built-in cross-validation.

:class:`~sklearn.svm.SVR`
    Support Vector Regression accepting a variety of kernels; uses a
    different loss (epsilon-insensitive) and can produce sparse models.

References
----------

Kevin P. Murphy, "Machine Learning: A Probabilistic Perspective", The MIT
Press, chapter 14.4.3, pp. 492–493.

Examples
--------

>>> from sklearn.kernel_ridge import KernelRidge
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> krr = KernelRidge(alpha=1.0)
>>> krr.fit(X, y)
KernelRidge(alpha=1.0)