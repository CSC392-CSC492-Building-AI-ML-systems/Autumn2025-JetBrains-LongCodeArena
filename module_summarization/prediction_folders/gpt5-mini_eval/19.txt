w_crawl — Crawl a WESTPA dataset and run a task for each iteration
=================================================================

Synopsis
--------
w_crawl [OPTIONS] TASK_CALLABLE
  -c, --crawler-instance CRAWLER_INSTANCE

Description
-----------
w_crawl traverses ("crawls") a WESTPA dataset by iteration and runs a user-supplied callable once per iteration. Tasks are dispatched in parallel (one task per iteration) and results are collected on the master process. No ordering guarantees are made for the execution of tasks.

Typical uses
- Postprocessing of trajectories for each iteration.
- Collecting per-iteration statistics.
- Cleanup or conversion of iteration data.
- Any computation that can be expressed as “run this function for iteration N and handle the returned result”.

Main options
- -c, --crawler-instance CRAWLER_INSTANCE
  Specify a crawler instance as module.instance (for example, mypkg.crawlers.MyCrawler.instance). The crawler coordinates initialization, per-iteration-result processing, and finalization. If omitted, a default no-op crawler (WESTPACrawler) is used.

- TASK_CALLABLE (positional)
  The callable to run for each iteration, specified as module.function (for example, mypkg.tasks.process_iteration). This argument is required.

In addition, w_crawl accepts options provided by:
- WESTDataReader (controls which WESTPA data set/backing store is used)
- IterRangeSelection (controls the iteration range to crawl)
- ProgressIndicatorComponent (progress and display controls)
These option groups are added to the command line and control data selection and progress reporting.

Behavior and execution model
----------------------------
- For each iteration n in the selected range [iter_start, iter_stop), w_crawl schedules a remote task that runs:
    task_callable(n, iter_group)
  where iter_group is the iteration group object returned by the WESTPA data manager for iteration n.
- Remote tasks run with the data manager’s backing opened in read-only mode.
- Tasks are dispatched in parallel using the tool’s work manager. Results are returned to the master as they complete.
- There is no guarantee on the order in which iteration tasks are executed or completed.
- On the master, a crawler instance (if provided) receives lifecycle hooks:
  - initialize(iter_start, iter_stop) — called once before tasks are dispatched.
  - process_iter_result(n_iter, result) — called for each completed iteration task with the iteration index and the callable’s returned result.
  - finalize() — called once after all tasks complete (or on error cleanup).

Task callable signature
-----------------------
The TASK_CALLABLE should be importable by worker processes and have the signature:

    result = task_callable(n_iter, iter_group)

- n_iter: integer iteration index.
- iter_group: an object representing the data for that iteration (as returned by the WESTPA data manager).
- The callable may return any picklable result; that result is forwarded to the crawler’s process_iter_result on the master.

Crawler instance API
--------------------
A crawler instance must be a Python object (passed as module.instance) implementing the following methods:

- initialize(iter_start, iter_stop)
  Prepare any state or resources needed before tasks run.

- process_iter_result(n_iter, result)
  Handle the result returned by the task for iteration n_iter. This is executed on the master as tasks complete.

- finalize()
  Final cleanup and resource release after all results have been processed.

If no crawler instance is provided, w_crawl uses a default WESTPACrawler that implements empty no-op methods.

Notes and recommendations
-------------------------
- Because remote tasks open the backing store read-only, task_callables should not rely on modifying backing data. Any required write/aggregation should be done on the master in process_iter_result.
- Ensure TASK_CALLABLE and CRAWLER_INSTANCE are importable by worker processes (use full module paths).
- Tasks are parallelized by iteration; design the task callable to be independent per iteration for correct parallel execution.

Examples
--------
Basic call (use default crawler):
  w_crawl mypackage.tasks.process_iteration

With a crawler instance:
  w_crawl -c mypkg.crawlers.MyCrawler mypkg.tasks.process_iteration

Select iteration range and data via the provided data/iteration options:
  w_crawl [WESTDataReader and IterRangeSelection options] -c pkg.crawler.Instance pkg.tasks.process_iter

Exit status
-----------
The command exits with zero on successful completion; non-zero if an unrecoverable error occurs during initialization, task dispatch/execution, result processing, or finalization.

See also
--------
Components referenced: WESTDataReader, IterRangeSelection, ProgressIndicatorComponent (options for these are available on w_crawl’s command line).