Random projection
=================

A simple and computationally efficient way to reduce the dimensionality of the
data by trading a controlled amount of accuracy (as additional variance) for
faster processing times and smaller model sizes.

Random projections are linear maps that project high-dimensional vectors into a
lower-dimensional Euclidean space using a random matrix. When the random
projection matrix is appropriately scaled and chosen (e.g. Gaussian or sparse
Achlioptas-style), pairwise Euclidean distances between samples are preserved
up to a small multiplicative distortion with high probability. The theoretical
foundation for this is the Johnson–Lindenstrauss lemma.

Johnson–Lindenstrauss lemma
---------------------------

Informally, the Johnson–Lindenstrauss (JL) lemma guarantees that any set of
n points in high-dimensional space can be embedded into a space of dimension
k = O(log n / eps^2) such that all pairwise distances are preserved within a
factor of (1 ± eps). That is, for a projection p and any points u and v:

  (1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2

The function johnson_lindenstrauss_min_dim(n_samples, *, eps=0.1) computes a
safe minimal number of target components k for a given number of samples and
maximum distortion eps by the commonly used bound:

  k >= 4 * log(n_samples) / (eps^2 / 2 - eps^3 / 3)

Usage
-----

Random projection tools include:

- GaussianRandomProjection
  - Dense projection matrix with entries drawn from N(0, 1.0 / n_components).
- SparseRandomProjection
  - Sparse projection matrices (Achlioptas / Li et al. style) that are
    computationally and memory efficient for high-dimensional sparse input.

Common parameters
-----------------

- n_components : int or 'auto'
  - Target dimensionality of the embedding. When 'auto' is used for sparse
    random matrices, a heuristic density of 1 / sqrt(n_features) is applied.
- eps : float in (0, 1)
  - Maximum distortion used with the Johnson–Lindenstrauss bound (for
    estimating a safe n_components).
- density : float in (0, 1] or 'auto'
  - For sparse random matrices, the fraction of non-zero elements per column.
  - 'auto' selects 1 / sqrt(n_features) by default.
- random_state : int, RandomState instance or None
  - Controls random number generation for reproducibility.

When to use random projections
------------------------------

- When you need a fast, simple dimensionality reduction step and can tolerate
  a controlled amount of distortion in pairwise distances.
- When working with very high-dimensional data where more expensive methods
  (e.g. PCA) are impractical.
- For preprocessing before approximate nearest neighbors, clustering, or
  as a dimensionality reduction step prior to training models that scale with
  dimensionality.

Trade-offs and practical considerations
--------------------------------------

- Random projections are data-independent: they do not adapt to the dataset,
  unlike PCA or manifold methods. This makes them very fast but sometimes less
  accurate for preserving structure that depends on data-specific directions.
- The required target dimension depends primarily on the number of samples,
  not the original number of features (see JL lemma).
- SparseRandomProjection is generally preferred for sparse input matrices as
  it yields fast multiplication and lower memory usage; GaussianRandomProjection
  may be preferred when a dense projection is acceptable.
- For very small n_components relative to the number of samples, the
  distortion may be large; use johnson_lindenstrauss_min_dim to estimate a
  reasonable k for a desired eps.

Examples
--------

Compute a safe dimension using the JL bound:

.. code-block:: python

  from sklearn.random_projection import johnson_lindenstrauss_min_dim
  k = johnson_lindenstrauss_min_dim(1e6, eps=0.5)
  # -> 663

Apply Gaussian random projection:

.. code-block:: python

  from sklearn.random_projection import GaussianRandomProjection
  transformer = GaussianRandomProjection(n_components=100, random_state=0)
  X_new = transformer.fit_transform(X)

Apply sparse random projection (suitable for sparse inputs):

.. code-block:: python

  from sklearn.random_projection import SparseRandomProjection
  transformer = SparseRandomProjection(n_components=200, density='auto', random_state=42)
  X_new = transformer.fit_transform(X_sparse)

References
----------

- Johnson, W. B., & Lindenstrauss, J. (1984). Extensions of Lipschitz mappings
  into a Hilbert space. In Conference in modern analysis and probability
  (pp. 189–206).
- Dasgupta, S., & Gupta, A. (1999). An elementary proof of the
  Johnson-Lindenstrauss lemma. (See also: https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)
- Achlioptas, D. (2001). Database-friendly random projections: Johnson–Lindenstrauss with binary coins.
- Li, A., Hastie, T., & Church, K. (2006). Very sparse random projections.

See also
--------

- :func:`johnson_lindenstrauss_min_dim`
- :class:`GaussianRandomProjection`
- :class:`SparseRandomProjection`