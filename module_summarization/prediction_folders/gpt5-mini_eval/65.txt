Calibration of predicted probabilities
=====================================

This tutorial describes probability calibration: why it is needed, common
methods, and how to use the CalibratedClassifierCV estimator to obtain well
calibrated probabilities from an arbitrary classifier.

Overview
--------

Many classification models produce scores that can be interpreted as
probabilities (for example, predict_proba outputs). However, these probabilities
are not always well calibrated: the predicted probability may not reflect the
true empirical frequency. Probability calibration aims to transform raw model
scores into calibrated probabilities that satisfy the interpretation:

  "Among all samples given probability p, roughly a fraction p truly belong to
   the positive class."

Good calibration is important when downstream decisions depend on estimated
probabilities (risk assessment, expected utility, cost-sensitive decisions,
ensembling).

Common methods
--------------

Two widely used calibration approaches are implemented:

- Sigmoid (Platt scaling)
  - Fits a logistic (sigmoid) function mapping scores to probabilities.
  - Parametric, robust, and tends to work well with limited calibration
    samples.
  - Implemented as method='sigmoid'.

- Isotonic regression
  - Non-parametric monotonic mapping from scores to probabilities.
  - More flexible but can overfit on small calibration sets (isotonic should
    generally be avoided when calibration set size << 1000).
  - Implemented as method='isotonic'.

Both methods can be applied on either the decision_function output of the
base estimator (if available) or on predict_proba scores.

When to perform calibration
---------------------------

- If your model's predicted probabilities are biased (e.g. overconfident or
  underconfident), calibration can help.
- Tree ensembles and some discriminative classifiers (e.g. SVMs without
  probabilistic output) commonly benefit from calibration.
- If your base estimator already returns well calibrated probabilities
  (e.g. some logistic regression models are inherently well calibrated),
  calibration may not be necessary or may offer only small improvement.

CalibratedClassifierCV: high-level description
---------------------------------------------

CalibratedClassifierCV wraps a base classifier and learns a calibration model
(sigmoid or isotonic). Key behaviors:

- When cv is not "prefit" (the default), cross-validation is used to obtain
  unbiased predictions for calibration. Two fitting modes are available:
  - ensemble=True (default): For each CV fold a fresh copy of the base
    estimator is trained on the fold's training data and calibrated on the
    fold's test data. Predictions at inference time are the average predicted
    probabilities over all calibrated estimators.
  - ensemble=False: CV is used only to obtain unbiased predictions via
    cross_val_predict which are used to train a single calibrator. At
    inference time the base estimator trained on all data is used together
    with that single calibrator.
- When cv="prefit": it is assumed the provided estimator is already fitted.
  In this mode all provided data is used solely to train the calibrator.
  The user must ensure the data used to calibrate is independent from data
  used to fit the base learner (to avoid biased calibration).
- The calibration model is trained on the estimator's decision_function if
  available, otherwise on predict_proba.
- Multi-class problems are handled by calibrating each class in a one-vs-rest
  fashion (label_binarize is used internally).

Main parameters
---------------

- estimator : estimator object, default=None
  - The base classifier to calibrate. If None, a linear SVM (LinearSVC) is
    used by default (this mimics the historical sklearn behavior).
- method : {'sigmoid', 'isotonic'}, default='sigmoid'
  - Calibration method.
- cv : int, CV generator, iterable or "prefit", default=None
  - Cross-validation splitting strategy. None defaults to 5-fold CV.
  - "prefit" treats the estimator as already fitted and uses all data for
    calibration.
- n_jobs : int, default=None
  - Number of parallel jobs to run when fitting base estimator clones.
- ensemble : bool, default=True
  - Whether to form an ensemble of calibrated estimators (True) or to learn a
    single calibrator from cross-validated predictions and use a single
    estimator trained on all data (False).

Using CalibratedClassifierCV
----------------------------

Simple example: calibrate an SVM using Platt scaling (sigmoid) with 5-fold CV.

.. code-block:: python

    from sklearn.datasets import make_classification
    from sklearn.svm import SVC
    from sklearn.calibration import CalibratedClassifierCV
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import brier_score_loss

    X, y = make_classification(n_samples=1000, n_classes=2, random_state=0)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                        random_state=0)

    base_svc = SVC(kernel='rbf', probability=False, random_state=0)
    calibrated = CalibratedClassifierCV(base_svc, method='sigmoid', cv=5)
    calibrated.fit(X_train, y_train)

    # predict_proba returns calibrated probabilities
    prob_pos = calibrated.predict_proba(X_test)[:, 1]

    print("Brier score:", brier_score_loss(y_test, prob_pos))

Notes:
- We pass probability=False to SVC so we calibrate using decision_function, but
  CalibratedClassifierCV will use predict_proba if decision_function is
  unavailable.
- brier_score_loss measures squared deviation between predicted probability
  and true outcome (lower is better).

Using ensemble=False vs ensemble=True
-------------------------------------

- ensemble=True
  - Fits multiple base estimators (one per CV fold) and calibrators, averaging
    their predicted probabilities at inference. This reduces variance but
    requires storing and evaluating multiple models.
- ensemble=False
  - Uses cross_val_predict to obtain unbiased scores used to train a single
    calibrator. At prediction time a single base_estimator trained on all
    data is used together with this calibrator. More memory efficient.

Example: prefit mode
--------------------

When your classifier is already trained, you can calibrate it directly:

.. code-block:: python

    clf = SomeClassifier().fit(X_train, y_train)
    calibrated = CalibratedClassifierCV(clf, cv="prefit", method="isotonic")
    calibrated.fit(X_calib, y_calib)  # X_calib must be disjoint from training data

Multiclass calibration
----------------------

CalibratedClassifierCV supports multi-class classification. Calibration is
performed in a one-vs-rest fashion for each class label. Example:

.. code-block:: python

    from sklearn.datasets import load_iris
    from sklearn.ensemble import RandomForestClassifier

    X, y = load_iris(return_X_y=True)
    base = RandomForestClassifier(random_state=0)
    cal = CalibratedClassifierCV(base, method='sigmoid', cv=5)
    cal.fit(X, y)

    # predict_proba returns an array of shape (n_samples, n_classes)
    probs = cal.predict_proba(X)

Evaluating calibration
----------------------

Typical tools to evaluate calibration:

- Reliability (calibration) diagram: group predictions into bins and plot
  average predicted probability vs empirical frequency in each bin.
- Brier score: mean squared error between predicted probabilities and truth.
- Log loss: negative log-likelihood, sensitive to confidence.
- Use cross-validation or a held-out calibration set to avoid optimistic bias.

Plotting a calibration curve (reliability diagram) example:

.. code-block:: python

    import matplotlib.pyplot as plt
    from sklearn.calibration import calibration_curve

    prob_pos = calibrated.predict_proba(X_test)[:, 1]
    fraction_of_positives, mean_predicted_value = calibration_curve(y_test,
                                                                    prob_pos,
                                                                    n_bins=10)

    plt.plot(mean_predicted_value, fraction_of_positives, "s-")
    plt.plot([0, 1], [0, 1], "--", color="gray")  # perfectly calibrated
    plt.xlabel("Mean predicted probability")
    plt.ylabel("Fraction of positives")
    plt.title("Reliability diagram")
    plt.show()

Practical tips and pitfalls
---------------------------

- Isotonic regression is flexible and can overfit with small calibration sets.
  If you have few calibration samples prefer method='sigmoid'.
- Ensure the calibration data is representative of the target distribution.
- When cv="prefit", ensure that the data used for calibration is disjoint
  from the data used to train the base estimator to avoid optimistic
  calibration.
- Evaluate calibration on held-out data or via nested cross-validation to
  avoid overfitting calibration parameters.
- For highly imbalanced problems calibrating per-class probabilities and
  evaluating calibration with appropriate metrics (e.g. per-class Brier
  scores) is recommended.
- Parallelization (n_jobs) speeds up fitting when ensemble=True and cv
  produces multiple independent fits.

Advanced notes
--------------

- CalibratedClassifierCV decides internally whether to use decision_function or
  predict_proba of the base estimator. For some estimators (e.g. SVC without
  probability=True) decision_function is available and often preferable.
- For multi-class tasks calibration is applied on a one-vs-rest basis using
  label binarization.
- The reliability of the calibration depends on the quality and size of the
  calibration set; using more calibration samples generally improves isotonic
  calibration.
- Common evaluation metrics: Brier score (squared error), log loss, and
  calibration curve visual inspection.

References
----------

- Platt, J. (1999). Probabilistic Outputs for Support Vector Machines and
  Comparisons to Regularized Likelihood Methods.
- Zadrozny, B. & Elkan, C. (2001). Obtaining calibrated probability estimates
  from decision trees and naive Bayesian classifiers.

See also
--------

- calibration_curve and plotting utilities (reliability diagrams).
- sklearn.model_selection.cross_val_predict for generating unbiased
  predictions used for calibrating models when ensemble=False.
- IsotonicRegression for the isotonic mapping used under method='isotonic'.

This tutorial illustrated common workflows for turning raw model scores into
calibrated probabilities using CalibratedClassifierCV. Use held-out data and
appropriate evaluation metrics (Brier score, log loss, calibration plots) to
assess the effect of calibration.