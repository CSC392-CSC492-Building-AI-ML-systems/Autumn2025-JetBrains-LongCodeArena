Tuning the hyper-parameters of an estimator
==========================================

Hyper-parameter tuning is the process of selecting the set of parameters that
maximizes the performance of an estimator on a held-out dataset. In
scikit-learn this is commonly performed by searching over a grid or a random
sampling of parameter settings and evaluating each candidate with
cross-validation. The main tools are ``GridSearchCV`` and ``RandomizedSearchCV``.
Use cross-validation to avoid overfitting to a single train / test split.

Grid search
-----------

Grid search exhaustively evaluates all candidates from a parameter grid.

Parameters of interest
- ``estimator`` : object implementing ``fit``. The estimator to tune.
- ``param_grid`` : dict or list of dicts. Keys are parameter names (see note on
  nested parameters below) and values are lists of parameter settings to try.
- ``scoring`` : string, callable, or dict, optional. Defines the metric used to
  evaluate performance. For multi-metric evaluation, pass a dict of scorers.
- ``cv`` : int, cross-validation generator, or iterable, optional. Determines the
  cross-validation splitting strategy.
- ``refit`` : bool or str, default=True. If True, refits the estimator on the
  whole dataset using the best found parameters. If string, refits using the
  scorer name in multi-metric setting.
- ``n_jobs`` : int, optional. Number of jobs to run in parallel.
- ``verbose`` : int, optional. Controls the verbosity.
- ``return_train_score`` : bool, default=False. If True, training scores are also
  computed and stored in ``cv_results_``.
- ``error_score`` : float or "raise", optional. Value assigned to the score when
  a parameter setting fails.

Attributes after fit
- ``best_estimator_`` : estimator refit on the whole dataset.
- ``best_params_`` : dict, parameter setting with highest score.
- ``best_score_`` : float, best cross-validated score.
- ``cv_results_`` : dict, detailed results for each candidate (means, stds,
  parameter settings, timings, etc.).

Example
^^^^^^^

.. code-block:: python

    from sklearn.model_selection import GridSearchCV
    from sklearn.svm import SVC

    param_grid = {
        'C': [0.1, 1, 10],
        'kernel': ['linear', 'rbf'],
        'gamma': ['scale', 'auto']
    }
    search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    search.fit(X_train, y_train)
    print(search.best_params_, search.best_score_)

Randomized search
-----------------

Randomized search samples parameter settings from given distributions. It is
typically much faster than grid search for large or continuous parameter
spaces and allows specifying the number of parameter settings to try.

Key parameters
- ``param_distributions`` : dict or list of dicts. Values are distributions
  (e.g. from scipy.stats) or lists to sample from.
- ``n_iter`` : int. Number of parameter settings sampled.
- Other parameters (``scoring``, ``cv``, ``refit``, ``n_jobs``) are the same as
  for ``GridSearchCV``.

Example
^^^^^^^

.. code-block:: python

    from sklearn.model_selection import RandomizedSearchCV
    from scipy.stats import loguniform

    param_dist = {'C': loguniform(1e-3, 1e3), 'gamma': loguniform(1e-4, 1e1)}
    rand_search = RandomizedSearchCV(SVC(), param_dist, n_iter=50, cv=5,
                                     scoring='roc_auc', n_jobs=-1, random_state=0)
    rand_search.fit(X_train, y_train)

Parameter naming and pipelines
------------------------------

When tuning parameters of an estimator wrapped in a pipeline, use the pipeline
step name followed by two underscores and the parameter name, e.g.:
``pipeline__clf__C``. When passing a list of parameter grids to search over,
each dict can set parameters for different pipelines or estimators.

Scoring and multi-metric evaluation
----------------------------------

- ``scoring`` may be a single scorer name or callable, or a dict of scorers for
  multi-metric evaluation.
- For multi-metric searches, set ``refit`` to the scorer name to refit the
  estimator using that metric, or ``False`` to avoid refitting.
- When ``return_train_score=True``, training set scores are stored in
  ``cv_results_`` for diagnosing overfitting.

Diagnostics and visualization
-----------------------------

Use learning and validation curves to better understand generalization behavior
and the effect of hyper-parameters.

- validation_curve: evaluates training and validation scores for different
  values of a single hyper-parameter while holding others fixed. Useful to
  detect under- or overfitting as a function of a parameter.
- learning_curve: evaluates training and validation scores as a function of
  training set size. Useful to determine whether more data would help.

The visualization utilities (for example, ``LearningCurveDisplay`` and the
validation curve display) help plot mean scores and variability (standard
deviation) across folds. These displays support showing scores with shaded
confidence regions or error bars and can annotate axis scales automatically
(e.g., log-scale when the parameter range spans many orders of magnitude).

Practical tips
--------------

- Prefer ``RandomizedSearchCV`` for large or continuous parameter spaces and
  set ``n_iter`` according to computational budget.
- Use coarse-to-fine strategies: start with broad ranges and few candidates,
  then narrow ranges around the best settings and search again.
- Use informative cross-validation splits (e.g., stratified splits for
  classification, group splits for grouped data).
- When computational cost is high, consider early-stopping/iterative or
  successive halving search strategies (e.g., halving grid/random search) to
  allocate resources efficiently.
- Monitor training vs validation scores (via learning/validation curves) to
  decide whether to collect more data, regularize more, or increase model
  complexity.

See also
--------

- sklearn.model_selection.GridSearchCV
- sklearn.model_selection.RandomizedSearchCV
- sklearn.model_selection.validation_curve
- sklearn.model_selection.learning_curve
- LearningCurveDisplay (visualization)