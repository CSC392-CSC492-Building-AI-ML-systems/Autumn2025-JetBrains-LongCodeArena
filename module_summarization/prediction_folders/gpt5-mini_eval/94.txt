# Stateless Learning Rate Schedulers (scale_schedule.md)

## Overview
This module provides a stateless scheduler API for learning-rate scheduling that integrates with Composer's Time abstraction. Schedulers are pure callables that compute a scalar multiplier (α) to apply to an optimizer's base learning rate. Because schedulers are stateless and driven by the external `State`, they can be composed, scaled, and converted into standard PyTorch schedulers for use with optimizers.

Key design goals:
- Use explicit time units (epochs, batches, duration).
- Allow arbitrary scheduler callables (functions or callable objects).
- Support scale schedule ratio (SSR) to stretch/shrink schedules.
- Provide conversion to a PyTorch-compatible scheduler object.

## ComposerScheduler (interface)
A Composer scheduler is any callable matching the signature:

(state: State, ssr: float = 1.0) -> float

- `state` — the current `composer.core.State`, including `timestamp`, `max_duration`, and (when needed) `dataloader_len`.
- `ssr` — scale schedule ratio (default 1.0). The scheduler should behave such that:
  ασ(t) = α(t / σ)
  i.e., an SSR of σ stretches the schedule by σ.

Return value:
- A float multiplier α that will multiply the optimizer's configured learning rate: η(t) = η_init × α(t).

Notes:
- Multiple schedulers may be applied; their multipliers stack multiplicatively.
- Schedulers should be pure functions of `state` and `ssr` (no internal mutable state required).
- A scheduler may be implemented as a plain function or as a callable class.

Example (conceptual):
```python
def ten_epoch_decay(state: State, ssr: float = 1.0) -> float:
    if state.timestamp.epoch < 10:
        return 1.0
    return 0.5
```

## Scale Schedule Ratio (SSR)
- SSR modifies the effective time axis of a scheduler. With SSR = s, the scheduler at time t should produce the same multiplier as the unscaled scheduler at time t / s.
- This enables stretching or compressing schedules without rewriting them.

## Time conversion: _convert_time
_converts time specifications into a `Time[int]` anchored to the training `state` and SSR._

Signature:
- _convert_time(time: Union[str, Time[int], Time[float]], state: State, ssr: float = 1.0) -> Time[int]

Behavior:
- Accepts a `Time` instance or a time string (parsed via `Time.from_timestring`).
- Requires `state.max_duration` to be set; raises an assertion otherwise.
- If `time.unit` is `TimeUnit.DURATION`, converts to the same unit as `state.max_duration`:
  - If `state.max_duration` is in epochs and `dataloader_len` is available, converts DURATION → BATCH using dataloader length.
- If `time.unit` is `TimeUnit.EPOCH`, converts to batches (epochs lack sufficient granularity for SSR) using `dataloader_len`.
- After unit conversion, applies SSR by multiplying the numeric value by `ssr`.
- Raises an error if conversions require `dataloader_len` but it is `None`.

This helper ensures scheduler definitions written in epochs, duration, or batch units are reconciled to the trainer state and SSR for consistent computation.

## compile_composer_scheduler
Converts a stateless Composer scheduler into a PyTorch-style scheduler object (a `PyTorchScheduler` / `torch.optim.lr_scheduler` wrapper) that can be stepped via `.step()` while referencing the live `State`.

Behavior & notes:
- Binds the provided Composer scheduler to the `state` and `ssr` so that when the returned scheduler's `.step()` is called, it computes the current multiplier from the Composer scheduler and applies it to the optimizer learning rates.
- Because the Composer scheduler is stateless and relies on `state`, internal state maintained by the PyTorch scheduler is ignored — the authoritative time/progress source is `state`.
- Requires `state.max_duration` to be set when called (the underlying helpers rely on it).

Typical use:
- Convert function-or-callable schedulers to a `LambdaLR`-like wrapper that queries the Composer scheduler each step and sets param-group lrs accordingly.

## Built-in scheduler types (summary)
The module exposes a set of commonly used scheduler patterns (names shown below). Each returns a multiplier α(t) ∈ ℝ≥0 based on the current `State` and supports SSR. Warmup variants combine a warmup phase followed by the corresponding base schedule.

- ConstantScheduler
  - Always returns 1.0 (or a fixed multiplier). Useful as a no-op or baseline.

- StepScheduler
  - Multiply-by-step decay: reduce the multiplier by a factor at regular step intervals.

- MultiStepScheduler
  - Decays the multiplier at specified milestone times.

- LinearScheduler
  - Linearly interpolates the multiplier between start and end values over a specified interval.

- ExponentialScheduler
  - Exponentially decays the multiplier with a given rate.

- CosineAnnealingScheduler
  - Cosine decay from initial multiplier to a final multiplier over a period (classic cosine annealing).

- CosineAnnealingWarmRestartsScheduler
  - Cosine annealing with scheduled restarts (periodic restarts).

- PolynomialScheduler
  - Polynomial decay α(t) = (1 − t/T)^power, or similar polynomial forms.

Warmup variants:
- ConstantWithWarmupScheduler
- LinearWithWarmupScheduler
- CosineAnnealingWithWarmupScheduler
- PolynomialWithWarmupScheduler
- MultiStepWithWarmupScheduler

Warmup behavior:
- Typically, a warmup phase increases α from a small value (often 0 or a specified start) to 1.0 over a configurable warmup duration (which can be given in epochs, batches, or duration). After warmup, the base schedule takes over. All warmup durations are subject to the same unit conversion and SSR rules described above.

## Usage patterns
- Define a scheduler as a function or callable class that reads `state.timestamp` (and optionally `state.max_duration`, `dataloader_len`) and returns a multiplier.
- Provide schedulers to the Trainer or convert them with `compile_composer_scheduler` when direct PyTorch scheduler compatibility is required.
- Use SSR to stretch/compress schedules globally without changing the scheduler definition.

Example (conceptual):
```python
# function scheduler
def my_scheduler(state: State, ssr: float = 1.0) -> float:
    # compute α based on state.timestamp (epoch/batch/duration)
    ...

# compile into a PyTorch-compatible scheduler bound to state
torch_scheduler = compile_composer_scheduler(my_scheduler, state, ssr=1.0)
```

## Errors & Preconditions
- `state.max_duration` must be set when schedulers are evaluated or when `_convert_time` / `compile_composer_scheduler` are used.
- When converting epoch-based or duration-based times to batches, `state.dataloader_len` must be available; otherwise conversion will raise an error.

## Implementation notes
- Schedulers are intentionally stateless: all time/progress comes from `State`. This simplifies composition, reproducibility, and SSR scaling.
- The module bridges high-level time-based scheduler definitions and low-level optimizer learning-rate updates via a lightweight wrapper that queries the Composer scheduler and applies the returned multiplier to optimizer parameter groups.

For concrete parameter/argument semantics of the built-in scheduler classes, consult their specific constructors and docstrings (each supports specifying start/end times, multipliers, decay rates, milestones, warmup duration, and units).