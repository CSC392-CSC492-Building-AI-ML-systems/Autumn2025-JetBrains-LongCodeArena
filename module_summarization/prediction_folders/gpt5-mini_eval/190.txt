Linear and Quadratic Discriminant Analysis
=========================================

Overview
--------
Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)
are classical parametric methods for supervised classification based on the
assumption that the class-conditional feature distributions are Gaussian.
Both estimate class priors and class-conditional densities and use Bayes'
rule to form a classifier. The difference is in the assumed covariance
structure:

- LDA assumes all classes share the same covariance matrix. This yields
  linear decision boundaries (linear in the input features).
- QDA allows each class to have its own covariance matrix. This yields
  quadratic decision boundaries (quadratic in the input features).

Because LDA pools covariance information across classes, it is lower
variance (fewer parameters) and can perform better when the common-covariance
assumption is reasonable. QDA is more flexible but requires more training
data to estimate separate covariance matrices reliably.

When to use
-----------
- Use LDA when class-conditional Gaussians are plausible and classes share
  similar covariance structure, or when the number of features is large
  relative to the number of training samples (LDA is more stable).
- Use QDA when classes are believed to have distinctly different covariance
  structures and there is sufficient data to estimate separate covariances.

Mathematical formulation
------------------------
Let x be a feature vector and consider K classes with priors π_k, means μ_k,
and covariance matrices Σ_k. Under Gaussian class-conditional models:

p(x | y = k) = N(x | μ_k, Σ_k)

The log-posterior (up to an additive constant independent of k) is:

log π_k − 1/2 (x − μ_k)^T Σ_k^{-1} (x − μ_k) − 1/2 log |Σ_k|

- For QDA each Σ_k is used directly; the resulting discriminant is a
  quadratic function of x.
- For LDA, Σ_k = Σ (shared for all k). Terms quadratic in x that are common
  across classes cancel when comparing classes, producing a linear
  discriminant of the form w_k^T x + b_k.

Key parameters and options
--------------------------
Typical parameters available in common implementations:

- priors: array-like or None
  - Prior probabilities of the classes. If None, class priors are inferred
    from the training data.
- store_covariance (LDA/QDA): bool
  - If True, the estimated covariance(s) are stored as attributes.
- solver / shrinkage (LDA):
  - Some implementations offer different solvers (e.g. 'svd', 'lsqr',
    'eigen') and an option for shrinkage of the covariance estimate. Shrinkage
    (analytical or numeric) can improve performance when data are
    high-dimensional or sample sizes are small.
- reg_param (QDA):
  - Regularization parameter that shrinks each class covariance matrix
    toward the diagonal to improve numeric stability and reduce variance.
- tol:
  - Tolerance for convergence where applicable.

Common attributes after fitting
-------------------------------
- classes_: array, shape (n_classes,)
  - Unique class labels.
- priors_: array, shape (n_classes,)
  - Estimated class priors.
- means_: array, shape (n_classes, n_features)
  - Estimated class means.
- covariance_ (LDA): array, shape (n_features, n_features)
  - Pooled covariance estimate (if stored).
- covariances_ (QDA): list or array, shape (n_classes, n_features, n_features)
  - Per-class covariance estimates (if stored).
- scalings_ (LDA):
  - Linear coefficients used to project input data onto the linear
    discriminants (useful for dimensionality reduction).
- explained_variance_ratio_ (implementation dependent):
  - When LDA is used as a transformer (fisher projection), the variance
    explained by each discriminant direction.

API and usage examples
----------------------
Basic usage (illustrative):

Fit and predict with LDA::

    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    clf = LinearDiscriminantAnalysis(priors=None, solver='svd', store_covariance=False)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    proba = clf.predict_proba(X_test)     # posterior probabilities
    X_proj = clf.transform(X)             # project onto linear discriminants

Fit and predict with QDA::

    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
    clf = QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=False)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    proba = clf.predict_proba(X_test)

Notes and practical considerations
---------------------------------
- Normality: LDA and QDA assume Gaussian class-conditional distributions.
  Violations of this assumption can degrade performance.
- Covariance estimation: Poor estimates of covariance matrices (especially
  in high-dimensional settings with small samples) harm performance. Use
  shrinkage (LDA) or regularization (QDA reg_param) where appropriate.
- Dimensionality reduction: LDA can be used to reduce dimensionality by
  projecting data onto the space spanned by the (K − 1) linear discriminants
  (Fisher’s discriminant directions).
- Numerical stability: For QDA, ensure covariance matrices are well-conditioned
  (regularize or increase reg_param when necessary).
- Multi-class: Both methods naturally extend to multiclass classification
  via the Gaussian model and one discriminant per class.

Advantages and limitations
--------------------------
Advantages
- Interpretable: parameters (means, covariances, priors) have clear
  statistical meaning.
- Efficient: closed-form estimates for parameters; very fast to train.
- Effective for well-separated Gaussian classes.

Limitations
- Strong distributional assumptions (Gaussianity).
- QDA requires estimating many parameters (one covariance per class),
  making it data-hungry.
- Sensitivity to outliers unless robust preprocessing is applied.

References
----------
- R. A. Fisher, "The use of multiple measurements in taxonomic problems",
  Annals of Eugenics, 1936 (Fisher’s Linear Discriminant).
- T. Hastie, R. Tibshirani, J. Friedman, "The Elements of Statistical Learning",
  Springer (discussions on LDA and QDA).
- C. M. Bishop, "Pattern Recognition and Machine Learning", Springer.

See also
--------
- Logistic regression — another linear classifier that models posterior
  probability directly without Gaussian class-conditional assumptions.
- Regularized discriminant analysis — approaches that interpolate between
  LDA and QDA via covariance regularization.