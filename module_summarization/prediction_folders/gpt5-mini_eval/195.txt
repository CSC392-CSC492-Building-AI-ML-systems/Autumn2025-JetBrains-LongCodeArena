Learn Gaussian Mixture Models
=============================

Overview
--------
A Gaussian mixture model (GMM) represents the distribution of data as a weighted
sum of multivariate Gaussian components. Parameters are estimated using the
Expectationâ€“Maximization (EM) algorithm: responsibilities (posterior
probabilities of components given data) are computed in the E-step and
component parameters are updated in the M-step. This implementation provides a
base class for mixture models that encapsulates common utilities and fitting
behavior used by concrete GMM implementations.

Key concepts
------------
- n_components: number of mixture components.
- Responsibilities (resp): an array of shape (n_samples, n_components)
  containing the posterior probability that each sample belongs to each
  component. Many initialization strategies create an initial resp matrix.
- EM algorithm: iterative E-step and M-step until the change in the (lower)
  bound / log-likelihood is below tol or max_iter is reached.
- Numerical stability: log-sum-exp is used when aggregating log-probabilities.

Constructor parameters
----------------------
The base class exposes the following parameters (common to derived GMMs):

- n_components (int)
  Number of mixture components.

- tol (float)
  Convergence threshold. EM stops when the change in the (lower) bound is
  less than tol.

- reg_covar (float)
  Non-negative regularization added to the diagonal of covariance matrices to
  ensure numerical stability.

- max_iter (int)
  Maximum number of EM iterations to perform per initialization.

- n_init (int)
  Number of initializations to perform. The parameters giving the largest
  likelihood (or lower bound) are retained.

- init_params (str)
  Strategy to initialize responsibilities / parameters. Supported options:
  - "kmeans": initialize component assignments by fitting KMeans.
  - "random": initialize responsibilities with random values (normalized).
  - "random_from_data": choose n_components distinct samples as initial
    component indicators.
  - "k-means++": choose initial component centers using k-means++ seeding.

- random_state
  Seed or RandomState for reproducible initialization and algorithms (e.g.
  k-means++).

- warm_start (bool)
  If True, reuse the solution of the previous call to fit as initialization
  for subsequent calls. When warm_start is True, n_init is ignored and only a
  single initialization is performed on the first call; subsequent calls
  continue from the previous solution.

- verbose (bool) and verbose_interval (int)
  Control verbosity of EM iteration logging.

Initialization methods
----------------------
Initialization prepares the responsibilities matrix resp (shape
(n_samples, n_components)), from which derived classes initialize component
parameters:

- kmeans: run KMeans with n_clusters == n_components, set resp to the hard
  one-hot assignments from labels.

- random: draw resp from a uniform distribution and normalize rows to sum to 1.

- random_from_data: select n_components distinct samples and set the resp
  rows for those samples to be one-hot (each chosen sample seeds a component).

- k-means++: use k-means++ seeding to select n_components data points as
  component seeds and set corresponding resp rows to one-hot.

These initialization routines call an implementation-specific _initialize(X, resp)
method to set the model parameters from resp.

Fitting and prediction
----------------------
- fit(X): fits the model to data X using fit_predict and returns the fitted
  estimator. Input X must be an array-like with shape (n_samples, n_features).

- fit_predict(X): fits the model (running up to n_init restarts unless
  warm_start=True) using the EM algorithm and returns the most probable
  component label for each sample (array of shape (n_samples,)).

Behavioral notes
----------------
- The EM loop is run for up to max_iter iterations per initialization. If the
  improvement in log-likelihood / lower bound across iterations is smaller
  than tol, EM is considered converged. If convergence is not reached within
  max_iter, a ConvergenceWarning is issued.

- When n_init > 1, the fit procedure keeps the solution with the highest
  (lower bound) score across random restarts.

- The base class performs common validation (input dtype, minimum number of
  samples) and delegates model-specific parameter checks to _check_parameters(X).

- For numerical stability when summing log-probabilities, logsumexp is used.

Developer hooks
---------------
Derived mixture classes should implement:

- _check_parameters(self, X)
  Validate any model-specific initial parameters (called before fitting).

- _initialize(self, X, resp)
  Initialize model-specific parameters (weights, means, covariances, etc.)
  from the responsibilities matrix resp.

Utilities
---------
- _check_shape(param, param_shape, name): helper to validate that an array
  parameter has the expected shape; raises ValueError on mismatch.

- kmeans_plusplus: used when init_params == "k-means++" to select initial
  centroids.

- check_random_state / random_state: used to control reproducibility.

Example (conceptual)
--------------------
Given a derived GaussianMixture class that implements _initialize and the
EM steps, typical usage is:

1. Instantiate with desired number of components and settings (n_components,
   tol, max_iter, init_params, random_state, n_init, ...).
2. Call fit(X) to estimate parameters.
3. Use predict(X) / fit_predict(X) to obtain component labels or predict_proba
   to obtain responsibilities.

See concrete implementations for attributes exposed after fit (weights,
means, covariances, converged flag, lower bound and number of iterations).