random_projection module
========================

Random Projection transformers
------------------------------

Random Projections provide a simple and computationally efficient way to
reduce the dimensionality of data by trading a controlled amount of
accuracy (expressed as additional variance) for faster processing and
smaller model sizes. Random projection matrices are constructed so as to
approximately preserve pairwise distances between samples; this property
is formalized by the Johnson–Lindenstrauss lemma.

The module implements functions and transformers to create such
projections (dense Gaussian or sparse Achlioptas-style), helpers for
parameter validation, and a utility to compute safe lower bounds on the
projection dimensionality.

Key concepts
------------
- Johnson–Lindenstrauss lemma: a small set of points in a high‑
  dimensional space can be embedded into a much lower dimensional space
  while approximately preserving pairwise Euclidean distances. The random
  linear map used for the embedding can be taken to be a (scaled)
  Gaussian projection or appropriately constructed sparse matrices.

- eps-embedding: a projection p is an eps-embedding for points u and v if
  (1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2.

Available public API
--------------------

.. list-table::
   :widths: 20 80
   :header-rows: 0

   * - Function
     - Description
   * - johnson_lindenstrauss_min_dim
     - Compute a safe minimal number of components (n_components) required
       to guarantee an eps-embedding with high probability for a dataset
       of a given number of samples. Accepts scalar or array-like inputs
       for n_samples and eps and returns an int or array of ints.
   * - GaussianRandomProjection
     - Transformer that projects data to a lower-dimensional space using a
       dense Gaussian random matrix. The matrix entries are drawn from a
       normal distribution with variance scaled by 1 / n_components so
       that pairwise distances are approximately preserved.
   * - SparseRandomProjection
     - Transformer that projects data using a sparse random matrix
       (Achlioptas-style / Li et al. sparsification). Useful when the
       original data is sparse or when memory/speed trade-offs favor sparse
       random matrices.

Module helpers (internal)
-------------------------
The module also exposes several helper routines (primarily for internal
use):

- _check_density(density, n_features)
  - Validate and normalize the requested density parameter for a sparse
    random matrix. Accepts "auto" (which defaults to 1 / sqrt(n_features))
    or a float in (0, 1]. Raises ValueError for invalid densities.

- _check_input_size(n_components, n_features)
  - Validate that n_components and n_features are strictly positive.

- _gaussian_random_matrix(n_components, n_features, random_state=None)
  - Generate a dense Gaussian random matrix of shape
    (n_components, n_features) with entries drawn from
    N(0, 1.0 / n_components). Intended for constructing the projection
    matrix used by GaussianRandomProjection.

johnson_lindenstrauss_min_dim
-----------------------------
Signature:
  johnson_lindenstrauss_min_dim(n_samples, *, eps=0.1)

Description:
  Compute a conservative lower bound on the number of target dimensions
  (n_components) required to produce an eps-embedding for n_samples.
  The bound is given by:

    n_components >= 4 * log(n_samples) / (eps^2 / 2 - eps^3 / 3)

  This value depends on the number of samples but not on the original
  number of features.

Parameters:
  - n_samples: int or array-like of int
      Number of samples (must be > 0). If array-like, the function
      computes the bound element-wise.
  - eps: float in (0, 1) or array-like
      Maximum acceptable distortion (0 < eps < 1). If array-like, the
      bound is computed element-wise.

Returns:
  - int or ndarray of int
      Minimal number of components (or array of such minima) to guarantee
      an eps-embedding with high probability.

Errors:
  - ValueError if eps is not in (0, 1) or if n_samples <= 0.

Examples
--------
>>> from sklearn.random_projection import johnson_lindenstrauss_min_dim
>>> johnson_lindenstrauss_min_dim(1e6, eps=0.5)
663

>>> johnson_lindenstrauss_min_dim(1e6, eps=[0.5, 0.1, 0.01])
array([    663,   11841, 1112658])

>>> johnson_lindenstrauss_min_dim([1e4, 1e5, 1e6], eps=0.1)
array([ 7894,  9868, 11841])

Notes
-----
- The theoretical bound returned by johnson_lindenstrauss_min_dim is a
  conservative estimate; in practice, smaller target dimensionalities
  may suffice for many datasets and downstream tasks, but the guarantee
  provided by the lemma holds for the returned bound.

- GaussianRandomProjection uses a dense matrix with entries drawn from a
  normal distribution scaled by 1 / n_components. SparseRandomProjection
  constructs sparse matrices following the sparsification schemes
  described in the literature (e.g., Achlioptas, Li et al.).

- The module integrates with scikit-learn's estimator/transformer design:
  the projection classes follow the fit/transform API, accept a
  random_state for reproducibility, and expose learned components as an
  attribute (typically components_).

References
----------
[1] Johnson, W. B., Lindenstrauss, J. (1984). Extensions of Lipschitz mappings
    into a Hilbert space. Contemporary Mathematics.

[2] Dasgupta, S., & Gupta, A. (1999). An elementary proof of the
    Johnson-Lindenstrauss lemma.
    https://citeseerx.ist.psu.edu/doc_view/pid/95cd464d27c25c9c8690b378b894d337cdf021f9

[3] Achlioptas, D. (2003). Database-friendly random projections: Johnson–
    Lindenstrauss with binary coins.

See also
--------
- User Guide sections on random projections and random-matrix-based
  dimensionality reduction for practical guidance and implementation
  details (including trade-offs between dense Gaussian and sparse
  constructions).