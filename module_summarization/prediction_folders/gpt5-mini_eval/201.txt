Partial Dependence and Individual Conditional Expectation
=======================================================

Overview
--------
Partial dependence (PD) and individual conditional expectation (ICE) plots are model-agnostic tools for visualizing the relationship between one or more features and the predicted response of a supervised learning model.

- Partial dependence shows the average effect of a feature (or feature pair) on the predicted outcome by marginalizing over the joint distribution of the other features.
- ICE plots show the dependence for individual samples, revealing heterogeneity in feature effects and potential interactions that are masked by the average PD curve.

These tools are useful for interpretation and debugging of complex models (e.g., tree ensembles, neural networks), but must be used with care when features are correlated.

Definitions and computation
---------------------------
Given a trained model f and features X = (X_S, X_C) partitioned into a set of interest S and its complement C, the partial dependence function for S is

    PD_S(x_S) = E_{X_C}[ f(x_S, X_C) ]

In practice the expectation is approximated by averaging predictions over the empirical distribution of X_C:

    PD_S(x_S) ≈ (1 / n) ∑_{i=1..n} f(x_S, x_{C,i})

for each grid point x_S considered for the features in S. For a single feature this yields a curve; for a pair of features this yields a surface/contour.

An ICE curve for sample i is

    ICE_i(x_S) = f(x_S, x_{C,i})

so ICE plots visualize ICE_i(x_S) for many (or all) i. Centered ICE (c-ICE) plots subtract a reference value (e.g., ICE_i at the left-most grid point) to emphasize shape differences between curves.

When to use PD vs ICE
----------------------
- Use PD to summarize the average effect of a feature. PD is compact and easy to read for global interpretation.
- Use ICE to detect heterogeneity across samples, non-additive interactions, and situations where averaging hides divergent behavior.
- Compare PD and ICE together: consistent ICE curves (parallel shifts) imply that PD is representative; crossing ICE curves indicate interactions or heterogeneous effects.

Practical considerations and cautions
-------------------------------------
- Correlated features: PD assumes that features are independent when filling in values for x_S. When features are strongly correlated, the marginalization step can evaluate f at unlikely or impossible feature combinations, leading to misleading PD/ICE plots. Consider conditional approaches, partialing restricted to plausible ranges, or alternative interpretation techniques when correlations are present.
- Interactions: PD for a single feature averages out interactions. Plotting pairwise PD (2D) or ICE can reveal interactions.
- Grid resolution: Choose a sufficiently fine grid for continuous features; typical defaults are 10–100 points. Higher resolution increases computation.
- Categorical features: Use the distinct categories as grid values; ordering is not required.
- Centering ICE: Centered ICE (subtracting a baseline per-sample value) helps reveal differences in curve shapes rather than level shifts.
- Computational cost: Computing PD/ICE requires many model evaluations (grid size × n_samples). Subsampling the dataset or using a representative sample (max_samples) can reduce cost. Parallel computation is often supported.
- Uncertainty: PD/ICE do not by themselves provide statistical uncertainty estimates. Use resampling (bootstrap) or compute variance of ICE curves to obtain uncertainty bands.

How to read plots
-----------------
- 1D PD: the curve shows how the model prediction changes with the feature on average. Slope and curvature indicate monotonicity and marginal effects.
- 1D ICE: a family of curves, one per sample. Parallel curves shifted vertically suggest additive effects; crossing curves indicate interactions or different conditional behaviors.
- Centered ICE: emphasizes differences in shape by removing vertical offsets.
- 2D PD: contour/heatmap shows joint marginal effect of two features. Interpret with caution for correlated features.
- Overplot PD on top of ICE: the PD curve overlays the mean of ICE curves and helps contrast global vs individual effects.

Options commonly available in implementations
----------------------------------------------
Implementations of PD/ICE plotting typically expose options such as:
- estimator: fitted model with predict (or predict_proba) method.
- X: data used to marginalize over the complementary features.
- features: index(es) or name(s) of feature(s) to plot (single or pair).
- kind: 'average' (PD), 'individual' (ICE), or 'both'.
- grid_resolution: number of points for continuous features.
- percentiles: lower/upper percentiles to limit the grid range for better focus on the observed range.
- subsample / max_samples: maximum number of rows to use when computing ICE to reduce cost and overplotting.
- center: whether to center ICE curves (c-ICE).
- target (for multi-output models): specification of which output to plot.
- n_jobs / parallelism: number of parallel jobs to accelerate computation.
- random_state: seed for reproducibility when subsampling.

Example (conceptual)
--------------------
A typical usage flow is:
1. Fit a supervised model to training data.
2. Select a feature (or pair) to investigate.
3. Compute PD and/or ICE curves on a grid of values, averaging over (or using) rows of X.
4. Plot:
   - 1D PD as a line with optional confidence band.
   - ICE as thin lines for many samples; highlight a few representative samples.
   - 2D PD as a contour/heatmap with ticks at grid values.

Interpretation workflow
-----------------------
1. Inspect the 1D PD curve for overall trend and nonlinearity.
2. Inspect ICE curves to assess heterogeneity and look for crossing.
3. If crossing ICE curves are present, investigate interactions by plotting 2D PD for the feature pair(s) suspected to interact.
4. Check correlations between features and consider conditional analyses or restricted marginalization if correlations might invalidate interpretations.
5. Consider bootstrapping or repeated subsampling to assess the stability of PD/ICE patterns.

References
----------
- Friedman, J. H. (2001). "Greedy function approximation: a gradient boosting machine." Annals of Statistics.
- Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. (2015). "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation." Journal of Computational and Graphical Statistics.

See also
--------
- Permutation importance: complementary technique to rank and quantify feature importance by measuring change in predictive performance after permuting features.
- Partial dependence surfaces and interaction detection methods for multi-feature inspection.