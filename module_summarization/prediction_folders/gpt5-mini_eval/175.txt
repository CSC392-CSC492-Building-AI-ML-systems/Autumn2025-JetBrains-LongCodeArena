Biclustering
===========

.. _biclustering:

Overview
--------
Biclustering (also called co-clustering) simultaneously clusters the rows and
columns of a data matrix, discovering submatrices (biclusters) that exhibit
coherent patterns. Biclustering is especially useful for high-dimensional
data such as gene expression matrices, document-term matrices, and user-item
ratings where structure is present only on subsets of rows and columns.

Why bicluster?
--------------
- Find local patterns that are invisible to global clustering of rows or
  columns alone.
- Reduce dimensionality and highlight interpretable substructures.
- Useful for exploratory analysis, feature discovery, and preprocessing for
  downstream models.

Common algorithms
-----------------
Spectral biclustering (block-diagonalization)
  - Uses singular value decomposition (SVD) or eigen-decomposition on a
    normalized adjacency/affinity representation of the data to identify
    a permutation of rows and columns that approximates a checkerboard or
    block-diagonal structure.
  - Practical variants include preprocessing with log-transform or
    bistochastic normalization (to equalize row/column sums) before spectral
    embedding; a final discretization (e.g., k-means) on the spectral
    coordinates yields discrete row and column cluster assignments.

Spectral co-clustering (graph-based co-clustering)
  - Treats the data matrix as a bipartite graph between rows and columns
    and performs a normalized cut / bipartite spectral partitioning.
  - Particularly suitable for rectangular matrices such as document-term or
    user-item matrices.

Other biclustering approaches
  - Greedy or enumerative methods that search for coherent submatrices (used
    in bioinformatics).
  - Model-based methods that fit probabilistic bicluster models.
  - Constraint-based and pattern-based biclustering targeting specific
    coherence criteria (mean shifts, constant rows/columns, plaid models).

When to use biclustering
------------------------
- You expect clusters that involve only subsets of features and observations.
- Data are sparse or highly structured (e.g., text, recommender-system
  interactions, biology).
- You want to simultaneously compress rows and columns to reveal
  interpretable blocks.

Input data and preprocessing
----------------------------
- Input: a 2D array-like matrix X of shape (n_samples, n_features) where rows
  correspond to objects and columns to features (or, equivalently, the
  bipartite adjacency matrix).
- Common preprocessing:
  - Centering or scaling rows and/or columns.
  - Log-transforming strictly positive data to reduce skew.
  - Replacing missing values (imputation) or masking them depending on the
    algorithm capability.
  - For spectral methods, enforce non-negativity if required by the chosen
    normalization; consider sparsity-preserving representations for large,
    sparse matrices.

Key outputs
-----------
- Row and column cluster assignments: discrete labels for each row and
  each column indicating membership in biclusters.
- Bicluster index sets: lists or boolean masks describing the rows and
  columns that constitute each bicluster.
- Reordered matrix (optional): a permutation of rows and columns that
  reveals block structure for visualization.

Typical API (usage sketch)
--------------------------
A typical workflow with a spectral biclustering implementation:

.. code-block:: python

   from sklearn.cluster import SpectralBiclustering  # or a co-clustering class
   model = SpectralBiclustering(n_clusters=(n_row_clusters, n_col_clusters),
                                method='log', random_state=0)
   model.fit(X)
   row_labels = model.row_labels_        # label per row
   column_labels = model.column_labels_  # label per column
   rows, cols = model.get_indices()      # sequence of row/column index arrays
   biclusters = model.biclusters_       # boolean masks (row_idx, col_idx) pairs

Notes and practical tips
------------------------
- Choosing the number of biclusters:
  - Provide (n_row_clusters, n_col_clusters) if interpreting both dimensions.
  - Try a range of values and validate using reconstruction error, stability
    across resampling, or external criteria when labels are available.
- Normalization matters:
  - Biclustering algorithms are sensitive to row/column scale; consider
    normalization strategies appropriate for the data and the algorithm.
- Sparse inputs:
  - Many spectral biclustering/co-clustering implementations accept sparse
    matrices and exploit sparsity for scalability.
- Interpretability:
  - Inspect representative rows/columns and visualize reordered matrices and
    heatmaps to validate discovered biclusters.
- Limitations:
  - Spectral methods assume an approximately block-structured signal; they
    may struggle with overlapping biclusters, complex nonlinear patterns, or
    very noisy data.
  - Some biclustering methods produce soft or overlapping memberships;
    conversion to hard, disjoint biclusters may be lossy.

Evaluation
----------
- Internal measures: reconstruction error of the permuted matrix, silhouette
  scores computed on row/column embeddings, stability of biclusters under
  subsampling.
- External measures: precision/recall or adjusted mutual information against
  ground-truth biclusters (when available).
- Visual evaluation: reordered heatmaps, cluster-wise summaries (means,
  variances), and examination of salient features per bicluster.

References
----------
- Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003).
  “Spectral Biclustering of microarray data: coclustering genes and conditions.”
  Genome Research, 13(4), 703–716.
- Dhillon, I. S. (2001). “Co-clustering documents and words using bipartite
  spectral graph partitioning.” Proceedings of the Seventh ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining.

See also
--------
- Clustering algorithms (k-means, spectral clustering) for one-sided partitioning.
- Matrix factorization methods (NMF, SVD) for low-rank approximations and
  feature extraction.
- Specialized biclustering tools and algorithms for biology and text mining.