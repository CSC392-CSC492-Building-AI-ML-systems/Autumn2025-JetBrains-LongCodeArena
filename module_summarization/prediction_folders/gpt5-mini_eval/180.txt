Cross-validation: evaluating estimator performance
================================================

Cross-validation is a resampling procedure used to evaluate machine learning models on a limited
data sample. It provides estimates of generalization performance (e.g. accuracy, F1, ROC AUC)
and can be used to compare estimators, tune hyperparameters, and diagnose high bias or high
variance issues.

Overview
--------

- Cross-validation splits the available data into complementary subsets, trains the estimator
  on a training subset and evaluates it on a validation (test) subset. Repeating this process
  for multiple splits yields distributions of scores that better reflect expected performance
  on unseen data.
- Common splitting strategies: K-fold, StratifiedKFold (preserves class proportions), Leave-One-Out,
  ShuffleSplit, GroupKFold (preserves groups).
- Typical uses:
  - Estimating generalization error (cross_val_score, cross_validate).
  - Model selection and hyperparameter tuning (GridSearchCV, RandomizedSearchCV).
  - Learning curves and validation curves to diagnose model behavior.

Core functions
--------------

- cross_val_score(estimator, X, y, scoring=None, cv=None, n_jobs=None, ...)
  - Returns an array of scores for each CV split.
  - Useful for quick single-metric evaluation.

- cross_validate(estimator, X, y, scoring=None, cv=None, n_jobs=None,
                 return_train_score=False, return_estimator=False, return_times=False, ...)
  - Returns a dict with keys such as 'test_score', (optional) 'train_score', 'fit_time', 'score_time', and (optional) 'estimator'.
  - Use when multiple pieces of information (train scores, timing) are needed.

- learning_curve(estimator, X, y, train_sizes, cv=None, scoring=None, n_jobs=None, ...)
  - Computes scores for varying sizes of the training set to produce a learning curve.
  - Returns (train_sizes_abs, train_scores, test_scores) where train_scores and test_scores
    have shape (n_ticks, n_cv_folds).

- validation_curve(estimator, X, y, param_name, param_range, cv=None, scoring=None, ...)
  - Computes training and validation scores for a range of values of a single hyperparameter.
  - Useful to visualize overfitting/underfitting across a hyperparameter range.

Scoring
-------

- scoring may be a string (e.g. "accuracy", "roc_auc"), a callable, or a dict of scorers.
- When a scorer is a negative loss (scikit-learn convention, e.g., "neg_mean_squared_error"),
  higher (less negative) values are better. Many utilities may provide options to negate or
  re-label scores for plotting and display.
- For cross_validate, pass multiple scorers via a dict to compute multiple metrics at once.

Interpreting results
--------------------

- Compare mean and standard deviation of test scores across splits to assess expected performance
  and its variability.
- High train score and low test score: high variance (overfitting).
- Low train and low test score: high bias (underfitting).
- Learning curves: examine how train and test scores evolve as training size increases.
  - Converging train and test scores at a low value suggests bias.
  - Large gap between train and test suggests variance; adding more data can help.

Plotting learning and validation curves
--------------------------------------

scikit-learn provides helpers to compute curves and visuals to display them.

- Learning curve (compute)
  - Example:
    ::
      from sklearn.model_selection import learning_curve
      train_sizes, train_scores, test_scores = learning_curve(
          estimator, X, y, train_sizes=[0.1, 0.33, 0.55, 0.78, 1.0], cv=5, scoring="accuracy"
      )

  - train_scores and test_scores have shape (n_ticks, n_cv_folds).

- Validation curve (compute)
  - Example:
    ::
      from sklearn.model_selection import validation_curve
      param_range = [0.001, 0.01, 0.1, 1, 10]
      train_scores, test_scores = validation_curve(
          estimator, X, y, param_name="alpha", param_range=param_range, cv=5, scoring="neg_mean_squared_error"
      )

- Plotting with LearningCurveDisplay
  - The visualization API exposes display classes for plotting precomputed curve results
    and for creating displays from estimators.
  - Key display features (LearningCurveDisplay):
    - from_estimator / from_predictions class methods create a display instance.
    - Attributes stored on the display include ax_, figure_, lines_, fill_between_, errorbar_,
      and the original train_scores/test_scores and train_sizes.
    - plot customization options:
      - std_display_style: "fill_between" (default), "errorbar", or None.
      - line_kw, fill_between_kw, errorbar_kw: dicts to customize plotting style.
      - score_name: label shown on the y-axis; if None, inferred from scoring (handles negated scorers).
      - negate_score: whether to negate the stored scores before plotting.
    - Default behavior chooses a log or linear x-scale automatically based on the ratio of
      the maximum and minimum intervals between x values. A deprecated `log_scale` parameter
      may still be accepted with a FutureWarning (prefer setting ax_.set_xscale manually).

  - Example plotting from computed learning curve:
    ::
      from sklearn.model_selection import learning_curve
      from sklearn.model_selection import LearningCurveDisplay
      import matplotlib.pyplot as plt

      train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5)
      display = LearningCurveDisplay(train_sizes=train_sizes,
                                     train_scores=train_scores,
                                     test_scores=test_scores,
                                     score_name="Accuracy")
      display.plot(std_display_style="fill_between")
      plt.show()

  - Example plotting directly from an estimator:
    ::
      from sklearn.model_selection import LearningCurveDisplay
      import matplotlib.pyplot as plt

      display = LearningCurveDisplay.from_estimator(estimator, X, y, cv=5, scoring="accuracy")
      display.plot()
      plt.show()

Options and customization
-------------------------

- std_display_style:
  - "fill_between" (default): shows mean ± std as a filled region (alpha can be controlled via fill_between_kw).
  - "errorbar": shows mean ± std via error bars (customize via errorbar_kw).
  - None: only the mean lines are plotted.
- score_name:
  - Override the label inferred from scoring. When not provided, the display infers a readable score
    name and adjusts for negated scorers (e.g., "Negative mean squared error").
- negate_score:
  - If True, stored scores are negated before plotting (useful when plotting negative loss scorers as positive values).
- Styling dictionaries:
  - line_kw, fill_between_kw, errorbar_kw: passed to matplotlib plot/fill_between/errorbar to customize appearance.
- Axes and figure:
  - display.ax_ and display.figure_ refer to the matplotlib Axes and Figure used. You can further
    customize scales (ax_.set_xscale, ax_.set_yscale), labels, limits, or combine with other plots.

Practical tips
--------------

- Use StratifiedKFold for classification to preserve class balance in splits.
- Use cross_validate(return_train_score=True) to inspect both train and test behavior.
- When tuning hyperparameters, use validation_curve to visualize the effect of a single parameter,
  and GridSearchCV/RandomizedSearchCV with cross-validation to find the best combination.
- Plot learning curves to decide whether collecting more training data is likely to improve performance.
- Interpret standard deviations: high variance in scores across folds suggests instability — consider
  more data, stronger regularization, or a different model.

See also
--------

- sklearn.model_selection.cross_val_score
- sklearn.model_selection.cross_validate
- sklearn.model_selection.GridSearchCV
- sklearn.model_selection.RandomizedSearchCV
- sklearn.model_selection.learning_curve
- sklearn.model_selection.validation_curve
- sklearn.model_selection.LearningCurveDisplay (visualization API)