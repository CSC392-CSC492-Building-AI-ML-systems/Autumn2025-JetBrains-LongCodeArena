sklearn.feature_extraction.text module
=====================================

Overview
--------
The ``sklearn.feature_extraction.text`` submodule gathers utilities to build
feature vectors from text documents. It provides text preprocessing helpers,
tokenization and n-gram generation utilities, stop word handling and high-level
vectorizer transformers such as count, hashing and TF–IDF based vectorizers.

Contents
--------
- Public classes and transformers
  - ``HashingVectorizer``
  - ``CountVectorizer``
  - ``TfidfTransformer``
  - ``TfidfVectorizer``
- Public utilities and constants
  - ``ENGLISH_STOP_WORDS``
  - ``strip_accents_ascii``
  - ``strip_accents_unicode``
  - ``strip_tags``

Utilities and helper functions
------------------------------

_preprocess(doc, accent_function=None, lower=False)
    Chain together an optional series of text preprocessing steps to apply to a
    document.

    Parameters
    ----------
    doc : str
        The string to preprocess.
    accent_function : callable, default=None
        Function for handling accented characters. Common strategies include
        normalizing and removing.
    lower : bool, default=False
        Whether to lowercase the text using ``str.lower``.

    Returns
    -------
    doc : str
        The preprocessed string.

_analyze(doc, analyzer=None, tokenizer=None, ngrams=None,
         preprocessor=None, decoder=None, stop_words=None)
    Chain together an optional series of text processing steps to go from a
    single document to n-grams, with or without tokenizing or preprocessing.

    If ``analyzer`` is provided, it replaces the preprocessing, tokenizing and
    n-gram steps; if ``decoder`` is provided it is applied first.

    Parameters
    ----------
    doc : str
        Input document (decoded as needed before passing to analyzer/tokenizer).
    analyzer : callable, default=None
        Full analyzer that directly yields n-grams/tokens from the document.
    tokenizer : callable, default=None
        Tokenizer producing tokens from a (preprocessed) document.
    ngrams : callable, default=None
        Callable producing n-grams (e.g. word n-grams, character n-grams) from
        a sequence of tokens.
    preprocessor : callable, default=None
        Callable applied before tokenization.
    decoder : callable, default=None
        Callable used to decode raw input (e.g. bytes) into a string.
    stop_words : list or set, default=None
        Stop words set used optionally by the n-gram callable.

    Returns
    -------
    ngrams : list or iterable
        Sequence of tokens or n-grams produced by the processing pipeline.

strip_accents_unicode(s)
    Transform accentuated unicode symbols into their basic (decomposed)
    counterparts by Unicode NFKD normalization and removal of combining marks.

    Note: This implementation preserves non-ASCII characters that do not
    decompose to ASCII; it is slower than the ASCII-based variant.

    Parameters
    ----------
    s : str
        The string to strip.

    Returns
    -------
    s : str
        The stripped string.

strip_accents_ascii(s)
    Transform accentuated unicode symbols by transliterating to ASCII when
    possible. Uses Unicode NFKD normalization then encodes to ASCII,
    ignoring characters without ASCII equivalents.

    Parameters
    ----------
    s : str
        The string to strip.

    Returns
    -------
    s : str
        The stripped string.

strip_tags(s)
    Basic regular-expression based HTML/XML tag stripper.

    For robust HTML parsing consider using a dedicated library such as
    BeautifulSoup or lxml.

    Parameters
    ----------
    s : str
        The string potentially containing tags.

    Returns
    -------
    s : str
        The string with tags replaced by a single space.

_check_stop_list(stop)
    Internal helper to resolve provided ``stop`` argument into an actual stop
    word collection.

    Behaviour
    - If ``stop == "english"``, returns ``ENGLISH_STOP_WORDS``.
    - If ``stop`` is a string other than ``"english"``, raises ``ValueError``.
    - If ``stop is None``, returns ``None``.
    - Otherwise assumes ``stop`` is a collection and returns a frozenset of it.

_VectorizerMixin
----------------
Provides common tokenization and decoding logic shared by text vectorizer
implementations.

Attributes
- ``_white_spaces``: compiled regular expression to collapse multiple
  whitespace characters.

Methods
- decode(doc)
  Decode the input into a unicode string according to the vectorizer's input
  and encoding configuration.

  Parameters
  ----------
  doc : bytes, str, file-like or filename
      Input to decode. If ``input == "filename"`` the file will be read in
      binary mode. If ``input == "file"`` the file object's ``read()`` result
      will be used. If the document is of type ``bytes`` it will be decoded
      using the vectorizer's ``encoding`` and ``decode_error`` settings.

  Returns
  -------
  doc : str
      Decoded unicode string.

  Raises
  ------
  ValueError
      If ``doc`` is ``np.nan`` (invalid document type).

- _word_ngrams(tokens, stop_words=None)
  Turn a token sequence into a sequence of n-grams after optional stop word
  filtering. (Used internally by token-based analyzers.)


Public classes and transformers
-------------------------------
HashingVectorizer
    Stateless transformer that converts a collection of text documents to a
    matrix of token occurrences using the hashing trick. Useful for large-scale
    streaming or memory-constrained scenarios. Produces a fixed-size numeric
    feature vector without storing an explicit vocabulary.

CountVectorizer
    Convert a collection of text documents to a matrix of token counts. Supports
    tokenization, preprocessing (decoding, accent stripping, lowercasing),
    stop word filtering, and n-gram extraction. Builds an explicit vocabulary
    from the training data.

TfidfTransformer
    Transformer that converts a matrix of raw counts to TF–IDF representation.
    Applies term frequency scaling and inverse document frequency weighting and
    supports optional normalization.

TfidfVectorizer
    Combination of ``CountVectorizer`` and ``TfidfTransformer``: converts a
    collection of raw documents to a TF–IDF weighted document-term matrix in
    one step.

Constants
---------
ENGLISH_STOP_WORDS
    A frozenset of common English stop words provided for convenience. Can be
    used with vectorizers or passed to stop word handling utilities.

Notes
-----
- The module intends to expose a stable API for text feature extraction. The
  high-level vectorizers listed above are the primary user-facing components.
- For HTML/XML stripping and complex accent handling, consider using
  specialized third-party libraries when appropriate.

See Also
--------
For implementation details and advanced options, refer to the docstrings of the
individual classes and functions: ``CountVectorizer``, ``HashingVectorizer``,
``TfidfVectorizer`` and ``TfidfTransformer``.