sklearn.kernel_approximation module
==================================

The sklearn.kernel_approximation module implements several approximate kernel
feature maps based on Fourier transforms and Count Sketches.

This module provides tools to approximate commonly used kernel feature maps
with lower-dimensional explicit feature representations. These approximations
are particularly useful for scaling linear methods to large datasets while
retaining properties of nonlinear kernels.

PolynomialCountSketch
---------------------

Polynomial kernel approximation via Tensor Sketch.

Implements Tensor Sketch, which approximates the feature map of the polynomial
kernel

    K(X, Y) = (gamma * <X, Y> + coef0)^degree

by efficiently computing a Count Sketch of the outer product of a vector with
itself using Fast Fourier Transforms (FFT).

.. versionadded:: 0.24

Parameters
~~~~~~~~~~
gamma : float, default=1.0
    Parameter of the polynomial kernel whose feature map will be approximated.

degree : int, default=2
    Degree of the polynomial kernel whose feature map will be approximated.

coef0 : int, default=0
    Constant term of the polynomial kernel whose feature map will be approximated.

n_components : int, default=100
    Dimensionality of the output feature space. Usually, `n_components`
    should be greater than the number of features in input samples to achieve
    good performance. A common heuristic is to use `n_components` roughly
    10Ã— the number of input features, but the optimal trade-off depends on the
    dataset and task.

random_state : int, RandomState instance or None, default=None
    Controls random number generation for hash initialization (indexHash and
    bitHash). Pass an int for reproducible results.

Attributes
~~~~~~~~~~
indexHash_ : ndarray of shape (degree, n_features), dtype=int64
    Array of indices in range [0, n_components) used to represent the
    2-wise independent hash functions for Count Sketch computation.

bitHash_ : ndarray of shape (degree, n_features), dtype=float32
    Array with random entries in {+1, -1}, used to represent the
    2-wise independent hash functions for Count Sketch computation.

n_features_in_ : int
    Number of features seen during fit.

feature_names_in_ : ndarray of shape (n_features_in_,)
    Names of features seen during fit. Defined only when X has feature names
    that are all strings.

_n_features_out : int
    Number of output features (equal to n_components).

Methods
~~~~~~~
fit(X, y=None)
    Fit the transformer on X. This initializes the internal random hashes
    but does not depend on the distribution of the data beyond the number of
    input features.

    Parameters
        X : array-like or sparse matrix of shape (n_samples, n_features)
            Training data.
        y : ignored, default=None

    Returns
        self

transform(X)
    Generate the feature map approximation for X.

    Parameters
        X : array-like, shape (n_samples, n_features)
            New data to transform.

    Returns
        X_new : array-like, shape (n_samples, n_components)
            Transformed feature matrix approximating the polynomial kernel.

Notes
~~~~~
- The method uses Tensor Sketch and FFT to compute an efficient Count Sketch
  approximation of polynomial kernel feature maps.
- If coef0 != 0, an implicit bias dimension is handled during hashing to
  account for the constant term in the kernel.

See Also
~~~~~~~~
AdditiveChi2Sampler
    Approximate feature map for additive chi2 kernel.

Nystroem
    Approximate a kernel map using a subset of the training data.

RBFSampler
    Approximate a RBF kernel feature map using random Fourier features.

SkewedChi2Sampler
    Approximate feature map for "skewed chi-squared" kernel.

sklearn.metrics.pairwise.kernel_metrics
    List of built-in kernels.

Examples
~~~~~~~~
>>> from sklearn.kernel_approximation import PolynomialCountSketch
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
>>> y = [0, 0, 1, 1]
>>> ps = PolynomialCountSketch(degree=3, random_state=1)
>>> X_features = ps.fit_transform(X)
>>> clf = SGDClassifier(max_iter=10, tol=1e-3)
>>> clf.fit(X_features, y)
SGDClassifier(max_iter=10)
>>> clf.score(X_features, y)
1.0

References
~~~~~~~~~~
Pham, N., & Pagh, R. (2013). Fast and scalable polynomial kernels via explicit
feature maps. In Proceedings of the 19th ACM SIGKDD international conference
on Knowledge discovery and data mining.

Acknowledgements
~~~~~~~~~~~~~~~~
Author: Andreas Mueller <amueller@ais.uni-bonn.de>
Contributor (TensorSketch): Daniel Lopez-Sanchez <lope@usal.es>