Naive Bayes
===========

Overview
--------
The naive_bayes module implements a family of supervised learning algorithms that
apply Bayes' theorem with a strong (naive) assumption of feature independence.
Given a feature vector x and a class label y, Naive Bayes methods model

    P(y | x) ∝ P(y) · P(x | y)

and make predictions by choosing the class with maximum posterior probability.
Implementations in this module work primarily with log-probabilities for
numerical stability and efficiency.

Available estimators
--------------------
- BernoulliNB
  - Naive Bayes for binary/boolean features. Models each feature as Bernoulli
    distributed conditioned on the class.

- MultinomialNB
  - Naive Bayes for count data (e.g., word counts in text classification).
    Models features using a multinomial distribution conditioned on the class.

- ComplementNB
  - Variation of MultinomialNB designed to address imbalanced data by
    training on complement class statistics.

- CategoricalNB
  - Naive Bayes for categorical features (discrete non-count categories).

- GaussianNB
  - Assumes continuous features are normally distributed and estimates class-
    conditional means and variances. Supports online parameter updates via
    partial_fit.

Base class: _BaseNB
-------------------
_All concrete Naive Bayes estimators inherit from an abstract base class that
provides common prediction and probability utilities._

Key responsibilities and methods:
- _joint_log_likelihood(X)
  - Abstract method implemented by subclasses. Returns the unnormalized joint
    log probability log P(y) + log P(x | y) for each sample and class as an
    array of shape (n_samples, n_classes).

- _check_X(X)
  - Abstract input validation method to be implemented by subclasses and used
    by public prediction/probability methods.

- predict_joint_log_proba(X)
  - Returns the joint log-probabilities log P(x, y) = log P(y) + log P(x | y).

- predict_log_proba(X)
  - Returns log P(y | x) by normalizing the joint log-probabilities using
    logsumexp across classes.

- predict_proba(X)
  - Returns P(y | x) as probabilities by exponentiating predict_log_proba.

- predict(X)
  - Returns the class with maximum joint log-likelihood for each sample.
  - All public prediction/probability methods call check_is_fitted and the
    estimator's _check_X before delegating to subclass implementations.

GaussianNB
----------
Gaussian Naive Bayes assumes that continuous features conditioned on each
class follow a Gaussian (normal) distribution. The estimate for class c and
feature j uses the class-conditional mean theta_[c, j] and variance var_[c, j].

Primary parameters
- priors : array-like of shape (n_classes,), default=None
  - Prior probabilities of the classes. If provided, these priors are used
    and not re-estimated from the training data.

- var_smoothing : float, default=1e-9
  - A small portion of the largest feature variance is added to all estimated
    variances for numerical stability. This yields an absolute additive term
  epsilon_ = var_smoothing * max(var_) that is added to variances.

Notes on learning and partial_fit
- GaussianNB supports online updates via partial_fit. The update algorithm for
  means and variances follows the online scheme described in the Stanford CS
  technical report by Chan, Golub, and LeVeque (STAN-CS-79-773).

Main attributes (after fitting)
- class_count_ : ndarray of shape (n_classes,)
  - Number of training samples observed in each class.

- class_prior_ : ndarray of shape (n_classes,)
  - Prior probability of each class (either provided or estimated from data).

- classes_ : ndarray of shape (n_classes,)
  - Sorted class labels known to the classifier.

- epsilon_ : float
  - Absolute additive value applied to variances for numerical stability
    (derived from var_smoothing).

- theta_ : ndarray of shape (n_classes, n_features)
  - Estimated mean of each feature per class.

- var_ : ndarray of shape (n_classes, n_features)
  - Estimated variance of each feature per class.

- n_features_in_ : int
  - Number of features seen during fit.

- feature_names_in_ : ndarray of shape (n_features_in_,), optional
  - Feature names seen during fit (present when input X had string feature
    names).

Common behavior and conventions
------------------------------
- Input checking
  - Concrete estimators implement a _check_X method to validate and transform
    input arrays before computing likelihoods and predictions. Public methods
    (predict, predict_proba, predict_log_proba, predict_joint_log_proba) call
    check_is_fitted and pass X through _check_X.

- Probability computations
  - Implementations compute class priors and class-conditional log-likelihoods
    and expose both joint log-probabilities and normalized posterior
    probabilities. Numerical stability is ensured using logsumexp for
    normalization.

- Multi-class and sparse support
  - Different Naive Bayes variants provide specialized handling (e.g., sparse
    count matrices for MultinomialNB, binary feature handling for BernoulliNB).

References
----------
- Bayes' theorem and Naive Bayes classifiers (general background).
- GaussianNB online update reference:
  Chan, T. F., Golub, G. H., & LeVeque, R. J., STAN-CS-79-773.

See also
--------
The user guide sections for the individual estimators (e.g., gaussian_naive_bayes)
and the API references for BernoulliNB, MultinomialNB, ComplementNB, and
CategoricalNB for details on their specific parameters, smoothing strategies,
and input requirements.