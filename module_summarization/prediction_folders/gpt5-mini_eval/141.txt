Connect to remote data
======================

This module provides utilities for reading bytes from local or remote filesystems
(via fsspec) and returning Dask delayed objects that load file blocks lazily.
It is intended for use with fsspec-backed protocols such as s3://, hdfs://,
gs://, ftp://, and many others supported by fsspec implementations.

Highlights
----------
- Use any fsspec-supported protocol by prefixing paths (for example, s3://bucket/...).
- Read files in block-aligned chunks suitable for parallel processing with Dask.
- Optional delimiter-aware splitting so blocks begin/end on record boundaries.
- Returns a small sample of the start of the first file (useful for header parsing).
- Works with multiple files (glob or list); all files must use the same protocol.

Primary function
----------------

read_bytes(urlpath, delimiter=None, not_zero=False, blocksize="128 MiB",
           sample="10 kiB", compression=None, include_path=False, **kwargs)

Read bytes from one or more files and return Dask delayed objects that read
individual blocks from those files.

Parameters
----------
urlpath : str or list or os.PathLike
    Absolute or relative path(s). Can be a glob (e.g. '2015-*-*.csv') or a list
    of paths. Prefix with a protocol like 's3://' to read from remote storage.
delimiter : bytes, optional
    If provided (e.g. b'\n'), blocks are adjusted so boundaries occur directly
    after a delimiter; useful for record-oriented formats.
not_zero : bool, optional
    If True, the first returned block will start at byte 1 (useful to discard
    a header byte).
blocksize : int or str or None, optional
    Size in bytes for chunking file reads. Strings like "128 MiB" are accepted.
    If None, files are not chunked and whole-file reads are used.
sample : bool or int or str, optional
    Request a sample of the start of the first file. Accepts:
      - False: no sample
      - True: treated as "10 kiB" (backwards compatibility)
      - int: number of bytes
      - str: human-readable sizes parsed (e.g. "1 MiB")
compression : str or None
    Compression name ('gzip', 'bz2', 'xz', etc.) or 'infer' to infer from
    filename. Note: chunked reads (blocksize != None) are not supported for
    compressed files that do not support efficient random access. If the file
    is compressed, use blocksize=None to read whole file.
include_path : bool, optional
    If True, the returned tuple includes the list of file paths (one per file).
**kwargs : dict
    Extra storage options forwarded to fsspec (for example, credentials,
    host, port, profile, etc.).

Return value
------------
If include_path is False:
    (sample, blocks)

If include_path is True:
    (sample, blocks, paths)

Where:
- sample is bytes containing the header sample (or False/None if not requested).
- blocks is a list of lists; each sublist corresponds to a single file and
  contains dask.Delayed objects. Each delayed object computes to a bytes
  object with the file block.
- paths is a list of the input file paths (only returned if include_path=True).

Behavior and notes
------------------
- fsspec is used to open files and resolve paths via get_fs_token_paths().
- If urlpath resolves to no files an OSError is raised.
- If urlpath is not a str/list/tuple/os.PathLike, a TypeError is raised.
- blocksize may be supplied as a human-readable string (parsed via dask.utils.parse_bytes)
  or as an integer number of bytes. Non-integer blocksize raises TypeError.
- When chunking:
  - For each file, file size is obtained (fs.info(path)["size"]). If size cannot
    be determined, chunked reads are not possible and a ValueError is raised.
  - Files of size 0 are skipped (empty block lists).
  - The code attempts to evenly distribute blocks and will shrink blocksize
    slightly to give the same number of parts across the file.
  - If compression is specified or inferred on a filename and the compression
    does not allow efficient random access, read_bytes will raise ValueError
    (use blocksize=None to read entire compressed files).
- Delimiter-aware splitting: when a delimiter is provided, read boundaries are
  adjusted so blocks begin directly after a delimiter and end at the delimiter.
- Sample behavior: when sample is requested, the first file is opened and up to
  the requested sample size is read; if a delimiter is provided, the sample is
  extended until the next delimiter is found (so the sample ends cleanly on a record boundary).
- include_path True returns the same ordering as blocks; each sublist in blocks
  corresponds to the path at the same index in paths.
- Storage-specific options (credentials, region, client kwargs, etc.) may be
  passed via **kwargs and are forwarded to get_fs_token_paths/OpenFile.

Integration with Dask
---------------------
- The function builds dask.delayed tasks (using dask.delayed and dask.base.tokenize)
  that call read_block_from_file for each block. Each delayed task, when computed,
  reads its portion of the file (via fsspec.OpenFile and fsspec.utils.read_block).
- The returned blocks can be used to construct larger Dask collections (for
  example, Dask bag or DataFrame partitions) by mapping parsing functions over
  the delayed byte blocks.

Helper: read_block_from_file
----------------------------
read_block_from_file(lazy_file, off, bs, delimiter)

- A small helper that opens a (copied) fsspec.OpenFile and returns the bytes for
  the requested offset and block size, using read_block to handle delimiter-aware reads.
- If off == 0 and bs is None, the helper returns the entire file contents.

Examples
--------
Simple CSV files on local disk (delimiter-aware):
.. code-block:: python

    sample, blocks = read_bytes('data/2015-*.csv', delimiter=b'\n')

Read files from an S3 bucket (requires s3fs and credentials via kwargs or environment):
.. code-block:: python

    sample, blocks = read_bytes('s3://my-bucket/2015-*.csv', delimiter=b'\n',
                                storage_options={'profile': 'default'})

Read entire compressed files (no chunking) and include paths:
.. code-block:: python

    sample, blocks, paths = read_bytes('s3://bucket/data-*.csv.gz',
                                       blocksize=None, compression='infer',
                                       include_path=True)

Error cases
-----------
- TypeError if urlpath is not a supported type.
- OSError if no files matched the given urlpath.
- TypeError if blocksize is not an integer (after parsing).
- ValueError if attempting chunked reads on compressed files or if filesystem
  cannot determine file size required for chunking.

Dependencies
------------
- fsspec (for filesystem abstractions and OpenFile)
- dask (for delayed, tokenize, utilities/parsing of sizes)
- Any fsspec-backed protocol implementation required for remote storage (s3fs,
  gcsfs, adlfs, hdfs3/pyarrow/HDFS implementations, etc.)

See also
--------
- fsspec documentation for configuring and authenticating remote filesystems.
- dask.delayed and higher-level Dask collections for turning blocks into
  parallel-parsed datasets.