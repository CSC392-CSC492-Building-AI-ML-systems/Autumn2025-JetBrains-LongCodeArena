Optimization algorithms for parameter estimation
===============================================

This document describes three types of algorithms commonly used for the estimation
of model parameters via numerical optimization of an objective function (for
example a log-likelihood). The fitting interface assumed here is generic: an
objective (scalar) function, an optional gradient, optional Hessian information,
start parameters, and extra positional arguments passed to the objective. The
interface supports options such as method selection, maximum iterations,
full solver output, convergence display, iteration callbacks and returning all
intermediate solutions.

Common fit inputs and behavior
------------------------------

- start_params : array_like
  Initial guess for the parameter vector.

- fargs : tuple
  Extra arguments passed to the objective: objective(x, *fargs).

- gradient / score : callable or None
  If provided, used by gradient-based solvers. If None, solvers may
  approximate the gradient numerically.

- hessian / fhess_p : callable or None
  If provided, used by second-order solvers (Newton, ncg). Some solvers
  accept a function that computes the Hessian-vector product.

- maxiter : int
  Maximum number of iterations.

- full_output : bool
  If True, solver-specific return information is made available to the
  caller (e.g., in a Results object's mle_retvals).

- disp : bool
  Print convergence messages when True.

- callback : callable(xk)
  Called after each iteration with the current parameter vector xk.

- retall : bool
  If True, collect and return the parameter vector at each iteration.

- Note: the 'basinhopping' global solver ignores maxiter, retall and
  full_output explicit arguments; its own options are used instead.

Three algorithm types
---------------------

1) Local second-order (Newton-like) and quasi-Newton methods
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Description
    These methods use gradient and (optionally) Hessian information to obtain
    fast local convergence to (local) optima. They are efficient when the
    objective is smooth and gradients (and possibly Hessian or Hessian-vector
    products) are available or can be well approximated.

Common methods
    - 'newton' : Newton–Raphson — uses gradient and Hessian for quadratic
      convergence near a solution when available.
    - 'ncg' : Newton-conjugate-gradient — uses Hessian-vector products or an
      approximation and conjugate-gradient style linear solves.
    - quasi-Newton (e.g. 'bfgs', 'lbfgs') : approximate the Hessian from
      gradients and achieve superlinear convergence without storing the full
      Hessian.

When to use
    - Objective is smooth and differentiable.
    - Accurate gradient (and Hessian or Hessian-vector product) is available.
    - You require fast local convergence near the optimum.

Key options
    - tol (newton) : relative parameter error tolerance.
    - fhess_p (ncg) : function computing Hessian times vector.
    - avextol, pgtol, factr, m (lbfgs) : stopping/accuracy and memory controls.
    - gtol, norm : gradient norm tolerance and norm used.
    - epsilon : step used to approximate gradient/Hessian if not provided.

2) Derivative-free local methods
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Description
    These methods do not require analytic gradients and rely only on
    function evaluations. They are robust for non-smooth, noisy or
    black-box objectives but may require many function evaluations and
    converge more slowly than gradient-based methods.

Common methods
    - 'nm' : Nelder–Mead (simplex) — derivative-free local optimizer.
    - 'powell' : Powell's modified method — line-search based,
      derivative-free.

When to use
    - Gradient information is unavailable or unreliable.
    - Objective may be noisy or non-differentiable.
    - Low-dimensional problems where function-evaluation cost is tolerable.

Key options
    - xtol, ftol : tolerances on parameter and function value changes.
    - maxfun : maximum number of function evaluations.
    - start_direc (powell) : initial set of search directions.

3) Global and stochastic approaches
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Description
    Global or stochastic methods attempt to escape local minima and locate
    a global optimum or better local minima. They typically combine random
    perturbations with local minimization steps.

Common methods
    - 'basinhopping' : a global basin-hopping algorithm that applies random
      perturbations followed by local minimization (uses scipy.optimize.minimize
      for the local step). This is useful when the objective has multiple
      separated minima.

When to use
    - Objective has multiple local minima and a global solution is desired.
    - You can afford many function/gradient evaluations or evaluations are cheap.

Key options (basinhopping)
    - niter : number of basin hopping iterations.
    - niter_success : stop if global minimum candidate is unchanged for this
      many iterations.
    - T : temperature controlling acceptance of uphill moves.
    - stepsize : initial random displacement scale.
    - interval : frequency for updating stepsize.
    - minimizer : dict of keyword arguments passed to the local minimizer
      (scipy.optimize.minimize), e.g. method name, tol, and mapped arguments:
        - args <- fargs
        - jac  <- score

Generic wrapper: scipy.optimize.minimize
---------------------------------------

The 'minimize' option is a generic wrapper around scipy.optimize.minimize that
lets you specify one of many methods via its own 'method' argument (for example
'BFGS', 'L-BFGS-B', 'TNC', 'SLSQP', etc.). It exposes method-specific options
and accepts the same fargs, jac, hess/hessp and callback semantics as above.

Solver-specific option summary
------------------------------

- newton
    - tol : float, relative parameter tolerance.

- nm (Nelder–Mead)
    - xtol, ftol : parameter/function tolerances.
    - maxfun : maximum function evaluations.

- bfgs
    - gtol : gradient norm tolerance.
    - norm : order of norm (np.Inf for max).
    - epsilon : step for numeric gradient approximation.

- lbfgs
    - m : number of corrections to approximate inverse Hessian.
    - pgtol : projected gradient tolerance.
    - factr : stopping criterion multiplier relative to machine eps.
    - maxfun : maximum iterations.
    - epsilon : step size for numeric gradient when approx_grad True.
    - approx_grad : whether to approximate gradients numerically.

- cg
    - gtol, norm, epsilon : gradient tolerance, norm and numeric step size.

- ncg
    - fhess_p : callable Hessian-vector product.
    - avextol : average relative error tolerance.
    - epsilon : step size for approximating fhess_p if needed.

- powell
    - xtol, ftol : tolerances.
    - maxfun : maximum function evaluations.
    - start_direc : initial direction set.

- basinhopping
    - niter, niter_success, T, stepsize, interval, minimizer (dict)

Return values from a fit
------------------------

- xopt : array
  Optimized parameter vector (the solution).

- retvals : dict or None
  If full_output is True, solver-specific return information (convergence
  status, number of iterations/evaluations, final function and gradient
  values, and possibly lists of intermediate solutions if retall=True).

- optim_settings : dict
  Dictionary of settings and options passed to the solver.

Practical guidance
------------------

- Prefer second-order/quasi-Newton methods (newton, bfgs, lbfgs) when gradients
  (and Hessians) are available and the objective is smooth — these typically
  converge fastest.

- Use derivative-free methods (Nelder–Mead, Powell) for non-differentiable or
  noisy objectives or when analytic gradients are unavailable.

- Use basinhopping or other global/stochastic strategies when the objective
  exhibits many local minima and a global solution is required; combine with a
  strong local solver for the inner minimization.

- Provide sensible start_params and use callback/retall to inspect progress and
  diagnose convergence issues. Increase maxiter or maxfun for difficult fits,
  and tune solver-specific tolerances (gtol, xtol, ftol, factr, pgtol) as needed.