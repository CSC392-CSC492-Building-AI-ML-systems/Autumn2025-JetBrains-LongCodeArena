Linear Mixed Effects Models
===========================

Overview
--------
Linear mixed effects models (also known as multilevel or hierarchical linear models)
are regression models for dependent data that estimate relationships involving
both means and variances. The MixedLM class fits linear mixed effects models to
grouped data and supports common post‑estimation tasks. This implementation is
group‑based and most efficient when the data can be partitioned into independent
groups (models with crossed effects can sometimes be handled by specifying a
single group).

Likelihood model (per group)
----------------------------
For group i with n_i observations the model is

Y = X * beta + Z * gamma + epsilon

where

- Y is an n_i-dimensional response vector (endog)
- X is an n_i x k_fe design matrix for the fixed effects (exog)
- beta is a k_fe-dimensional vector of fixed effects parameters (fe_params)
- Z is an n_i x m design matrix for the random effects (exog_re). The number
  of columns of Z may vary by group.
- gamma is a random vector with mean 0. The covariance matrix of the first
  k_re elements of gamma (cov_re, sometimes called Psi) is common to all
  groups. Remaining elements may represent variance components.
- epsilon ~ iid N(0, sigma^2 I) are independent errors within and between groups

Y, X and Z must be fully observed. beta, cov_re (Psi) and sigma^2 (scale) are
estimated (by ML or REML). gamma and epsilon are random and define the
probability model. The marginal mean is E[Y | X, Z] = X * beta. If only the
mean structure is of interest, GEE is an alternative.

Random effects: two types
-------------------------
- Standard random effects: correlated with each other in arbitrary ways.
  Every group has the same number (k_re) of these, with the same joint
  distribution but independent realizations across groups.

- Variance components: uncorrelated with each other and with standard random
  effects. Each variance component has mean zero; all realizations of a given
  variance component share the same variance parameter. The number of realized
  variance components per variance parameter can differ across groups.
The vector of variance parameters is often referred to as vcomp; its length is
determined by the keys in exog_vc or by vc_formula when using formulas.

Covariance notation
-------------------
- cov_re (Psi) — random effects covariance matrix.
- scale — scalar error variance (sigma^2).

For a single group, the marginal covariance of the response is
scale * I + Z * cov_re * Z'.

Parameterizations
-----------------
Three parameterizations are used; beta (fe_params) is the same across them,
but variance parameters differ:

1. User parameterization: cov(endog) = scale * I + Z * cov_re * Z' (exposed to users).
2. Profile parameterization: cov(endog) = I + Z * cov_re1 * Z' where cov_re1 =
   cov_re / scale. This is the parameterization of the profiled likelihood
   used for optimization.
3. Square‑root parameterization: optimization works with the Cholesky factor
   of cov_re1 instead of cov_re directly (hidden from user).

Parameters can be packed into a vector by concatenating fe_params with the
lower triangle or Cholesky square root of the dependence structure, followed
by variance component parameters. When unpacking, square or reflect components
appropriately based on the active parameterization.

Estimation and optimization
---------------------------
- Maximum likelihood (ML) and restricted maximum likelihood (REML) are
  supported.
- Numerical optimization is performed on the profiled likelihood: the likelihood
  is profiled over both the scale parameter and the fixed effects parameters,
  which are handled via GLS so they are not explicitly optimized.
- Two score methods are implemented:
  - Score with respect to elements of the random effects covariance matrix
    (for inference at the MLE).
  - Score with respect to parameters of the Cholesky square root (for optimization).
- Because the profiled log-likelihood is used and profiling removes scale and
  fixed effects, the Hessian of the profiled likelihood is not computed. Thus
  optimization methods requiring an explicit Hessian (e.g., Newton–Raphson)
  are not available.

Implementation notes
--------------------
- The implementation follows Lindstrom and Bates (1988) closely and adapts it to
  support variance components.
- All likelihood, gradient, and (where implemented) Hessian calculations follow
  Lindstrom & Bates (1988) with extensions for variance components.
- The model is most efficient when data are partitionable into disjoint groups.

Practical notes
---------------
- The design matrices Y, X and Z must be entirely observed (no missing values).
- MixedLM supports specification of variance components either via exog_vc or
  via vc_formula when using formula interfaces.
- For models where only marginal mean structure is of interest, consider GEE.

References
----------
- Lindstrom, M. J., & Bates, D. M. (1988). Newton–Raphson and EM algorithms for
  linear mixed-effects models for repeated measures data. Journal of the
  American Statistical Association, 83(404), 1014–1022.
- Additional implementation notes:
  http://econ.ucsb.edu/~doug/245a/Papers/Mixed%20Effects%20Implement.pdf
- Introductory/user‑oriented materials:
  http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf
  http://lme4.r-forge.r-project.org/slides/2009-07-07-Rennes/3Longitudinal-4.pdf