Support Vector Machines
=======================

Overview
--------
Support Vector Machines (SVMs) are a family of supervised learning methods used for
classification and regression. SVM classifiers find a decision boundary that
maximizes the margin between classes. Implementations in this module provide
both kernelized SVMs (e.g. :class:`~sklearn.svm.SVC`) and linear SVMs based on
liblinear (e.g. :class:`~sklearn.svm.LinearSVC`). The liblinear-based
implementation is typically faster and more scalable for large sparse datasets
and supports a variety of loss and penalty combinations.

LinearSVC
---------
``LinearSVC`` implements linear support vector classification using the
liblinear library. It behaves similarly to ``SVC(kernel='linear')`` but differs
in default loss, in how the intercept is regularized, and in the set of
supported penalties and optimizers. Multiclass classification is handled via a
one-vs-the-rest scheme by default.

Key characteristics
- Implemented with liblinear (not libsvm): more choices for loss/penalty,
  better scaling to large sample counts, native sparse support.
- Supports dense and sparse inputs.
- Multiclass support uses one-vs-rest unless the ``crammer_singer`` option is
  selected.
- Default loss is ``squared_hinge`` (squared hinge loss) rather than the
  classical hinge loss used by many other SVM implementations.

Parameters
----------
penalty : {'l1', 'l2'}, default='l2'
    The norm used in the regularization term.
    - ``'l2'``: standard ridge-style regularization.
    - ``'l1'``: yields sparse coefficients (sparse ``coef_``).
    Note: some combinations of ``penalty`` and ``loss`` are not supported.

loss : {'hinge', 'squared_hinge'}, default='squared_hinge'
    The loss function to be optimized.
    - ``'hinge'``: the (non-squared) hinge loss (standard SVM loss).
    - ``'squared_hinge'``: squared hinge loss which gives a differentiable
      objective and often better numerical behavior.
    The combination ``penalty='l1'`` and ``loss='hinge'`` is not supported.

dual : {"auto", "warn"} or bool, default=True
    Select whether to solve the dual or primal optimization problem.
    - If ``True``, the dual problem is solved.
    - If ``False``, the primal problem is solved.
    - ``"auto"`` chooses automatically based on the data shape and whether the
      chosen solver supports the requested configuration: when
      n_samples < n_features and the solver supports the chosen configuration,
      ``dual`` will be set to ``True``, otherwise to ``False``.
    - ``"warn"`` preserves legacy behavior (equivalent to ``True``) but emits a
      ``FutureWarning`` that the default will change to ``"auto"`` in a future
      release.
    Prefer ``dual=False`` when n_samples > n_features.

tol : float, default=1e-4
    Tolerance for stopping criteria.

C : float, default=1.0
    Inverse of regularization strength; must be strictly positive. Smaller
    values specify stronger regularization.

multi_class : {'ovr', 'crammer_singer'}, default='ovr'
    Multi-class strategy:
    - ``'ovr'`` (one-vs-rest): trains one classifier per class.
    - ``'crammer_singer'``: optimizes a joint objective over all classes.
      If ``'crammer_singer'`` is chosen, options ``loss``, ``penalty`` and
      ``dual`` are ignored. ``crammer_singer`` is consistent in theory but
      rarely yields better accuracy in practice and is computationally more
      expensive.

fit_intercept : bool, default=True
    Whether a constant (bias or intercept) should be added to the decision
    function. If ``True``, the input features are extended with a synthetic
    feature of constant value ``intercept_scaling`` (see below).

intercept_scaling : float, default=1.0
    When ``fit_intercept=True``, the feature vector is effectively extended to
    ``[x1, ..., xn, intercept_scaling]``. The intercept term equals
    ``intercept_scaling * w_synthetic`` where ``w_synthetic`` is the weight for
    the synthetic feature. Note that liblinear penalizes the intercept like any
    other weight; increasing ``intercept_scaling`` reduces the effective
    regularization on the intercept.

Notes
-----
- The default loss in ``LinearSVC`` is ``squared_hinge``, which differs from
  the classic hinge loss. This yields a differentiable objective and may
  affect the learned decision boundary compared to kernel SVMs or other
  implementations.
- ``LinearSVC`` and ``SVC(kernel='linear')`` may produce different results due
  to differences in formulation (liblinear vs libsvm), default loss, and
  intercept regularization.
- The helper selection logic for the ``dual`` parameter can automatically
  select primal or dual formulations when ``dual='auto'``. When in doubt,
  choose ``dual=False`` for datasets with many more samples than features.

Examples
--------
Basic usage:

.. code-block:: python

    from sklearn.svm import LinearSVC
    clf = LinearSVC(penalty='l2', loss='squared_hinge', C=1.0, max_iter=1000)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

See Also
--------
- :class:`~sklearn.svm.SVC` : Support Vector Classification with kernels.
- :mod:`sklearn.svm` user guide section :ref:`svm_classification` for more
  conceptual information and examples.