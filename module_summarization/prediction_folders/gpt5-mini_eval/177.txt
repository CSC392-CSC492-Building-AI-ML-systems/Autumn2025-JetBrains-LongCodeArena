Pipelines and composite estimators
================================

Overview
--------
Pipelines provide a light-weight way of composing several data transforms
followed by a final estimator into a single object that can be treated like
any other estimator. Common use cases are:

- chaining preprocessing and estimator steps so the whole chain can be
  cross-validated together (e.g. with GridSearchCV);
- guaranteeing that the same preprocessing is applied to training and test
  data;
- building composite feature extractors that concatenate features generated by
  several transformers.

A pipeline is itself an estimator: it exposes the usual fit / predict /
transform / score APIs when the contained estimators implement them, and
parameters of contained estimators can be set using the "stepname__param"
naming convention.

Pipeline
--------
A Pipeline is constructed from a sequence of (name, estimator) pairs:

- intermediate steps (all but the last) must implement fit and transform;
- the final step must implement fit (and may implement predict/transform/etc.
  depending on use).

Parameters
- steps : list of (name, estimator)
  The ordered sequence of steps that make up the pipeline.
- memory : str or object with a joblib.Memory interface, default=None
  If provided, fitted transformers (not the final estimator) are cached to
  avoid refitting them when the same inputs are seen again. If a string is
  supplied it is used as the path to a cache directory. Note that enabling
  caching triggers cloning of the transformers.
- verbose : bool, default=False
  If True, prints elapsed time for each step during fit.

Key attributes
- named_steps : Bunch-like mapping of step names to estimators (read-only).
- classes_ : array-like, present if the final estimator is a classifier.
- n_features_in_ : number of features seen during fit if the first estimator
  exposes that attribute.
- feature_names_in_ : input feature names if the first estimator exposes that
  attribute.

Behavior and conveniences
- Parameter setting: parameters of inner estimators can be set with
  pipeline.set_params(stepname__param=value).
- Replacing/removing steps: an entire step can be replaced by setting the
  step name to another estimator; a transformer can be removed by setting the
  step to "passthrough" or None.
- Caching: when memory is provided the pipeline clones and caches fitted
  intermediate transformers; the final step is never cached.
- set_output: calling set_output on the pipeline will call set_output on each
  contained step, allowing control over transform / fit_transform output
  formats (for example to request pandas DataFrame output).
- Integration: a pipeline can be used anywhere an estimator is expected
  (cross-validation, model selection, stacking, etc.).

Examples
- Basic pipeline:

  >>> from sklearn.pipeline import Pipeline
  >>> from sklearn.preprocessing import StandardScaler
  >>> from sklearn.svm import SVC
  >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])
  >>> pipe.fit(X_train, y_train)
  >>> y_pred = pipe.predict(X_test)

- Grid search on pipeline parameters:

  >>> from sklearn.model_selection import GridSearchCV
  >>> param_grid = {'svc__C': [0.1, 1.0, 10.0]}
  >>> gs = GridSearchCV(pipe, param_grid).fit(X_train, y_train)

- Convenience constructor:

  make_pipeline(*steps) builds a Pipeline with automatic step names
  (lowercased estimator class names), useful for quick composition.

Composite estimators: FeatureUnion and make_union
-------------------------------------------------
FeatureUnion enables combining multiple transformer objects in parallel,
concatenating their outputs. Each transformer is applied to the same input
and the output features are horizontally stacked to form a single transformed
feature matrix. This is useful to combine heterogeneous feature extractors.

Key points
- Each transformer in a FeatureUnion must implement fit and transform.
- The resulting transform is the concatenation of the individual transforms.
- Like Pipeline, a convenience function make_union builds a FeatureUnion
  using automatic names.

Example
  >>> from sklearn.pipeline import FeatureUnion, make_union
  >>> from sklearn.decomposition import PCA
  >>> from sklearn.feature_extraction.text import TfidfVectorizer
  >>> union = FeatureUnion([
  ...     ('pca', PCA(n_components=5)),
  ...     ('tfidf', TfidfVectorizer())
  ... ])
  >>> X_transformed = union.fit_transform(X_raw)

Best practices and notes
- Use pipelines to avoid data leakage in model selection and evaluation.
- When enabling caching, understand that transformers are cloned before
  fitting; the original instances passed to the pipeline are not the fitted
  objects in the cache.
- When writing custom transformers intended for pipelines, implement at least
  fit and transform, and follow scikit-learn API conventions (including
  get_params / set_params) so they integrate cleanly with model selection
  tools.
- Pipelines forward available methods/properties from the final estimator
  when appropriate. Some availability checks are guarded so that attribute
  access reflects the capabilities of the final step.

Further reading
---------------
See the user guide section on pipelines and composition for worked examples,
advanced techniques (nested pipelines, column-wise transformers, and
metadata routing), and recipes for combining and tuning composite estimators.