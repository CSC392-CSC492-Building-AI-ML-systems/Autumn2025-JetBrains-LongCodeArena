Permutation feature importance
==============================

Permutation feature importance is a model inspection technique that quantifies the importance of each feature by measuring the change in a chosen score when the values of that feature are randomly permuted. The idea is simple: if permuting a feature's values decreases the model score significantly, the model relied on that feature for its predictions.

How it works
------------
1. Compute the baseline score of the fitted estimator on the provided (X, y) using the chosen scorer.
2. For each feature column, repeat n_repeats times:
   - Randomly permute the values of that column (optionally on a subsample of rows).
   - Compute the score of the estimator on the perturbed data.
3. For each feature and repetition compute the importance as baseline_score - permuted_score.
4. Aggregate results across repeats to yield the mean and standard deviation of the importances.

This implementation returns importances in the same sign convention as scikit-learn scorers (higher is better). Thus a positive importance means that permuting the feature decreases the score (the feature is useful). For metrics where lower is better (e.g. loss), use a scorer that returns higher-is-better values (for example, negative loss).

Parameters
----------
estimator : estimator instance
    A fitted estimator exposing a predict or other methods required by the chosen scorer.
    The estimator is used as-is (it is not refit).

X : array-like of shape (n_samples, n_features)
    Data on which to compute the feature importances. If a pandas DataFrame is
    passed, the column names are used as feature names in outputs.

y : array-like of shape (n_samples,) or None
    Targets corresponding to X. May be None if the scorer does not require y.

scoring : str, callable, list, tuple, dict or None, default=None
    A scikit-learn scoring name, callable, or a mapping / sequence of scorers.
    If None, the estimator's default scorer is used (if available). Multi-metric
    scoring is supported; in that case the result is a dict mapping scorer names
    to Bunch objects.

n_repeats : int, default=5
    Number of times to permute a feature. Larger values yield more precise
    estimates at the cost of additional computation.

random_state : int, RandomState instance or None, default=None
    Controls the randomness for permutations to ensure reproducibility.

n_jobs : int or None, default=None
    The number of jobs to run in parallel for computing importances.
    None or 1 means no parallelism. -1 means using all processors.

sample_weight : array-like of shape (n_samples,) or None, default=None
    Sample weights passed to the scorer when supported.

max_samples : int or float or None, default=None
    If int, the number of samples to draw without replacement to estimate
    importances. If float in (0, 1], the fraction of samples to draw.
    If None or >= n_samples, use all samples. Using a smaller value can
    reduce computation and memory use.

Returns
-------
importances : Bunch or dict
    If a single scorer is provided (or the estimator's default scorer is used),
    returns a Bunch with the following attributes:

    - importances_mean : ndarray of shape (n_features,)
        Mean importance for each feature (baseline_score - permuted_score).
    - importances_std : ndarray of shape (n_features,)
        Standard deviation of the importance across repeats.
    - importances : ndarray of shape (n_features, n_repeats)
        Raw importance values for each repeat.

    If multiple scorers are provided, returns a dict that maps scorer names
    to Bunch objects (one per scorer).

Notes
-----
- The method measures the drop (or increase) in model score when feature values
  are permuted. It does not measure causal effect nor whether a feature is
  correlated with a surviving feature; correlated features may share importance.
- Permutation importance is model-agnostic: it only requires evaluating the
  fitted estimator on perturbed inputs using the provided scorer.
- When X is a pandas DataFrame, column names are preserved in the returned
  arrays ordering and can be retrieved separately to label results. Duplicate
  feature names are not allowed.
- max_samples performs permutation on a subsample of the rows without
  replacement. The permutation is applied within the subsample to compute the
  permuted score.
- Use an appropriate scorer for the task. For example, to assess importance with
  respect to log loss, pass a scorer that returns higher-is-better (e.g.
  negative log loss).

Examples
--------
Compute permutation importance for a fitted estimator (classification example):

    from sklearn.ensemble import RandomForestClassifier
    from sklearn.inspection import permutation_importance
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split

    X, y = load_iris(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    clf = RandomForestClassifier(random_state=0).fit(X_train, y_train)

    result = permutation_importance(
        clf, X_test, y_test, n_repeats=10, random_state=0, n_jobs=2
    )
    # result is a Bunch with importances_mean, importances_std, and importances

For multi-metric scoring, pass a dict of scorers; the function will return a dict
mapping each scorer name to a Bunch with importances.

References
----------
- Breiman, L. (2001). Random Forests. Machine Learning.
- Fisher, A., Rudin, C., & Dominici, F. (2019). All models are wrong, but many are useful:
  Learning a variable's importance by studying an entire class of prediction models.