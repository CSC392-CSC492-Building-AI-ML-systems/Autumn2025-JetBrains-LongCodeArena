Decision Trees
==============

Decision Trees are a versatile and widely used class of supervised learning algorithms for classification and regression tasks. They work by recursively partitioning the input space into subsets based on feature values, aiming to create homogeneous groups with respect to the target variable.

Overview
--------

A decision tree is composed of nodes that represent tests on features, branches that correspond to the outcomes of these tests, and leaf nodes that represent the predicted output values or classes. The tree is built by selecting splits that optimize a certain criterion, which measures the quality of the split in terms of reducing impurity or variance.

Impurity Criteria
-----------------

The quality of a split in a decision tree is evaluated using impurity criteria. These criteria quantify how well a split separates the data into homogeneous subsets. Common impurity measures include Gini impurity, entropy (information gain), and variance reduction.

The `Criterion` interface defines the methods required to calculate the impurity and evaluate splits:

- **Initialization**: The criterion is initialized with the target values, sample weights, and indices of the samples considered at the current node.

- **Reset and Update**: Methods to reset the criterion statistics and update them when samples are moved between child nodes during the split evaluation.

- **Impurity Calculation**: Methods to compute the impurity of the current node and its children.

- **Node Value**: Computes the predicted value or class for the node based on the samples it contains.

- **Impurity Improvement**: Calculates the improvement in impurity achieved by a potential split, guiding the tree-building process to select the best splits.

Handling Missing Values
-----------------------

The criterion interface includes support for missing values by allowing initialization of statistics for missing samples. This enables the decision tree to handle datasets with incomplete information effectively.

Split Evaluation and Selection
------------------------------

During tree construction, the algorithm evaluates potential splits by moving samples between left and right child nodes and updating the criterion statistics accordingly. The split that maximizes the impurity improvement is selected.

A proxy measure of impurity improvement is also used to speed up the search for the best split by neglecting constant terms, allowing efficient pruning of the search space.

Node Prediction
---------------

Once a node is formed, its predicted value or class is computed using the node value method. This value is used for making predictions on new data points that fall into the node.

Monotonicity Constraints
------------------------

The criterion interface provides a method to compute the middle value of a split, which is useful for enforcing monotonicity constraints during tree construction. This ensures that the predictions respect specified monotonic relationships with the features.

Summary
-------

Decision Trees are powerful models that rely on impurity criteria to guide the recursive partitioning of data. The modular design of the criterion interface allows for flexible implementation of various impurity measures and supports advanced features such as handling missing values and monotonicity constraints. This makes decision trees a fundamental building block in many machine learning pipelines.