Stateless Learning Rate Schedulers
==================================

Overview
--------

This module provides stateless learning rate schedulers designed to address some of the limitations of PyTorch's built-in schedulers found in :mod:`torch.optim.lr_scheduler`. The primary goal of these schedulers is to integrate seamlessly with Composer's :mod:`~composer.core.time` abstraction, allowing schedulers to be configured using explicit and arbitrary time units such as epochs, batches, or durations.

Unlike traditional stateful schedulers, stateless schedulers compute the learning rate multiplier purely as a function of the current training state, without maintaining internal state. This design enables more flexible and composable scheduling strategies.

Key Concepts
------------

- **Stateless Scheduler**: A function or callable class that, given the current training state, returns a multiplier to adjust the optimizer's learning rate.
- **Learning Rate Multiplier (α)**: The output of a scheduler, representing the factor by which the initial learning rate should be scaled at a given time.
- **Scale Schedule Ratio (SSR)**: A parameter that allows the scheduler's timeline to be stretched or compressed, effectively scaling the schedule duration.

ComposerScheduler Interface
---------------------------

The core specification for stateless schedulers is defined by the `ComposerScheduler` protocol. Any function or callable class matching the following signature can serve as a scheduler:

```python
def scheduler(state: State, ssr: float = 1.0) -> float:
    ...
```

- **Parameters**:
  - `state` (:class:`~composer.core.State`): The current state of the trainer, including timing information.
  - `ssr` (float, optional): The scale schedule ratio, defaulting to 1.0. This parameter scales the scheduler's timeline such that the learning rate multiplier at time `t` with SSR `s` is equivalent to the multiplier at time `t / s` with SSR 1.

- **Returns**:
  - A float representing the learning rate multiplier α to apply to the optimizer's base learning rate.

Example Usage
-------------

A simple scheduler that halves the learning rate after 10 epochs can be implemented as a function:

```python
def ten_epoch_decay_scheduler(state: State) -> float:
    if state.timestamp.epoch < 10:
        return 1.0
    return 0.5
```

Alternatively, the same behavior can be encapsulated in a callable class:

```python
class VariableEpochDecayScheduler(ComposerScheduler):

    def __init__(self, num_epochs: int):
        self.num_epochs = num_epochs

    def __call__(self, state: State) -> float:
        if state.timestamp.epoch < self.num_epochs:
            return 1.0
        return 0.5

ten_epoch_decay_scheduler = VariableEpochDecayScheduler(num_epochs=10)
```

Both approaches are valid and interchangeable within Composer's training framework.

Time Conversion Utility
-----------------------

The module includes a utility function `_convert_time` that converts various time representations (strings, :class:`~composer.core.Time` objects) into a consistent :class:`~composer.core.Time` object with appropriate units, taking into account the current training state and the scale schedule ratio. This ensures that schedulers can operate with precise timing granularity, such as batches, even when configured with coarser units like epochs or durations.

Compiling to PyTorch Scheduler
------------------------------

The function `compile_composer_scheduler` converts a stateless `ComposerScheduler` into a PyTorch-compatible scheduler object (`PyTorchScheduler`). This allows the scheduler to be used with the familiar `.step()` interface while internally relying on the stateless computation based on the current training state.

This conversion facilitates integration with existing PyTorch training loops and tools, while preserving the benefits of stateless scheduling and explicit time unit handling.

Available Schedulers
--------------------

The module exports a variety of scheduler implementations, including but not limited to:

- `StepScheduler`
- `MultiStepScheduler`
- `ConstantScheduler`
- `LinearScheduler`
- `ExponentialScheduler`
- `CosineAnnealingScheduler`
- `CosineAnnealingWarmRestartsScheduler`
- `PolynomialScheduler`
- `MultiStepWithWarmupScheduler`
- `ConstantWithWarmupScheduler`
- `LinearWithWarmupScheduler`
- `CosineAnnealingWithWarmupScheduler`
- `PolynomialWithWarmupScheduler`

These schedulers provide a range of common learning rate schedules, often with support for warmup phases, and can be combined multiplicatively to create complex scheduling behaviors.

Notes
-----

- All schedulers return a multiplier rather than an absolute learning rate. The effective learning rate at time `t` is computed as:

  \[
  \eta(t) = \eta_i \times \alpha(t)
  \]

  where \(\eta_i\) is the initial learning rate and \(\alpha(t)\) is the scheduler output.

- Multiple schedulers can be used simultaneously, with their multipliers combined multiplicatively.

- The scale schedule ratio (SSR) allows dynamic adjustment of the schedule's speed without modifying the scheduler's internal logic.

References
----------

For more detailed information on the `ComposerScheduler` interface and examples, see :class:`~.ComposerScheduler`.

For integration with Composer's training framework and time abstractions, refer to :mod:`~composer.core.time` and :class:`~composer.core.State`.