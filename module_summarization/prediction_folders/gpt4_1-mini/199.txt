Neural Network Models (Supervised)
==================================

Overview
--------

This module provides utilities and functions for building and training supervised neural network models. It includes implementations of common activation functions and their derivatives, as well as loss functions used for regression and classification tasks.

Activation Functions
--------------------

Activation functions introduce non-linearity into the neural network, enabling it to learn complex patterns. The following activation functions are implemented with in-place computation for efficiency:

- **Identity**  
  Leaves the input unchanged.  
  Function: \( f(x) = x \)

- **Logistic (Sigmoid)**  
  Computes the logistic sigmoid function.  
  Function: \( f(x) = \frac{1}{1 + e^{-x}} \)

- **Tanh (Hyperbolic Tangent)**  
  Computes the hyperbolic tangent function.  
  Function: \( f(x) = \tanh(x) \)

- **ReLU (Rectified Linear Unit)**  
  Computes the rectified linear unit function.  
  Function: \( f(x) = \max(0, x) \)

- **Softmax**  
  Computes the K-way softmax function, typically used in the output layer for multi-class classification.  
  Function:  
  \[
  f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
  \]

Each activation function modifies the input array in-place to optimize memory usage.

Activation Function Derivatives
-------------------------------

The derivatives of activation functions are used during backpropagation to compute gradients. The following derivatives are implemented with in-place modification of the backpropagated error signal:

- **Identity Derivative**  
  No change to the error signal since the derivative of identity is 1.

- **Logistic Derivative**  
  Uses the output of the logistic function \( Z \) to compute the derivative:  
  \[
  f'(x) = Z \times (1 - Z)
  \]

- **Tanh Derivative**  
  Uses the output of the tanh function \( Z \) to compute the derivative:  
  \[
  f'(x) = 1 - Z^2
  \]

- **ReLU Derivative**  
  Sets the gradient to zero where the output \( Z \) is zero, reflecting the piecewise nature of ReLU.

Loss Functions
--------------

Loss functions measure the discrepancy between the predicted outputs and the true targets, guiding the optimization process.

- **Squared Loss (Regression)**  
  Computes the mean squared error divided by 2:  
  \[
  \text{loss} = \frac{1}{2n} \sum_{i=1}^n (y_{\text{true},i} - y_{\text{pred},i})^2
  \]  
  Used for regression tasks.

- **Log Loss (Classification)**  
  Computes the logistic loss for multi-class classification:  
  \[
  \text{loss} = -\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^k y_{\text{true},ij} \log(y_{\text{prob},ij})
  \]  
  Where \( y_{\text{prob}} \) are predicted probabilities clipped to avoid numerical instability.

- **Binary Log Loss**  
  Computes logistic loss for binary classification or multilabel classification:  
  \[
  \text{loss} = -\frac{1}{n} \sum_{i=1}^n \left[ y_{\text{true},i} \log(y_{\text{prob},i}) + (1 - y_{\text{true},i}) \log(1 - y_{\text{prob},i}) \right]
  \]

Usage
-----

These utilities are designed to be used internally within neural network implementations to perform forward and backward passes efficiently. Activation functions and their derivatives are applied in-place to minimize memory overhead, and loss functions provide the necessary feedback for model training.

Author and License
------------------

- Author: Issam H. Laradji <issam.laradji@gmail.com>  
- License: BSD 3-Clause License