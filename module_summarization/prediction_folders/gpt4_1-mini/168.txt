Feature Extraction for Model Inspection
=======================================

This module provides utilities to extract intermediate features from PyTorch models using the FX graph tracing framework. Feature extraction is useful for model inspection, debugging, and understanding the internal representations learned by a model.

Key Components
--------------

- **create_feature_extractor**: A function to create a new `torch.fx.GraphModule` that returns intermediate activations from specified nodes in the original model's computation graph.

- **get_graph_node_names**: A utility to retrieve the names of all nodes in a model's FX graph, which can be used to specify which features to extract.

Tracing with Leaf Modules
-------------------------

The feature extraction utilities use a custom tracer, `LeafModuleAwareTracer`, which extends `torch.fx.Tracer`. This tracer allows specifying a set of *leaf modules* â€” modules that should not be traced through. Instead, calls to these leaf modules are represented as single nodes in the FX graph. This behavior simplifies the graph and makes feature extraction more intuitive by treating complex submodules as atomic units.

NodePathTracer: Tracking Node Names
-----------------------------------

`NodePathTracer` extends `LeafModuleAwareTracer` to record the qualified names of nodes during tracing. The qualified name is a dot-separated path that identifies the node's position in the module hierarchy, excluding the top-level module name. For example, if a model's forward method applies a ReLU module named `relu`, the node name will be simply `"relu"`.

Features of `NodePathTracer` include:

- Maintaining a mapping from each FX `Node` to its qualified name (`node_to_qualname`).
- Ensuring unique node names by appending suffixes like `_1`, `_2` when duplicates occur.
- Recording nodes in the order they are executed during tracing.

This detailed naming scheme helps users specify exactly which intermediate activations to extract.

Handling Differences Between Training and Evaluation Modes
----------------------------------------------------------

Models may have different computation graphs in training and evaluation modes (e.g., due to dropout or batch normalization). The utility function `_warn_graph_differences` compares the traced nodes between these modes and warns the user if there are discrepancies. This helps ensure that feature extraction nodes are correctly specified for both modes.

Usage Overview
--------------

1. **Identify Nodes to Extract**: Use `get_graph_node_names` to list all node names in the model's FX graph.

2. **Create Feature Extractor**: Call `create_feature_extractor` with the model and a list of node names to extract. This returns a new `GraphModule` that outputs the specified intermediate features.

3. **Run Model and Inspect Features**: Use the feature extractor as a normal model. The output will be a dictionary mapping node names to their corresponding activations.

Benefits
--------

- Enables introspection of intermediate layers without modifying the original model code.
- Supports complex models with nested modules by treating specified modules as leaves.
- Provides clear and unique naming for extracted features.
- Warns about potential inconsistencies between training and evaluation graphs.

Example
-------

```python
from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names
import torchvision.models as models

model = models.resnet18(pretrained=True)
# List all node names
node_names = get_graph_node_names(model)
print(node_names)

# Specify nodes to extract features from
return_nodes = ['layer1.0.relu', 'layer3.1.conv2']

# Create feature extractor
feature_extractor = create_feature_extractor(model, return_nodes=return_nodes)

# Run input through feature extractor
input_tensor = torch.randn(1, 3, 224, 224)
features = feature_extractor(input_tensor)

# features is a dict mapping node names to activations
for name, activation in features.items():
    print(f"{name}: {activation.shape}")
```

This approach facilitates detailed model inspection and debugging by exposing internal activations in a flexible and user-friendly manner.