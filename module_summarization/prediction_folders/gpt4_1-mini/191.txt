Validation Curves: Plotting Scores to Evaluate Models
======================================================

Validation curves are a useful tool to evaluate the performance of a machine learning model with respect to a specific hyperparameter. By plotting the training and validation scores for different values of the hyperparameter, one can diagnose whether the model is underfitting, overfitting, or has a good fit.

Overview
--------

A validation curve shows how the model’s score varies as a function of a hyperparameter value. Typically, the curve plots the training score and the validation (test) score for a range of values of the hyperparameter. This visualization helps in selecting the optimal hyperparameter value that balances bias and variance.

Key Concepts
------------

- **Training Scores**: The scores obtained by the model on the training dataset for each hyperparameter value.
- **Validation Scores**: The scores obtained by the model on a validation or test dataset for each hyperparameter value.
- **Standard Deviation**: The variability of the scores across different cross-validation folds, often visualized as shaded regions or error bars around the mean scores.
- **Score Type**: You can choose to plot training scores, validation scores, or both.
- **Score Name**: The metric used to evaluate the model’s performance (e.g., accuracy, F1-score, R²).

Plotting Validation Curves
--------------------------

The validation curve plot typically includes:

- The x-axis representing the values of the hyperparameter.
- The y-axis representing the score (e.g., accuracy).
- Lines for training and validation scores showing the mean score across cross-validation folds.
- Shaded regions or error bars representing the standard deviation of the scores.

Visualization Styles
--------------------

There are multiple ways to display the variability of the scores:

- **Fill Between**: A shaded region between (mean - std) and (mean + std) around the score lines.
- **Error Bars**: Error bars extending above and below the mean score points.
- **None**: No variability is shown, only the mean scores.

Interpreting Validation Curves
------------------------------

- If the training and validation scores are both low and close, the model is likely underfitting.
- If the training score is high but the validation score is low, the model is likely overfitting.
- If both scores are high and close, the model has a good fit.

Example Usage
-------------

Below is an example of how a validation curve might be generated and visualized:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import validation_curve
from sklearn.svm import SVC
from sklearn.datasets import load_digits

# Load data
X, y = load_digits(return_X_y=True)

# Define the model and hyperparameter range
param_range = np.logspace(-6, -1, 5)
train_scores, test_scores = validation_curve(
    SVC(), X, y, param_name="gamma", param_range=param_range,
    scoring="accuracy", n_jobs=1, cv=5
)

# Calculate mean and std
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot validation curve
plt.figure()
plt.title("Validation Curve with SVM")
plt.xlabel("Gamma")
plt.ylabel("Score")
plt.semilogx(param_range, train_mean, label="Training score", color="r")
plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color="r")
plt.semilogx(param_range, test_mean, label="Cross-validation score", color="g")
plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2, color="g")
plt.legend(loc="best")
plt.show()
```

Best Practices
--------------

- Use cross-validation to obtain reliable estimates of training and validation scores.
- Choose a range of hyperparameter values that covers underfitting and overfitting regimes.
- Use log scale for hyperparameters that span several orders of magnitude.
- Visualize both mean scores and variability to understand the stability of the model.

Related Functions and Classes
-----------------------------

- `validation_curve`: Compute training and test scores for varying hyperparameter values.
- `LearningCurveDisplay` and similar visualization utilities can be adapted to plot validation curves.
- Use matplotlib’s plotting functions to customize the appearance of the validation curve.

Summary
-------

Validation curves provide an intuitive way to understand how a model’s performance changes with respect to a hyperparameter. By analyzing these curves, practitioners can make informed decisions about hyperparameter tuning to improve model generalization.