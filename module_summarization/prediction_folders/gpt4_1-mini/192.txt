Manifold Learning
=================

Manifold learning is a class of algorithms for non-linear dimensionality reduction. Unlike linear methods such as Principal Component Analysis (PCA), manifold learning techniques aim to uncover a low-dimensional manifold embedded within the high-dimensional data space. These methods are particularly useful when the data lies on or near a curved manifold, and linear methods fail to capture the intrinsic geometry.

Overview
--------

The goal of manifold learning is to find a meaningful low-dimensional representation of high-dimensional data by preserving certain geometric or topological properties. This is achieved by assuming that the data points lie on a smooth manifold of much lower dimension than the ambient space.

Common manifold learning algorithms include:

- **Multi-dimensional Scaling (MDS)**: Attempts to preserve pairwise distances between points.
- **Isomap**: Extends MDS by preserving geodesic distances on a neighborhood graph.
- **Locally Linear Embedding (LLE)**: Preserves local neighborhood relationships.
- **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: Focuses on preserving local similarities probabilistically.

Multi-dimensional Scaling (MDS)
-------------------------------

Multi-dimensional Scaling (MDS) is a classical manifold learning technique that seeks a low-dimensional embedding of data points such that the distances between points in the embedding space approximate the dissimilarities (distances) in the original space.

### SMACOF Algorithm

The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is an iterative majorization technique used to solve the MDS optimization problem. It minimizes a stress function that measures the discrepancy between the distances in the embedding space and the original dissimilarities.

Key features of the SMACOF implementation:

- Supports both **metric** and **non-metric** MDS.
- Allows specification of the number of embedding dimensions.
- Can initialize the embedding with a random configuration or a user-provided starting point.
- Iteratively updates the embedding to reduce stress until convergence or a maximum number of iterations is reached.
- Provides options for verbosity and convergence tolerance.
- Supports normalized stress calculation in non-metric MDS.

### Parameters

- **dissimilarities**: A symmetric matrix of pairwise dissimilarities between samples.
- **metric**: Boolean flag to choose between metric (True) and non-metric (False) MDS.
- **n_components**: Number of dimensions for the embedding space.
- **init**: Initial configuration of points in the embedding space.
- **max_iter**: Maximum number of iterations for the algorithm.
- **verbose**: Controls the verbosity of the output.
- **eps**: Convergence tolerance for the relative change in stress.
- **random_state**: Seed or random number generator for reproducibility.
- **normalized_stress**: Whether to compute and return normalized stress (Stress-1) in non-metric MDS.

### Returns

- **X**: Coordinates of the points in the low-dimensional embedding space.
- **stress**: Final value of the stress function indicating the quality of the embedding.
- **n_iter**: Number of iterations run until convergence.

### Interpretation of Stress Values

- 0: Perfect fit
- 0.025: Excellent fit
- 0.05: Good fit
- 0.1: Fair fit
- 0.2: Poor fit

References
----------

- Kruskal, J. (1964). Nonmetric multidimensional scaling: a numerical method. *Psychometrika*, 29.
- Kruskal, J. (1964). Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. *Psychometrika*, 29.
- Borg, I., & Groenen, P. (1997). *Modern Multidimensional Scaling - Theory and Applications*. Springer Series in Statistics.

Summary
-------

Manifold learning methods such as MDS provide powerful tools for visualizing and understanding complex high-dimensional data by uncovering low-dimensional structures. The SMACOF algorithm offers an efficient and flexible approach to perform MDS, supporting both metric and non-metric variants, and enabling insightful embeddings that preserve the intrinsic geometry of the data.