Kernel Ridge Regression (:mod:`sklearn.kernel_ridge`)
=======================================================

The :mod:`sklearn.kernel_ridge` module implements Kernel Ridge Regression (KRR), a regression technique that combines ridge regression with the kernel trick. This allows learning a linear function in a high-dimensional feature space induced by a kernel, enabling nonlinear regression in the original input space.

Overview
--------

Kernel Ridge Regression (KRR) integrates ridge regression (linear least squares with L2-norm regularization) and kernel methods. It learns a linear model in the kernel-induced feature space, which corresponds to a nonlinear function in the original input space when using nonlinear kernels.

KRR shares the same model form as Support Vector Regression (SVR) but differs in the loss function: KRR uses squared error loss, whereas SVR uses epsilon-insensitive loss. Unlike SVR, KRR fitting can be done in closed form, often resulting in faster training on medium-sized datasets. However, KRR models are typically non-sparse, which can lead to slower prediction times compared to SVR models that are sparse for epsilon > 0.

This module supports multi-output regression, allowing the target variable `y` to be a 2D array with shape `(n_samples, n_targets)`.

Class: KernelRidge
------------------

.. autoclass:: sklearn.kernel_ridge.KernelRidge
   :members:
   :show-inheritance:

### Parameters

- **alpha** : float or array-like of shape (n_targets,), default=1.0  
  Regularization strength; must be positive. Larger values imply stronger regularization, improving conditioning and reducing variance. If an array is provided, each target has its own regularization parameter.

- **kernel** : str or callable, default="linear"  
  Kernel function to use. Can be a string identifier of a kernel supported by `sklearn.metrics.pairwise.pairwise_kernels` or "precomputed" if the input is a kernel matrix. Alternatively, a callable that takes two samples and returns a kernel value.

- **gamma** : float, default=None  
  Kernel coefficient for RBF, Laplacian, polynomial, exponential chi2, and sigmoid kernels. Ignored by other kernels.

- **degree** : int, default=3  
  Degree for polynomial kernel. Ignored by other kernels.

- **coef0** : float, default=1  
  Independent term in polynomial and sigmoid kernels. Ignored by other kernels.

- **kernel_params** : dict, default=None  
  Additional parameters for the kernel function if a callable is used.

### Attributes

- **dual_coef_** : ndarray of shape (n_samples,) or (n_samples, n_targets)  
  Weight vector(s) in the kernel space.

- **X_fit_** : ndarray or sparse matrix of shape (n_samples, n_features)  
  Training data used for fitting. If `kernel="precomputed"`, this is the precomputed kernel matrix.

- **n_features_in_** : int  
  Number of features seen during fit.

- **feature_names_in_** : ndarray of shape (n_features_in_,)  
  Names of features seen during fit, if available.

### See Also

- :class:`sklearn.gaussian_process.GaussianProcessRegressor`  
  Gaussian Process regression with automatic kernel hyperparameter tuning and uncertainty estimates.

- :class:`sklearn.linear_model.Ridge`  
  Linear ridge regression.

- :class:`sklearn.linear_model.RidgeCV`  
  Ridge regression with built-in cross-validation.

- :class:`sklearn.svm.SVR`  
  Support Vector Regression with various kernels.

### References

- Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. The MIT Press. Chapter 14.4.3, pp. 492-493.

### Example

.. code-block:: python

    import numpy as np
    from sklearn.kernel_ridge import KernelRidge

    rng = np.random.RandomState(0)
    n_samples, n_features = 10, 5
    X = rng.randn(n_samples, n_features)
    y = rng.randn(n_samples)

    krr = KernelRidge(alpha=1.0)
    krr.fit(X, y)
    y_pred = krr.predict(X)

Additional Information
----------------------

For more details and usage examples, see the :ref:`User Guide <kernel_ridge>`.

License
-------

This module is distributed under the BSD 3-Clause License.

Authors
-------

- Mathieu Blondel <mathieu@mblondel.org>  
- Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>