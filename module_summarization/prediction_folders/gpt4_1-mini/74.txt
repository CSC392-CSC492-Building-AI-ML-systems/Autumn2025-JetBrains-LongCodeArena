Random Projection Transformers
==============================

Overview
--------

Random Projections provide a simple and computationally efficient technique to reduce the dimensionality of data. By allowing a controlled amount of accuracy loss (manifested as additional variance), random projections enable faster processing times and smaller model sizes. This is particularly useful in high-dimensional data scenarios where dimensionality reduction is essential for performance and scalability.

The key property of random projection matrices is that they are constructed to approximately preserve pairwise distances between samples in the dataset. This ensures that the geometric structure of the data is maintained after projection.

The theoretical foundation of random projections is based on the Johnson-Lindenstrauss lemma, which states that a small set of points in a high-dimensional space can be embedded into a much lower-dimensional space such that the distances between the points are nearly preserved. The embedding map can be taken as a random projection, often an orthogonal projection or a projection using a random Gaussian matrix.

Johnson-Lindenstrauss Lemma
----------------------------

The Johnson-Lindenstrauss lemma guarantees that for any 0 < ε < 1, and any integer \( n \), there exists a mapping \( f: \mathbb{R}^d \to \mathbb{R}^k \) with \( k = O(\log n / \epsilon^2) \) such that for all points \( u, v \) in a set of \( n \) points:

\[
(1 - \epsilon) \|u - v\|^2 \leq \|f(u) - f(v)\|^2 \leq (1 + \epsilon) \|u - v\|^2
\]

This lemma underpins the use of random projections for dimensionality reduction with controlled distortion.

Module Contents
---------------

The module provides the following main components:

### Functions

- **johnson_lindenstrauss_min_dim(n_samples, *, eps=0.1)**

  Computes the minimum number of components required to guarantee, with high probability, an ε-embedding of a dataset with `n_samples` samples. The function uses the Johnson-Lindenstrauss lemma to determine the safe dimensionality for random projection.

  Parameters:
  - `n_samples` : int or array-like of int  
    Number of samples in the dataset. Must be greater than zero.
  - `eps` : float or array-like of floats, default=0.1  
    Maximum distortion rate allowed in the embedding, must be in the interval (0, 1).

  Returns:
  - `n_components` : int or ndarray of int  
    The minimal number of components to guarantee an ε-embedding.

  Example:
  ```python
  from sklearn.random_projection import johnson_lindenstrauss_min_dim
  n_components = johnson_lindenstrauss_min_dim(1e6, eps=0.5)
  print(n_components)  # Output: 663
  ```

### Classes

- **SparseRandomProjection**

  Implements a sparse random projection transformer. It projects data to a lower-dimensional space using a sparse random matrix, which is computationally efficient for high-dimensional sparse data.

- **GaussianRandomProjection**

  Implements a Gaussian random projection transformer. It projects data using a dense random matrix with entries drawn from a Gaussian distribution \( \mathcal{N}(0, 1/n\_components) \).

Utility Functions
-----------------

- **_check_density(density, n_features)**

  Validates and computes the density parameter for sparse random projection matrices. If set to `"auto"`, it defaults to \( 1 / \sqrt{n\_features} \).

- **_check_input_size(n_components, n_features)**

  Validates that the number of components and features are strictly positive integers.

- **_gaussian_random_matrix(n_components, n_features, random_state=None)**

  Generates a dense Gaussian random matrix with entries drawn from \( \mathcal{N}(0, 1/n\_components) \). This matrix is used for Gaussian random projection.

Usage Notes
-----------

- Random projections are particularly useful when the original feature space is very high-dimensional, and a lower-dimensional embedding is desired without significant loss of information.

- The choice of the number of components (`n_components`) is critical and can be guided by the `johnson_lindenstrauss_min_dim` function to ensure the embedding preserves distances within a specified distortion level.

- Sparse random projections are more efficient for sparse input data, while Gaussian random projections are suitable for dense data.

References
----------

- Johnson, W. B., & Lindenstrauss, J. (1984). Extensions of Lipschitz mappings into a Hilbert space. *Contemporary Mathematics*, 26, 189–206.

- Wikipedia contributors. Johnson–Lindenstrauss lemma. *Wikipedia, The Free Encyclopedia*.  
  https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma

- Dasgupta, S., & Gupta, A. (1999). An elementary proof of the Johnson-Lindenstrauss lemma.  
  https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.3400

Examples
--------

```python
from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection

# Create a Gaussian random projection transformer
grp = GaussianRandomProjection(n_components=100, random_state=42)
X_new = grp.fit_transform(X)

# Create a sparse random projection transformer
srp = SparseRandomProjection(n_components=100, density='auto', random_state=42)
X_new_sparse = srp.fit_transform(X_sparse)
```

See Also
--------

- :ref:`User Guide <random_projection>` for detailed explanations and examples.

License
-------

This module is licensed under the BSD 3-Clause License.

Authors
-------

- Olivier Grisel <olivier.grisel@ensta.org>  
- Arnaud Joly <a.joly@ulg.ac.be>