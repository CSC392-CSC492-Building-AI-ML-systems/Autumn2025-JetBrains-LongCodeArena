Feature Extraction from Text
============================

The `sklearn.feature_extraction.text` submodule provides utilities to build feature vectors from text documents. It includes classes and functions for tokenizing, preprocessing, and transforming text data into numerical feature representations suitable for machine learning models.

Overview
--------

This submodule offers several key components:

- **Vectorizers**: Convert a collection of text documents to a matrix of token counts or TF-IDF features.
- **Transformers**: Transform count matrices to normalized TF-IDF representations.
- **Text preprocessing utilities**: Functions to strip accents, remove HTML tags, and handle stop words.

Main Classes and Functions
--------------------------

### Vectorizers

- **CountVectorizer**  
  Converts a collection of text documents to a matrix of token counts. It supports tokenization, stop word filtering, n-gram extraction, and text preprocessing.

- **HashingVectorizer**  
  Uses a hashing trick to convert text documents to a matrix of token occurrences, enabling efficient and scalable feature extraction without storing a vocabulary.

- **TfidfVectorizer**  
  Combines the functionality of `CountVectorizer` and `TfidfTransformer` to convert text documents directly into TF-IDF features.

### Transformers

- **TfidfTransformer**  
  Transforms a count matrix to a normalized TF-IDF representation.

### Utilities

- **strip_accents_unicode(s)**  
  Removes accentuated unicode symbols by decomposing them into their base characters. This method is slower but more general.

- **strip_accents_ascii(s)**  
  Removes accentuated unicode symbols by transliterating them to ASCII characters. This method is faster but only suitable for languages with direct ASCII equivalents.

- **strip_tags(s)**  
  Removes HTML or XML tags from a string using a basic regular expression.

- **ENGLISH_STOP_WORDS**  
  A built-in set of English stop words for filtering common words during tokenization.

Internal Functions
------------------

- **_preprocess(doc, accent_function=None, lower=False)**  
  Applies optional preprocessing steps such as lowercasing and accent stripping to a document string.

- **_analyze(doc, analyzer=None, tokenizer=None, ngrams=None, preprocessor=None, decoder=None, stop_words=None)**  
  Processes a document through a customizable pipeline of decoding, preprocessing, tokenizing, n-gram extraction, and stop word filtering.

- **_check_stop_list(stop)**  
  Validates and returns the appropriate stop word list based on the input parameter.

Mixin Classes
-------------

- **_VectorizerMixin**  
  Provides common functionality for text vectorizers, including decoding input documents from various formats and handling whitespace normalization.

Usage Notes
-----------

- The vectorizers support input as raw text, filenames, or file-like objects.
- Stop word filtering can be customized by passing a built-in stop word list name, a custom collection, or disabling it.
- Accent stripping functions help normalize text for languages with accented characters.
- For serious HTML/XML preprocessing, external libraries such as BeautifulSoup or lxml are recommended over the basic `strip_tags` function.

Authors and License
-------------------

This module is authored by Olivier Grisel, Mathieu Blondel, Lars Buitinck, Robert Layton, Jochen Wersd√∂rfer, and Roman Sinayev.

It is distributed under the BSD 3-Clause License.

---

This submodule is an essential part of scikit-learn's text processing pipeline, enabling efficient and flexible feature extraction from raw text data for machine learning applications.