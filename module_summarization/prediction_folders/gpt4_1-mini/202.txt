Permutation Feature Importance
==============================

Permutation feature importance is a model inspection technique that can be used to interpret the importance of features in a fitted estimator. It measures the increase in the prediction error of the model after permuting the feature’s values, which breaks the relationship between the feature and the target. A feature is considered important if shuffling its values increases the model error, because the model relied on the feature for the prediction.

Overview
--------

Permutation importance works by randomly shuffling the values of each feature one at a time, and evaluating the model performance on the modified dataset. The decrease in the model score indicates the importance of the feature. This method can be applied to any fitted estimator and any scoring metric.

Key Functions and Utilities
---------------------------

- **_check_feature_names(X, feature_names=None)**  
  Validates and returns the feature names for the input data `X`. If `feature_names` is not provided, it attempts to infer them from the input data, such as column names of a pandas DataFrame or generates generic names for NumPy arrays.

- **_get_feature_index(fx, feature_names=None)**  
  Converts a feature name or index to a feature index integer. Raises an error if the feature name is not found in the provided feature names.

- **_weights_scorer(scorer, estimator, X, y, sample_weight)**  
  Computes the score of the estimator on data `(X, y)` using the provided scorer, optionally using sample weights.

- **_calculate_permutation_scores(estimator, X, y, sample_weight, col_idx, random_state, n_repeats, scorer, max_samples)**  
  Calculates the scores of the estimator when the feature column at `col_idx` is permuted. This is repeated `n_repeats` times to obtain a distribution of scores. Supports subsampling of the dataset via `max_samples` for efficiency.

- **_create_importances_bunch(baseline_score, permuted_score)**  
  Computes the permutation importances as the difference between the baseline score and the permuted scores. Returns a `Bunch` object containing the mean importance, standard deviation, and raw importance scores for each feature.

Permutation Importance API
--------------------------

The main API function is `permutation_importance`, which has the following signature:

```python
permutation_importance(
    estimator,
    X,
    y,
    *,
    scoring=None,
    n_repeats=5,
    n_jobs=None,
    random_state=None,
    sample_weight=None,
    max_samples=None,
)
```

Parameters
~~~~~~~~~~

- **estimator** : estimator object  
  A fitted estimator implementing `predict` or `predict_proba`.

- **X** : array-like of shape (n_samples, n_features)  
  The data on which permutation importance will be computed.

- **y** : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None  
  The target variable for supervised learning problems.

- **scoring** : string, callable, list, tuple, dict or None, default=None  
  A single string or a callable to evaluate the predictions on the test set. If `None`, the estimator’s default scorer is used.

- **n_repeats** : int, default=5  
  Number of times to permute a feature. The importance is averaged over these repetitions.

- **n_jobs** : int or None, default=None  
  Number of jobs to run in parallel. `None` means 1 unless in a joblib.parallel_backend context.

- **random_state** : int, RandomState instance or None, default=None  
  Controls the randomness of the permutation.

- **sample_weight** : array-like of shape (n_samples,), default=None  
  Sample weights.

- **max_samples** : int or float, default=None  
  If not None, randomly sample `max_samples` samples to speed up the computation. If float, it is a fraction of the dataset size.

Returns
~~~~~~~

- **result** : Bunch  
  Dictionary-like object with the following attributes:

  - **importances_mean** : ndarray of shape (n_features,)  
    Mean of feature importance over `n_repeats`.

  - **importances_std** : ndarray of shape (n_features,)  
    Standard deviation of feature importance over `n_repeats`.

  - **importances** : ndarray of shape (n_features, n_repeats)  
    Raw permutation importance scores.

Usage Notes
-----------

- Permutation importance can be used with any fitted estimator and any scoring metric.

- It is model-agnostic and does not require retraining the model.

- The method can be computationally expensive for large datasets or many features, but can be sped up by using the `max_samples` parameter and parallel computation via `n_jobs`.

- The importance values are relative to the scoring metric used; a higher decrease in score after permutation indicates a more important feature.

- The method assumes that the features are independent; correlated features can lead to misleading importance values.

Example
-------

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

clf = RandomForestClassifier(random_state=0)
clf.fit(X_train, y_train)

result = permutation_importance(clf, X_test, y_test, n_repeats=10, random_state=0)

for i in result.importances_mean.argsort()[::-1]:
    print(f"Feature {i}: {result.importances_mean[i]:.3f} +/- {result.importances_std[i]:.3f}")
```

References
----------

- Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5–32.  
- Fisher, A., Rudin, C., & Dominici, F. (2019). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. Journal of Machine Learning Research, 20(177), 1–81.

See Also
--------

- `sklearn.inspection.partial_dependence` — Partial dependence plots for visualizing feature effects.  
- `sklearn.feature_selection` — Feature selection utilities.