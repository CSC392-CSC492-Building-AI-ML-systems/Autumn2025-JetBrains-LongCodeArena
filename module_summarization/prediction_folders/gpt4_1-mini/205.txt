Stochastic Gradient Descent (SGD)
==================================

Overview
--------

Stochastic Gradient Descent (SGD) is an iterative optimization algorithm used for minimizing an objective function that is typically a sum of differentiable functions. It is widely used in machine learning for training models such as linear classifiers, logistic regression, and neural networks.

Unlike traditional gradient descent which computes the gradient using the entire dataset, SGD updates the model parameters using the gradient computed from a single or a small batch of training samples. This often leads to faster convergence, especially for large datasets.

Key Concepts
------------

- **Loss Function**: The objective function to minimize. Common loss functions include squared loss for regression and logistic loss for classification.
- **Gradient**: The vector of partial derivatives of the loss function with respect to the model parameters.
- **Learning Rate**: A hyperparameter that controls the step size at each iteration while moving toward a minimum of the loss function.
- **Sample Weighting**: Each training sample can be assigned a weight to emphasize its importance during training.
- **Regularization**: Techniques such as L1 or L2 regularization can be applied to prevent overfitting by penalizing large weights.

Multinomial Logistic Loss
-------------------------

SGD can be used to optimize the multinomial logistic loss, which is suitable for multi-class classification problems. The multinomial logistic loss for a single sample is defined as:

.. math::

    \text{loss} = w_s \left( \log \sum_c \exp(\text{prediction}_c) - \text{prediction}_y \right)

where:

- :math:`\text{prediction}_c` is the predicted score for class :math:`c`.
- :math:`y` is the true class label.
- :math:`w_s` is the sample weight.

The gradient of the multinomial logistic loss with respect to the prediction for class :math:`c` is:

.. math::

    \text{grad}_c = w_s \left( p_c - \delta_{y,c} \right)

where:

- :math:`p_c = \frac{\exp(\text{prediction}_c)}{\sum_k \exp(\text{prediction}_k)}` is the predicted probability for class :math:`c`.
- :math:`\delta_{y,c}` is the Kronecker delta, equal to 1 if :math:`y = c` and 0 otherwise.

This gradient is then used to update the model parameters.

Numerical Stability
-------------------

To improve numerical stability when computing the log-sum-exp term, the implementation uses the identity:

.. math::

    \log \sum_i \exp(x_i) = m + \log \sum_i \exp(x_i - m)

where :math:`m = \max_i x_i`. This helps to avoid overflow or underflow issues during exponentiation.

Soft Thresholding
-----------------

Soft thresholding is used in the context of regularization, particularly for L1 regularization (Lasso). It is defined as:

.. math::

    S(x, \lambda) = \text{sign}(x) \max(|x| - \lambda, 0)

This operation shrinks the coefficients towards zero by the shrinkage parameter :math:`\lambda`, promoting sparsity in the model parameters.

Implementation Details
----------------------

- The code supports multiple data types (float32 and float64) for numerical computations.
- Loss functions and their gradients are implemented in Cython for performance.
- The implementation includes utilities for sequential datasets and efficient mathematical operations.
- The multinomial logistic loss and its gradient are implemented as Cython classes for fast computation.
- The code is designed to be extensible and maintainable, with clear separation between loss functions and optimization routines.

References
----------

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. (Chapter 4.3.4)
- Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In *Proceedings of COMPSTAT'2010* (pp. 177-186). Springer.

License
-------

The implementation is licensed under the BSD 3-Clause License.

Authors
-------

- Danny Sullivan <dbsullivan23@gmail.com>
- Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
- Arthur Mensch <arthur.mensch@m4x.org>
- Arthur Imbert <arthurimbert05@gmail.com>
- Joan Massich <mailsik@gmail.com>