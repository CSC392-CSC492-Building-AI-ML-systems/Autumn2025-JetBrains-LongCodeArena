Connect to Remote Data
======================

The `read_bytes` function provides a convenient way to connect to and read data from remote filesystems or local filesystems using URLs or file paths. It supports reading from single or multiple files, including those specified by glob patterns, and can handle various storage protocols such as `s3://`, `hdfs://`, and others, provided the necessary libraries are installed.

Key Features
------------

- Supports reading from local and remote filesystems transparently.
- Accepts single file paths, lists of paths, or glob patterns.
- Supports chunked reading of files by specifying a block size.
- Can split data cleanly on a specified delimiter (e.g., newline).
- Supports compression formats that allow efficient random access.
- Returns Dask delayed objects for lazy and parallel computation.
- Optionally returns a sample of the file header for inspection.
- Optionally includes file paths alongside the data blocks.

Function Signature
------------------

```python
read_bytes(
    urlpath,
    delimiter=None,
    not_zero=False,
    blocksize="128 MiB",
    sample="10 kiB",
    compression=None,
    include_path=False,
    **kwargs,
)
```

Parameters
----------

- **urlpath** : str or list  
  The path(s) to the file(s) to read. Can be a single file path, a list of paths, or a glob pattern. Paths can include a protocol prefix such as `s3://` or `hdfs://` to specify remote filesystems.

- **delimiter** : bytes, optional  
  A byte string delimiter on which to split the data blocks. For example, `b'\n'` to split on newlines.

- **not_zero** : bool, optional  
  If `True`, forces the first block to start after the first delimiter, effectively skipping a header row.

- **blocksize** : int or str, optional  
  The size of each data chunk in bytes. Can be an integer or a human-readable string like `"128 MiB"`. If `None`, the entire file is read as a single block.

- **sample** : int, str, or bool, optional  
  Specifies whether to return a sample of the file header. Can be `False` to disable sampling, `True` to use the default sample size (`"10 kiB"`), or an integer/string specifying the sample size.

- **compression** : str or None, optional  
  Compression codec to use, e.g., `'gzip'`, `'xz'`. Must support efficient random access. Use `'infer'` to guess based on file extension.

- **include_path** : bool, optional  
  If `True`, the returned output includes the list of file paths alongside the data blocks.

- **kwargs** : dict, optional  
  Additional parameters passed to the underlying filesystem connection, such as authentication credentials or connection options.

Returns
-------

- **sample** : bytes  
  A sample of the file header, useful for inspecting the data format.

- **blocks** : list of lists of `dask.Delayed` objects  
  Each inner list corresponds to a file, containing delayed objects that read chunks of bytes from that file.

- **paths** : list of str (only if `include_path=True`)  
  The list of file paths corresponding to each set of blocks.

Usage Examples
--------------

Read multiple CSV files from a local directory, splitting on newlines:

```python
sample, blocks = read_bytes('2015-*-*.csv', delimiter=b'\n')
```

Read files from an S3 bucket:

```python
sample, blocks = read_bytes('s3://bucket/2015-*-*.csv', delimiter=b'\n')
```

Include file paths in the output:

```python
sample, paths, blocks = read_bytes('2015-*-*.csv', include_path=True)
```

Notes
-----

- Chunked reading is not supported for compressed files unless the compression format supports random access. For compressed files, set `blocksize=None` to read the entire file at once.

- The function uses `fsspec` to handle different filesystems and protocols, enabling seamless access to remote data.

- Returned blocks are Dask delayed objects, allowing integration with Dask's parallel and distributed computing capabilities.

- The `sample` output can be used to infer file structure or schema before processing the full dataset.

This functionality enables efficient, scalable, and flexible access to remote and local data sources, facilitating large-scale data processing workflows.