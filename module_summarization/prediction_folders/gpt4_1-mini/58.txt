Optimization Utilities Module
=============================

This module provides general-purpose functions and classes for model fitting and optimization. The utilities are designed to be independent of any specific model class, allowing reuse across different modeling contexts.

Overview
--------

The core functionality centers around the `Optimizer` class, which implements a flexible fitting method capable of handling a variety of optimization algorithms. This design enables users to maximize or minimize objective functions, such as log-likelihoods, using different numerical solvers provided by SciPy.

Key Features
------------

- **Flexible Optimization Methods**: Supports multiple optimization algorithms including Newton-Raphson, Nelder-Mead, BFGS, Powell, Conjugate Gradient, Newton-Conjugate Gradient, Basin-Hopping, and a generic minimize wrapper.
- **Customizable Solver Parameters**: Allows fine-tuning of solver-specific options such as tolerance levels, maximum iterations, step sizes, and convergence criteria.
- **Callback Support**: Enables users to specify callback functions that are called after each iteration, facilitating monitoring or custom processing during optimization.
- **Comprehensive Output Control**: Offers options to retrieve detailed solver output and intermediate results for in-depth analysis of the optimization process.
- **Error Handling**: Includes utility functions to validate method names and ensure correct usage.

`Optimizer` Class
-----------------

The `Optimizer` class provides the `_fit` method, which performs the optimization given an objective function, its gradient, and optional Hessian. The method signature is as follows:

```python
_fit(objective, gradient, start_params, fargs, kwargs,
     hessian=None, method='newton', maxiter=100, full_output=True,
     disp=True, callback=None, retall=False)
```

Parameters:

- `objective`: The objective function to be minimized or maximized.
- `gradient`: The gradient (first derivative) of the objective function.
- `start_params`: Initial parameter estimates for the optimization.
- `fargs`: Additional arguments passed to the objective function.
- `kwargs`: Additional keyword arguments for the optimizer.
- `hessian`: Optional Hessian (second derivative) function.
- `method`: Optimization algorithm to use (e.g., 'newton', 'bfgs', 'nm', etc.).
- `maxiter`: Maximum number of iterations allowed.
- `full_output`: Whether to return detailed solver output.
- `disp`: Whether to display convergence messages.
- `callback`: Function called after each iteration with current parameters.
- `retall`: Whether to return all intermediate solutions.

Returns:

- `xopt`: The optimized parameter values.
- `retvals`: Dictionary of solver-specific return values (if `full_output` is True).
- `optim_settings`: Dictionary of settings used for the optimization.

Supported Optimization Methods and Options
------------------------------------------

- **newton**: Newton-Raphson method with tolerance control.
- **nm** (Nelder-Mead): Simplex method with options for function and parameter tolerances, and maximum function evaluations.
- **bfgs**: Quasi-Newton method with gradient tolerance, norm order, and step size options.
- **lbfgs**: Limited-memory BFGS with memory size, projected gradient tolerance, function tolerance, and gradient approximation options.
- **cg**: Conjugate gradient with gradient tolerance and step size.
- **ncg**: Newton-conjugate gradient with Hessian-vector product function and convergence tolerances.
- **powell**: Powellâ€™s method with line search tolerances and initial direction settings.
- **basinhopping**: Global optimization using basin-hopping with parameters controlling iterations, temperature, step size, and minimizer options.
- **minimize**: Generic wrapper for `scipy.optimize.minimize` allowing specification of any supported method.

Notes
-----

- The basin-hopping solver ignores some explicit arguments such as `maxiter`, `retall`, and `full_output`.
- Solver-specific options can be passed through the `kwargs` argument and are accessible in the results for further inspection.
- The module is designed to be extensible and adaptable to various optimization scenarios beyond likelihood maximization.

Dependencies
------------

- NumPy: For numerical operations and array handling.
- SciPy Optimize: For access to a wide range of optimization algorithms.

This module serves as a foundational component for statistical modeling and other applications requiring robust and flexible optimization capabilities.