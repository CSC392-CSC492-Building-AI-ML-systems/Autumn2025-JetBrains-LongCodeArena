Naive Bayes Methods
===================

Overview
--------

The `sklearn.naive_bayes` module implements Naive Bayes algorithms, which are supervised learning methods based on applying Bayes' theorem with strong (naive) feature independence assumptions. These algorithms are particularly suited for classification tasks and are known for their simplicity, efficiency, and effectiveness in many practical applications.

The module provides several variants of the Naive Bayes classifier, each tailored to different types of data and assumptions about feature distributions:

- **GaussianNB**: Assumes that the features follow a Gaussian (normal) distribution.
- **MultinomialNB**: Suitable for discrete count data, such as word counts in text classification.
- **BernoulliNB**: Designed for binary/boolean features.
- **ComplementNB**: An adaptation of MultinomialNB that is particularly suited for imbalanced data.
- **CategoricalNB**: Handles categorical features without requiring one-hot encoding.

Base Class: _BaseNB
-------------------

All Naive Bayes classifiers in this module inherit from the abstract base class `_BaseNB`, which provides common functionality and enforces the implementation of key methods.

Key methods provided by `_BaseNB` include:

- `_joint_log_likelihood(X)`: Computes the unnormalized posterior log probability of the input samples `X`. This corresponds to the joint log probability \(\log P(c) + \log P(x|c)\) for each class \(c\).
- `predict(X)`: Predicts the class labels for the input samples.
- `predict_proba(X)`: Returns the probability estimates for each class.
- `predict_log_proba(X)`: Returns the log-probability estimates for each class.
- `predict_joint_log_proba(X)`: Returns the joint log probability estimates for each class.

These methods ensure consistent interfaces across all Naive Bayes classifiers.

Gaussian Naive Bayes (GaussianNB)
---------------------------------

The `GaussianNB` classifier assumes that the features are normally distributed within each class. It models the likelihood of the features using a Gaussian distribution characterized by a mean and variance for each feature per class.

### Features

- Supports online updates to model parameters via the `partial_fit` method, allowing incremental learning.
- Uses a variance smoothing parameter to improve numerical stability by adding a small portion of the largest variance to all variances.
- Efficiently computes class prior probabilities and feature statistics during training.

### Parameters

- `priors` (array-like of shape `(n_classes,)`, default=`None`): Prior probabilities of the classes. If specified, these priors are not adjusted based on the training data.
- `var_smoothing` (float, default=`1e-9`): Portion of the largest variance of all features added to variances for calculation stability.

### Attributes

- `class_count_` (ndarray of shape `(n_classes,)`): Number of training samples observed in each class.
- `class_prior_` (ndarray of shape `(n_classes,)`): Probability of each class.
- `classes_` (ndarray of shape `(n_classes,)`): Class labels known to the classifier.
- `epsilon_` (float): Absolute additive value to variances for numerical stability.
- `n_features_in_` (int): Number of features seen during fitting.
- `feature_names_in_` (ndarray of shape `(n_features_in_,)`, optional): Names of features seen during fitting, if available.
- `var_` (ndarray of shape `(n_classes, n_features)`): Variance of each feature per class.
- `theta_` (ndarray of shape `(n_classes, n_features)`): Mean of each feature per class.

### References

For details on the algorithm used to update feature means and variances online, see the Stanford CS technical report:

Chan, T. F., Golub, G. H., & LeVeque, R. J. (1979). Algorithms for computing the sample variance: analysis and recommendations. *Stanford University Technical Report STAN-CS-79-773*.  
http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf

Additional Naive Bayes Classifiers
----------------------------------

- **MultinomialNB**: Suitable for classification with discrete features (e.g., word counts). It models the feature counts using a multinomial distribution.
- **BernoulliNB**: Designed for binary/boolean features, modeling the presence or absence of features.
- **ComplementNB**: A variant of MultinomialNB that is designed to correct the "severe assumptions" made by the standard multinomial model, especially useful for imbalanced datasets.
- **CategoricalNB**: Handles categorical features directly without requiring one-hot encoding, modeling each feature as a categorical distribution.

Usage
-----

Naive Bayes classifiers can be used for classification tasks by fitting the model to training data and then predicting labels for new samples. They are particularly effective for high-dimensional data and text classification problems.

Example:

```python
from sklearn.naive_bayes import GaussianNB

# Create a Gaussian Naive Bayes classifier
gnb = GaussianNB()

# Fit the model
gnb.fit(X_train, y_train)

# Predict class labels
y_pred = gnb.predict(X_test)

# Predict class probabilities
y_proba = gnb.predict_proba(X_test)
```

See Also
--------

- :class:`sklearn.naive_bayes.BernoulliNB`
- :class:`sklearn.naive_bayes.MultinomialNB`
- :class:`sklearn.naive_bayes.ComplementNB`
- :class:`sklearn.naive_bayes.CategoricalNB`

For more detailed information and examples, refer to the :ref:`User Guide <naive_bayes>`.