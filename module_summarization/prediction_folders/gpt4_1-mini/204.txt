Random Projection
=================

Random Projections provide a simple and computationally efficient method to reduce the dimensionality of data. This technique achieves dimensionality reduction by trading a controlled amount of accuracy—manifested as additional variance—for faster processing times and smaller model sizes.

Overview
--------

The core idea behind Random Projections is to project high-dimensional data into a lower-dimensional space using a random matrix, while approximately preserving the pairwise distances between data points. This is achieved by carefully controlling the dimensions and distribution of the random projection matrices.

The theoretical foundation of Random Projections is the Johnson-Lindenstrauss lemma, which states that a small set of points in a high-dimensional Euclidean space can be embedded into a space of much lower dimension such that the distances between the points are nearly preserved. The embedding map can be taken as a random linear projection, often constructed from Gaussian or sparse random matrices.

Johnson-Lindenstrauss Lemma
----------------------------

The Johnson-Lindenstrauss lemma guarantees that for any 0 < ε < 1, and any set of n points in high-dimensional space, there exists a mapping \( p \) into a lower-dimensional space of dimension \( k \) such that for any two points \( u \) and \( v \):

\[
(1 - \varepsilon) \|u - v\|^2 \leq \|p(u) - p(v)\|^2 \leq (1 + \varepsilon) \|u - v\|^2
\]

The minimal number of components \( k \) required to achieve this with high probability is bounded by:

\[
k \geq \frac{4 \log(n)}{\varepsilon^2 / 2 - \varepsilon^3 / 3}
\]

where \( n \) is the number of samples and \( \varepsilon \) is the maximum distortion allowed.

This result implies that the required embedding dimension depends logarithmically on the number of samples and is independent of the original feature dimension.

Applications
------------

Random Projection is particularly useful when dealing with very high-dimensional data where traditional dimensionality reduction techniques (such as PCA) are computationally expensive. It is widely used in:

- Preprocessing for machine learning algorithms to reduce computational cost.
- Data compression and storage.
- Speeding up nearest neighbor searches.
- Privacy-preserving data transformations.

Types of Random Projection Matrices
-----------------------------------

Two common types of random projection matrices are:

- **Gaussian Random Projection**: The projection matrix is dense, with entries drawn independently from a Gaussian distribution \( \mathcal{N}(0, 1/k) \), where \( k \) is the target dimension.

- **Sparse Random Projection**: The projection matrix is sparse, with most entries being zero, which reduces computation and memory usage. The sparsity level is often chosen based on the number of features.

Advantages
----------

- **Computational Efficiency**: Random projections are faster to compute than many other dimensionality reduction methods.
- **Simplicity**: The method is straightforward to implement and does not require eigenvalue decomposition or iterative optimization.
- **Theoretical Guarantees**: The Johnson-Lindenstrauss lemma provides strong theoretical guarantees on distance preservation.
- **Scalability**: Suitable for very large datasets and high-dimensional data.

Limitations
-----------

- **Approximate Preservation**: Distances are preserved only approximately, with some distortion controlled by \( \varepsilon \).
- **Randomness**: Results can vary depending on the random seed used to generate the projection matrix.
- **Not Data-Adaptive**: Unlike PCA, random projections do not adapt to the data distribution.

References
----------

- Johnson, W. B., & Lindenstrauss, J. (1984). Extensions of Lipschitz mappings into a Hilbert space. *Contemporary Mathematics*, 26, 189–206.
- Dasgupta, S., & Gupta, A. (1999). An elementary proof of the Johnson-Lindenstrauss lemma. *International Computer Science Institute*.
- Wikipedia contributors. Johnson–Lindenstrauss lemma. *Wikipedia, The Free Encyclopedia*. https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma

Examples
--------

Using the Johnson-Lindenstrauss lemma, one can compute the minimal number of components required for a given dataset size and distortion level:

```python
from sklearn.random_projection import johnson_lindenstrauss_min_dim

# Compute minimal dimension for 1,000,000 samples with 50% distortion
min_dim = johnson_lindenstrauss_min_dim(1_000_000, eps=0.5)
print(min_dim)  # Output: 663

# Compute minimal dimensions for multiple distortion levels
min_dims = johnson_lindenstrauss_min_dim(1_000_000, eps=[0.5, 0.1, 0.01])
print(min_dims)  # Output: [   663  11841 1112658]
```

See Also
--------

- :ref:`User Guide <random_projection>`
- :class:`sklearn.random_projection.GaussianRandomProjection`
- :class:`sklearn.random_projection.SparseRandomProjection`