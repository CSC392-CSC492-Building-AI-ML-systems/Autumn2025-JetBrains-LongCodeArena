Naive Bayes (:mod:`sklearn.naive_bayes`)
=========================================

Overview
--------

The `sklearn.naive_bayes` module implements Naive Bayes algorithms, which are supervised learning methods based on applying Bayes' theorem with strong (naive) feature independence assumptions. These algorithms are widely used for classification tasks due to their simplicity, efficiency, and effectiveness, especially in high-dimensional datasets.

The module provides several variants of the Naive Bayes classifier, each suitable for different types of data and assumptions:

- `GaussianNB`
- `BernoulliNB`
- `MultinomialNB`
- `ComplementNB`
- `CategoricalNB`

Base Class: `_BaseNB`
---------------------

`_BaseNB` is an abstract base class for all Naive Bayes estimators in this module. It inherits from `BaseEstimator` and `ClassifierMixin` and uses the `ABCMeta` metaclass to enforce implementation of abstract methods in subclasses.

Key methods include:

- `_joint_log_likelihood(X)`:  
  Abstract method to compute the unnormalized posterior log probability of input samples `X`. It returns the joint log likelihood, i.e., `log P(c) + log P(x|c)` for each class `c` and sample `x`.

- `_check_X(X)`:  
  Abstract method to perform input validation and preprocessing on `X`. This is called internally before prediction methods.

- `predict_joint_log_proba(X)`:  
  Returns the joint log probability estimates for the input samples.

- `predict(X)`:  
  Predicts the class labels for the input samples.

- `predict_log_proba(X)`:  
  Returns the log-probability estimates for the input samples.

- `predict_proba(X)`:  
  Returns the probability estimates for the input samples.

All prediction methods ensure the model is fitted before making predictions and validate the input data accordingly.

Gaussian Naive Bayes (`GaussianNB`)
----------------------------------

`GaussianNB` implements the Gaussian Naive Bayes algorithm, which assumes that the likelihood of the features is Gaussian (normal distribution). It is suitable for continuous data.

Features:

- Supports online updates to model parameters via the `partial_fit` method.
- Uses an algorithm for online updates of feature means and variances based on the Stanford CS technical report STAN-CS-79-773 by Chan, Golub, and LeVeque.
- Includes a `var_smoothing` parameter to add a portion of the largest variance to variances for numerical stability.

Parameters
~~~~~~~~~~

- `priors` : array-like of shape (n_classes,), default=None  
  Prior probabilities of the classes. If specified, these priors are not adjusted according to the data.

- `var_smoothing` : float, default=1e-9  
  Portion of the largest variance of all features added to variances for calculation stability.

Attributes
~~~~~~~~~~

- `class_count_` : ndarray of shape (n_classes,)  
  Number of training samples observed in each class.

- `class_prior_` : ndarray of shape (n_classes,)  
  Probability of each class.

- `classes_` : ndarray of shape (n_classes,)  
  Class labels known to the classifier.

- `epsilon_` : float  
  Absolute additive value to variances.

- `n_features_in_` : int  
  Number of features seen during fitting.

- `feature_names_in_` : ndarray of shape (n_features_in_,)  
  Names of features seen during fitting, if available.

- `var_` : ndarray of shape (n_classes, n_features)  
  Variance of each feature per class.

- `theta_` : ndarray of shape (n_classes, n_features)  
  Mean of each feature per class.

See Also
--------

- `BernoulliNB` : Naive Bayes classifier for multivariate Bernoulli models.
- `MultinomialNB` : Naive Bayes classifier for multinomial models.
- `ComplementNB` : Complement Naive Bayes classifier, an adaptation of MultinomialNB.
- `CategoricalNB` : Naive Bayes classifier for categorical features.

References
----------

- Bayes, T. (1763). An Essay towards solving a Problem in the Doctrine of Chances.
- Chan, T. F., Golub, G. H., & LeVeque, R. J. (1979). Algorithms for computing the sample variance: analysis and recommendations. Stanford CS Technical Report STAN-CS-79-773.  
  http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf

Usage Example
-------------

```python
from sklearn.naive_bayes import GaussianNB

# Create a Gaussian Naive Bayes classifier
gnb = GaussianNB()

# Fit the model
gnb.fit(X_train, y_train)

# Predict class labels
y_pred = gnb.predict(X_test)

# Predict class probabilities
probs = gnb.predict_proba(X_test)
```

This module is part of the scikit-learn library and is distributed under the BSD 3-Clause License.