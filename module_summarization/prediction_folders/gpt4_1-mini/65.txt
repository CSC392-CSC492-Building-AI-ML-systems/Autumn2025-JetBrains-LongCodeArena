Calibration of Predicted Probabilities
=======================================

Overview
--------

Probability calibration is the process of adjusting the predicted probabilities of a classifier so that they better reflect the true likelihood of an event. Many classifiers produce scores or probabilities that are not well calibrated, meaning that the predicted probability does not correspond well to the true frequency of the event. Calibration methods aim to transform these outputs into well-calibrated probabilities.

The `CalibratedClassifierCV` class provides a convenient way to calibrate the predicted probabilities of any classifier using cross-validation. It supports two main calibration methods:

- **Sigmoid calibration (Platt scaling):** Fits a logistic regression model to the classifier’s scores.
- **Isotonic regression:** A non-parametric calibration method that fits a piecewise constant non-decreasing function.

This tutorial explains how to use `CalibratedClassifierCV` to improve the probability estimates of your classifiers.

Why Calibrate Probabilities?
----------------------------

Some classifiers, such as Support Vector Machines (SVMs) or decision trees, do not naturally output well-calibrated probabilities. For example, an SVM’s decision function scores are not probabilities, and their raw outputs can be poorly calibrated. Calibration helps to transform these scores into probabilities that can be interpreted as confidence levels, which is important for decision making, risk assessment, and combining predictions from multiple models.

Calibration Methods
------------------

### Sigmoid Calibration (Platt Scaling)

This method fits a logistic regression model to the classifier’s decision function or predicted probabilities. It is a parametric approach and works well when the relationship between the classifier’s scores and the true probabilities is approximately logistic.

### Isotonic Regression

Isotonic regression is a non-parametric method that fits a non-decreasing function to the data. It is more flexible than sigmoid calibration but requires more data to avoid overfitting. It is not recommended to use isotonic calibration with fewer than 1000 calibration samples.

Using `CalibratedClassifierCV`
------------------------------

The `CalibratedClassifierCV` class wraps around any classifier and calibrates its predicted probabilities using cross-validation. It can be used as follows:

```python
from sklearn.calibration import CalibratedClassifierCV
from sklearn.svm import LinearSVC
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Generate a binary classification dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Base classifier without probability calibration
base_clf = LinearSVC()

# Calibrated classifier with sigmoid method (Platt scaling)
calibrated_clf = CalibratedClassifierCV(base_clf, method='sigmoid', cv=5)

# Fit the calibrated classifier
calibrated_clf.fit(X_train, y_train)

# Predict calibrated probabilities
probs = calibrated_clf.predict_proba(X_test)
```

Parameters
----------

- **estimator**: The base classifier to be calibrated. If `None`, a `LinearSVC` is used by default.
- **method**: Calibration method, either `'sigmoid'` (default) or `'isotonic'`.
- **cv**: Cross-validation splitting strategy. Can be an integer (number of folds), a cross-validation generator, an iterable yielding train/test splits, or `"prefit"` if the base estimator is already fitted.
- **n_jobs**: Number of jobs to run in parallel during cross-validation.
- **ensemble**: If `True` (default), fits a calibrated classifier for each fold and averages their predictions. If `False`, uses cross-validated predictions for calibration and fits the base estimator on all data for prediction.

Advanced Usage
--------------

### Calibrating a Pre-fitted Classifier

If you already have a fitted classifier and want to calibrate it on separate data, use `cv="prefit"`:

```python
base_clf.fit(X_train, y_train)
calibrated_clf = CalibratedClassifierCV(base_clf, cv="prefit", method='sigmoid')
calibrated_clf.fit(X_calibration, y_calibration)
```

Make sure that the calibration data is disjoint from the training data to avoid biased calibration.

### Choosing Between Sigmoid and Isotonic

- Use `'sigmoid'` for smaller datasets or when you expect a logistic relationship.
- Use `'isotonic'` for larger datasets and when you want a more flexible calibration.

Limitations and Considerations
------------------------------

- Isotonic regression can overfit if the calibration dataset is too small.
- Calibration requires a separate calibration set or cross-validation to avoid biased estimates.
- The base estimator must provide either a `decision_function` or `predict_proba` method.

Further Reading
---------------

For more details on probability calibration and examples, see the :ref:`User Guide <calibration>`.

References
----------

- Platt, J. (1999). Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers.
- Zadrozny, B., & Elkan, C. (2002). Transforming classifier scores into accurate multiclass probability estimates. Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining.

---

This tutorial provides a practical introduction to probability calibration using `CalibratedClassifierCV` in scikit-learn, enabling you to improve the reliability of your classifier’s predicted probabilities.