w_crawl Command
===============

Overview
--------

The `w_crawl` command is a tool for crawling through a weighted ensemble dataset, executing a specified function for each iteration. It is designed to facilitate postprocessing of trajectories, cleanup of datasets, or any other operation that can be expressed as "perform task X for iteration N, then process the result." Tasks are parallelized by iteration, allowing efficient execution across multiple iterations. Note that no guarantees are made about the order in which iterations are evaluated.

Functionality
-------------

- Executes a user-specified callable on each iteration of the dataset.
- Supports initialization and finalization steps via a crawler instance.
- Processes results of each iteration’s task through an optional crawler instance.
- Provides progress indication during execution.
- Parallelizes tasks by iteration for improved performance.

Usage
-----

```bash
w_crawl [OPTIONS] TASK_CALLABLE
```

Where `TASK_CALLABLE` is specified as `module.function` and represents the function to be run on each iteration.

Command-Line Options
--------------------

The command-line options are grouped as follows:

### Data Reader Options

Options to specify and configure the dataset to be crawled. These are inherited from the WESTDataReader component.

### Iteration Range Selection

Options to select the range of iterations to process. These are inherited from the IterRangeSelection component.

### Task Options

- `-c, --crawler-instance CRAWLER_INSTANCE`  
  Specify a crawler instance as `module.instance` that inherits from `WESTPACrawler`. This instance coordinates the crawling process, including initialization, finalization, and processing of per-iteration results. This option is required only if such coordination is needed.

- `TASK_CALLABLE`  
  The callable to run on each iteration, specified as `module.function`. This is a required positional argument.

### Progress Indicator Options

Options to control the display of progress information during execution.

Crawler Interface
-----------------

The crawling process can be customized by providing a crawler instance that inherits from the `WESTPACrawler` base class. This class defines three key methods:

- `initialize(iter_start, iter_stop)`  
  Called once before processing begins. Use this to set up any necessary state.

- `process_iter_result(n_iter, result)`  
  Called after each iteration’s task completes, with the iteration number and the result returned by the task callable.

- `finalize()`  
  Called once after all iterations have been processed. Use this to perform any cleanup or final aggregation.

If no crawler instance is provided, a default `WESTPACrawler` instance is used, which performs no special processing.

Execution Details
-----------------

- The tool opens the dataset in read mode.
- Iterations are processed in the specified range `[iter_start, iter_stop)`.
- For each iteration, the specified task callable is executed with the iteration number and the iteration group data.
- Tasks are dispatched in parallel and results are processed as they complete.
- Progress is displayed throughout the operation.

Example
-------

```bash
w_crawl -c mymodule.MyCrawler mymodule.my_task_function
```

This command runs `my_task_function` on each iteration, coordinating the process with the `MyCrawler` instance.

Notes
-----

- The task callable must accept two arguments: the iteration number and the iteration group data.
- The crawler instance must be importable and follow the interface described above.
- The order of iteration processing is not guaranteed due to parallel execution.

For more detailed information on the options related to data reading, iteration selection, and progress indication, refer to the respective components’ documentation.