Cross Decomposition
===================

The `cross_decomposition` module in scikit-learn provides tools for performing cross decomposition methods, which are statistical techniques used to analyze the relationships between two sets of variables. These methods are particularly useful when the predictor variables are highly collinear or when the number of predictors exceeds the number of observations.

Overview
--------

Cross decomposition methods aim to find latent variables that capture the covariance structure between two datasets, typically denoted as \(X\) and \(Y\). The main goal is to model the relationship between these two datasets by extracting components that explain their shared variance.

Key Methods
-----------

The module implements several variants of Partial Least Squares (PLS) and related algorithms:

- **PLSRegression**: A regression method that finds a linear model by projecting the predictors and the response variables to a new space. It is particularly effective when predictors are many and highly collinear.

- **PLSCanonical**: A canonical correlation analysis method that finds linear combinations of the variables in \(X\) and \(Y\) that are maximally correlated.

- **PLSSVD**: A method based on singular value decomposition (SVD) of the cross-covariance matrix between \(X\) and \(Y\).

Algorithmic Details
-------------------

The core of these methods involves decomposing the covariance matrix \(X^T Y\) to extract singular vectors and singular values that represent the directions of maximum covariance between \(X\) and \(Y\).

Two main approaches are used to compute the first singular vectors:

- **Power Method**: An iterative algorithm that approximates the first singular vectors by repeatedly projecting and normalizing vectors. This method can be more efficient for large datasets.

- **Singular Value Decomposition (SVD)**: A direct computation of the full SVD of the covariance matrix \(X^T Y\), from which the first singular vectors are extracted.

Data Preprocessing
------------------

Before applying cross decomposition methods, the data matrices \(X\) and \(Y\) are typically centered (mean subtracted) and optionally scaled (standardized). This preprocessing ensures that the components extracted reflect meaningful relationships rather than artifacts of scale or location.

Usage Notes
-----------

- The methods handle multi-output regression and can be used for dimensionality reduction, feature extraction, and regression tasks.

- Convergence warnings may be issued if iterative algorithms reach the maximum number of iterations without converging.

- The module includes utilities to handle numerical stability, such as pseudo-inverse computations and sign flipping of singular vectors to ensure consistent output.

References
----------

- Wold, H. (1985). Partial least squares. In *Encyclopedia of Statistical Sciences* (pp. 581â€“591). Wiley.

- Wegelin, J. A. (2000). A survey of partial least squares (PLS) methods, with emphasis on the two-block case. *Technical Report*, University of Washington.

See Also
--------

- `sklearn.cross_decomposition.PLSRegression`

- `sklearn.cross_decomposition.PLSCanonical`

- `sklearn.cross_decomposition.PLSSVD`

This module is part of the scikit-learn library and is designed to provide robust and efficient implementations of cross decomposition techniques for statistical learning and data analysis.