Cross-validation: Evaluating Estimator Performance
====================================================

Cross-validation is a robust statistical method used to estimate the skill of machine learning models. It is primarily used to assess how the results of a predictive model will generalize to an independent dataset. This technique is essential for evaluating estimator performance, tuning model parameters, and selecting the best model.

Overview
--------

Cross-validation involves partitioning the original dataset into a training set to train the model, and a test set to evaluate it. The most common form is *k*-fold cross-validation, where the data is split into *k* subsets (folds). The model is trained on *k-1* folds and tested on the remaining fold. This process is repeated *k* times, with each fold used exactly once as the test set. The results are then averaged to produce a single estimation.

Key Benefits
------------

- **Reliable Performance Estimation:** Provides a more accurate measure of model performance compared to a single train/test split.
- **Model Selection:** Helps in comparing different models or tuning hyperparameters.
- **Bias-Variance Trade-off Insight:** Offers insight into the variability of the model’s performance.

Common Cross-validation Tools
-----------------------------

### Learning Curve

The learning curve is a diagnostic tool that shows how the training and validation scores evolve with varying sizes of the training dataset. It helps in understanding whether a model would benefit from more training data or if it is suffering from high bias or variance.

The `LearningCurveDisplay` class provides a convenient visualization of learning curves. It plots the training and test scores against the number of training examples, including the variability (standard deviation) across cross-validation folds.

**Parameters:**

- `train_sizes`: Array of training set sizes used to generate the curve.
- `train_scores`: Scores on the training sets for each training size.
- `test_scores`: Scores on the validation sets for each training size.
- `score_name`: Name of the scoring metric used.

**Features:**

- Supports plotting mean scores with error bars or shaded regions representing standard deviation.
- Automatically adjusts the x-axis scale based on the distribution of training sizes.
- Provides attributes to access the underlying matplotlib objects for further customization.

Example usage:

```python
from sklearn.model_selection import learning_curve, LearningCurveDisplay
import matplotlib.pyplot as plt

train_sizes, train_scores, test_scores = learning_curve(
    estimator, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 5)
)

display = LearningCurveDisplay(
    train_sizes=train_sizes,
    train_scores=train_scores,
    test_scores=test_scores,
    score_name="Accuracy"
)
display.plot()
plt.show()
```

### Validation Curve

Validation curves are used to evaluate how the performance of a model changes with respect to a hyperparameter. By plotting the training and validation scores against different values of a parameter, one can identify the optimal parameter value that balances bias and variance.

Additional Utilities
--------------------

- **Score Validation:** Functions to validate and interpret scoring metrics.
- **Plotting Support:** Utilities to support visualization of cross-validation results, including handling of standard deviation display styles such as error bars or shaded regions.

Best Practices
--------------

- Use cross-validation to avoid overfitting and to get a realistic estimate of model performance.
- Combine learning curves and validation curves to diagnose model behavior and guide improvements.
- Choose appropriate scoring metrics aligned with the problem domain.
- Visualize results to better understand model performance and variability.

Further Reading
---------------

- :ref:`User Guide <visualizations>` for detailed information on visualization tools.
- :ref:`learning_curve` for comprehensive documentation on learning curve computation.
- Scikit-learn’s `model_selection` module for a variety of cross-validation strategies and utilities.

By leveraging cross-validation and its associated visualization tools, practitioners can make informed decisions about model selection, tuning, and deployment, ensuring robust and reliable predictive performance.