Working with Text Data
======================

This tutorial provides an overview of how to work with text data using the utilities provided in the `sklearn.feature_extraction.text` submodule. This submodule offers tools to convert text documents into numerical feature vectors, which can then be used for machine learning tasks.

Overview
--------

Text data is inherently unstructured and must be transformed into a structured numerical format before it can be used by machine learning algorithms. The `sklearn.feature_extraction.text` submodule provides several classes and functions to facilitate this transformation, including:

- **Vectorizers**: Convert a collection of text documents to a matrix of token counts or TF-IDF features.
- **Preprocessing utilities**: Functions to clean and normalize text data.
- **Stop words**: Commonly used words that can be filtered out to reduce noise.

Key Components
--------------

### Text Preprocessing

Text preprocessing involves cleaning and normalizing raw text data. Common preprocessing steps include:

- **Lowercasing**: Converting all characters to lowercase to ensure uniformity.
- **Accent stripping**: Removing accentuated characters to their ASCII or Unicode base forms.
- **HTML tag stripping**: Removing HTML or XML tags from text.

The submodule provides utility functions for these tasks:

- `strip_accents_unicode(s)`: Removes accents from unicode characters by decomposing them and removing combining characters.
- `strip_accents_ascii(s)`: Removes accents by transliterating unicode characters to ASCII equivalents.
- `strip_tags(s)`: Removes HTML/XML tags using regular expressions.

Example:

```python
from sklearn.feature_extraction.text import strip_accents_ascii, strip_tags

text = "<p>Héllo Wörld!</p>"
clean_text = strip_tags(text)
clean_text = strip_accents_ascii(clean_text.lower())
print(clean_text)  # Output: "hello world!"
```

### Tokenization and Analysis

Tokenization is the process of splitting text into meaningful units called tokens (e.g., words). The submodule supports flexible tokenization and analysis pipelines through the `_analyze` function, which chains together preprocessing, tokenization, n-gram generation, and stop word filtering.

Parameters involved in analysis include:

- `preprocessor`: Function to preprocess raw text.
- `tokenizer`: Function to split text into tokens.
- `ngrams`: Function to generate n-grams from tokens.
- `stop_words`: List or set of words to filter out.

Example of a simple analysis pipeline:

```python
from sklearn.feature_extraction.text import _analyze

def simple_tokenizer(doc):
    return doc.split()

tokens = _analyze(
    doc="This is a sample document.",
    preprocessor=str.lower,
    tokenizer=simple_tokenizer,
    stop_words={'is', 'a'}
)
print(list(tokens))  # Output: ['this', 'sample', 'document.']
```

### Stop Words

Stop words are common words that often do not contribute meaningful information to text analysis. The submodule includes a built-in set of English stop words accessible via `ENGLISH_STOP_WORDS`. You can also provide your own list or use the string `"english"` to use the built-in list.

Example:

```python
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

print("the" in ENGLISH_STOP_WORDS)  # Output: True
```

### Vectorizers

The submodule provides several vectorizer classes to convert text documents into numerical feature vectors:

- **CountVectorizer**: Converts a collection of text documents to a matrix of token counts.
- **TfidfVectorizer**: Converts text documents to a matrix of TF-IDF features.
- **HashingVectorizer**: Uses a hashing trick to convert text to feature vectors without storing a vocabulary.

These vectorizers support various parameters to customize tokenization, preprocessing, stop word removal, and n-gram generation.

Example usage of `CountVectorizer`:

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?",
]

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names_out())
# Output: ['document' 'first' 'second' 'third']

print(X.toarray())
# Output: array representation of token counts
```

Advanced Features
-----------------

- **Custom Preprocessing and Tokenization**: You can provide your own functions for preprocessing and tokenization to tailor the vectorization process to your needs.
- **N-gram Generation**: Generate contiguous sequences of tokens (n-grams) to capture context beyond single words.
- **Handling Accents and Encoding**: The vectorizers can handle different encodings and provide options to strip accents from characters.

Summary
-------

The `sklearn.feature_extraction.text` submodule offers a comprehensive set of tools to preprocess, tokenize, and vectorize text data for machine learning. By leveraging these utilities, you can efficiently convert raw text into meaningful numerical features suitable for various natural language processing tasks.

For more detailed examples and advanced usage, refer to the scikit-learn documentation and tutorials on text feature extraction.