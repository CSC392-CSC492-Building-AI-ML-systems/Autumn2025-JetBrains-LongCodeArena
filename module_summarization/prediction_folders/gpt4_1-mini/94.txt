# Stateless Learning Rate Schedulers

This module provides a collection of **stateless learning rate schedulers** designed to address limitations found in PyTorch's built-in learning rate schedulers (`torch.optim.lr_scheduler`). The primary goal of these schedulers is to integrate seamlessly with the Composer framework's time abstraction (`composer.core.time`), allowing learning rate schedules to be defined using explicit and flexible time units such as epochs, batches, or durations.

---

## Overview

Traditional PyTorch schedulers maintain internal state and rely on calls to `.step()` to update the learning rate. In contrast, stateless schedulers in this module are pure functions or callable objects that compute a learning rate multiplier based solely on the current training state and time, without maintaining internal state. This design enables:

- **Explicit time-based scheduling:** Schedulers can be configured using arbitrary time units (e.g., epochs, batches, durations).
- **Composable schedules:** Multiple schedulers can be combined multiplicatively.
- **Better integration with Composer:** Schedulers interface directly with Composer's `State` and `Time` abstractions.

---

## Key Concepts

### ComposerScheduler Protocol

The core interface for all schedulers in this module is the `ComposerScheduler` protocol. It defines a callable interface:

```python
def __call__(self, state: State, ssr: float = 1.0) -> float:
    ...
```

- **Parameters:**
  - `state` (`State`): The current training state, including the current timestamp, maximum duration, and other relevant metadata.
  - `ssr` (`float`): The *scale schedule ratio* (SSR), which allows the schedule to be stretched or compressed in time. The scheduler output at time `t` with SSR `σ` is defined as the output at time `t / σ` with SSR 1.0.

- **Returns:**
  - A floating-point multiplier `α` to scale the optimizer's base learning rate.

Schedulers implementing this interface return a multiplier `α(t)` such that the effective learning rate at time `t` is:

\[
\eta(t) = \eta_i \times \alpha(t)
\]

where \(\eta_i\) is the initial learning rate set in the optimizer.

### Statelessness

Schedulers are pure functions of the current state and do not maintain internal state. This means:

- The learning rate multiplier depends only on the current time and configuration.
- Multiple schedulers can be combined by multiplying their outputs.
- The scheduler can be re-evaluated at any time without side effects.

### Time Units and Conversion

Schedulers can be configured using various time units:

- **Epochs**
- **Batches**
- **Durations**

The module provides utilities to convert between these units based on the current training state, including the total number of batches per epoch (`dataloader_len`) and the maximum training duration (`max_duration`).

---

## Provided Schedulers

The module exports a variety of commonly used learning rate schedules, including but not limited to:

- `ConstantScheduler`: Keeps the learning rate multiplier constant.
- `StepScheduler`: Decays the learning rate by a factor at fixed intervals.
- `MultiStepScheduler`: Decays the learning rate at multiple specified milestones.
- `LinearScheduler`: Linearly increases or decreases the learning rate multiplier.
- `ExponentialScheduler`: Applies exponential decay to the learning rate.
- `CosineAnnealingScheduler`: Applies cosine annealing decay.
- `CosineAnnealingWarmRestartsScheduler`: Cosine annealing with warm restarts.
- `PolynomialScheduler`: Applies polynomial decay.
- Warmup variants of the above schedulers, such as `MultiStepWithWarmupScheduler`, `ConstantWithWarmupScheduler`, `LinearWithWarmupScheduler`, `CosineAnnealingWithWarmupScheduler`, and `PolynomialWithWarmupScheduler`.

---

## Usage Examples

### Simple Function Scheduler

A scheduler that halves the learning rate after 10 epochs:

```python
def ten_epoch_decay_scheduler(state: State) -> float:
    if state.timestamp.epoch < 10:
        return 1.0
    return 0.5
```

This function can be passed directly to the Composer `Trainer` as a scheduler.

### Callable Class Scheduler

A more configurable scheduler implemented as a callable class:

```python
class VariableEpochDecayScheduler(ComposerScheduler):
    def __init__(self, num_epochs: int):
        self.num_epochs = num_epochs

    def __call__(self, state: State) -> float:
        if state.timestamp.epoch < self.num_epochs:
            return 1.0
        return 0.5

scheduler = VariableEpochDecayScheduler(num_epochs=10)
```

---

## Integration with PyTorch Schedulers

The function `compile_composer_scheduler` converts a stateless `ComposerScheduler` into a PyTorch-compatible scheduler object (`PyTorchScheduler`). This allows the scheduler to be used with PyTorch optimizers and training loops that expect a `.step()` interface, while internally relying on the stateless scheduler logic and the current training state.

---

## Notes

- The learning rate multiplier returned by schedulers is always relative to the optimizer's initial learning rate.
- The scale schedule ratio (`ssr`) parameter enables flexible stretching or compression of the schedule timeline.
- Epoch-based times are internally converted to batch-based times for finer granularity and SSR compatibility.
- The module requires that `state.max_duration` and `state.dataloader_len` be set to perform time conversions correctly.

---

## Summary

This module provides a flexible, stateless, and composable approach to learning rate scheduling that integrates tightly with the Composer training framework. By abstracting over time units and removing internal state, these schedulers enable more predictable, configurable, and reusable learning rate schedules for training deep learning models.