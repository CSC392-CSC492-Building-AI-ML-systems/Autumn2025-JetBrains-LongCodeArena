optimization
===========

.. currentmodule:: optimization

The optimization module provides a set of general-purpose functions and classes for model fitting and numerical optimization. These utilities are designed to be model-agnostic and can be integrated with various likelihood models or objective functions.

Overview
--------

This module includes an `Optimizer` class that serves as a flexible interface to multiple optimization algorithms available in `scipy.optimize`. It supports several methods for minimizing or maximizing objective functions, with options for supplying gradients and Hessians, configuring convergence criteria, and handling solver-specific parameters.

Classes
-------

.. autoclass:: Optimizer
   :members:
   :inherited-members:

The `Optimizer` class provides the `_fit` method, which is a versatile fitting function capable of using various numerical optimization techniques. It is designed to work with user-provided objective functions, gradients, and optional Hessian information.

Supported Methods
-----------------

The following optimization methods are supported via the `method` parameter in `_fit`:

- **newton**: Newton-Raphson method
- **nm**: Nelder-Mead simplex algorithm
- **bfgs**: Broyden-Fletcher-Goldfarb-Shanno algorithm
- **lbfgs**: Limited-memory BFGS
- **cg**: Conjugate gradient method
- **ncg**: Newton-conjugate gradient method
- **powell**: Modified Powell's method
- **basinhopping**: Global optimization with basin-hopping
- **minimize**: Generic wrapper for `scipy.optimize.minimize`

Each method supports a range of optional parameters for fine-tuning the optimization process, such as tolerance settings, maximum iterations, and step sizes. These can be passed through the `kwargs` argument in `_fit`.

Usage Example
-------------

Below is a basic example demonstrating how to use the `Optimizer` class to minimize an objective function:

.. code-block:: python

   from optimization import Optimizer
   import numpy as np

   # Define an objective function and its gradient
   def objective(params, *args):
       return np.sum(params**2)

   def gradient(params, *args):
       return 2 * params

   # Initialize the optimizer and fit
   opt = Optimizer()
   start_params = np.array([1.5, -2.0])
   result, retvals, settings = opt._fit(
       objective, gradient, start_params, method='bfgs'
   )

Parameters and Settings
-----------------------

The `_fit` method accepts several common parameters across all methods, including `start_params`, `maxiter`, `full_output`, `disp`, `fargs`, `callback`, and `retall`. Additionally, each optimization method has specific optional arguments (e.g., `tol` for Newton-Raphson, `xtol` and `ftol` for Nelder-Mead). These should be provided as keyword arguments in `kwargs`.

For detailed descriptions of all parameters, refer to the docstring of the `_fit` method.

Notes
-----

- The 'basinhopping' solver ignores some common parameters like `maxiter`, `retall`, and `full_output`, using its own set of controls (e.g., `niter`, `T`, `stepsize`).
- When using methods that approximate gradients or Hessians numerically, the `epsilon` parameter can be used to set the step size.
- The `full_output` flag controls whether additional solver output is stored and returned.

See the `scipy.optimize` documentation for in-depth information on each solver's behavior and parameters.