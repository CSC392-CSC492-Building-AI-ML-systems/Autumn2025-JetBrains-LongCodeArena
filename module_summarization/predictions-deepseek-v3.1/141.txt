```rst
Connect to Remote Data
======================

Dask provides utilities for efficiently reading data from remote storage systems
and distributed file systems. The ``read_bytes`` function enables chunked reading
of binary data from various sources with support for different protocols and
compression formats.

Overview
--------

The ``read_bytes`` function creates delayed objects that read binary data from
local or remote files. It supports:

- Multiple file protocols (S3, HDFS, etc.)
- Chunked reading with configurable block sizes
- Delimiter-based block splitting
- Header sampling
- Compression handling

Basic Usage
-----------

Read a single file with newline delimiter:

.. code-block:: python

    sample, blocks = read_bytes('data.csv', delimiter=b'\\n')

Read from cloud storage:

.. code-block:: python

    sample, blocks = read_bytes('s3://bucket/data/*.csv', delimiter=b'\\n')

Read multiple files with paths included:

.. code-block:: python

    sample, paths, blocks = read_bytes('*.csv', include_path=True)

API Reference
-------------

.. function:: read_bytes(urlpath, delimiter=None, not_zero=False, blocksize="128 MiB", sample="10 kiB", compression=None, include_path=False, **kwargs)

    Read bytes from file(s) with configurable chunking and sampling.

    :param urlpath: Absolute or relative filepath(s). Can be a glob pattern or
                    list of paths. Prefix with protocol (e.g., ``s3://``) for
                    remote filesystems.
    :type urlpath: str, list, tuple, or os.PathLike
    :param delimiter: Byte delimiter for splitting blocks (e.g., ``b'\\n'``)
    :type delimiter: bytes, optional
    :param not_zero: Force seek of start-of-file delimiter, discarding header
    :type not_zero: bool, default False
    :param blocksize: Chunk size in bytes (integer or string like "128 MiB")
    :type blocksize: int or str, default "128 MiB"
    :param sample: Sample size for header reading (False, integer, or string)
    :type sample: bool, int, or str, default "10 kiB"
    :param compression: Compression type ('gzip', 'xz', etc.) or None
    :type compression: str, optional
    :param include_path: Whether to include file paths in output
    :type include_path: bool, default False
    :param \\*\\*kwargs: Additional storage-specific options (host, port, etc.)

    :returns: Sample bytes and list of delayed byte blocks. If include_path=True,
              also returns list of file paths.
    :rtype: tuple (bytes, list) or (bytes, list, list)

Supported Protocols
-------------------

The function supports any protocol available through ``fsspec``, including:

- Local files (``file://`` or no prefix)
- Amazon S3 (``s3://``)
- Hadoop HDFS (``hdfs://``)
- Google Cloud Storage (``gcs://``)
- HTTP/HTTPS (``http://``, ``https://``)
- And many others via fsspec plugins

Compression Support
-------------------

Supported compression formats:

- gzip (``.gz``)
- bzip2 (``.bz2``)
- xz/lzma (``.xz``)
- zip (``.zip``)

Note: Chunked reading (``blocksize != None``) is not supported for compressed files.

Examples
--------

Simple file reading:

.. code-block:: python

    # Read entire file
    sample, blocks = read_bytes('data.bin', blocksize=None)

    # Read with 1MB chunks
    sample, blocks = read_bytes('largefile.bin', blocksize='1MB')

Delimited text files:

.. code-block:: python

    # Read CSV with newline delimiter
    sample, blocks = read_bytes('data.csv', delimiter=b'\\n', blocksize='64MB')

Remote storage with authentication:

.. code-block:: python

    # S3 with credentials
    sample, blocks = read_bytes(
        's3://bucket/data/*.csv',
        delimiter=b'\\n',
        key='AWS_ACCESS_KEY',
        secret='AWS_SECRET_KEY'
    )

Handling Empty Files
--------------------

Empty files are automatically skipped during processing, and no blocks are
generated for them.

Error Handling
--------------

The function raises:

- ``TypeError`` for invalid path types
- ``OSError`` when no files match the path pattern
- ``ValueError`` for unsupported operations (e.g., chunked reads on compressed files)

Implementation Notes
--------------------

- Uses ``fsspec`` for filesystem abstraction
- Delayed objects enable parallel reading across files and blocks
- Token-based caching avoids redundant reads of same data
- Efficient delimiter handling ensures blocks split on boundaries
```