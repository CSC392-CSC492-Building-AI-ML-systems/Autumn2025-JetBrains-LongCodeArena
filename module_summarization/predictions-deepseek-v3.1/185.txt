Tuning the hyper-parameters of an estimator
============================================

Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn, they are passed as arguments to the constructor of the estimator classes. It is possible and recommended to search the hyper-parameter space for the best cross-validation score.

Grid Search
-----------

The grid search provided by :class:`~sklearn.model_selection.GridSearchCV` exhaustively generates candidates from a grid of parameter values specified with the `param_grid` parameter. For instance, the following `param_grid`::

    param_grid = [
      {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
      {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
    ]

specifies that two grids should be explored: one with a linear kernel and C values in [1, 10, 100, 1000], and the second one with an RBF kernel, and the cross-product of C values in [1, 10, 100, 1000] and gamma values in [0.001, 0.0001].

The :class:`~sklearn.model_selection.GridSearchCV` instance implements the usual estimator API: when "fitting" it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.

Randomized Search
-----------------

The randomized search implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:

- A budget can be chosen independent of the number of parameters and possible values.
- Adding parameters that do not influence the performance does not decrease efficiency.

Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for :class:`~sklearn.model_selection.GridSearchCV`. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified::

    from scipy.stats import expon, randint
    param_dist = {
        'C': expon(scale=100), 
        'gamma': expon(scale=.1),
        'kernel': ['rbf', 'linear']
    }

A continuous distribution should provide a `rvs` method for sampling (such as those from `scipy.stats.distributions`). The `rvs` method should take a keyword argument `size` which determines the number of samples to draw.

Comparing Grid and Randomized Search
------------------------------------

While both grid search and randomized search evaluate parameter settings from a grid or distribution, they differ in how they explore the parameter space:

- Grid Search explores all combinations within the grid, which can be computationally expensive for large parameter spaces
- Randomized Search explores a fixed number of parameter settings sampled from the specified distributions

Randomized search can be more efficient when only a small number of parameters actually influence the final performance of the machine learning model.

Tips for Parameter Search
-------------------------

- Use :class:`~sklearn.pipeline.Pipeline` to chain transformers and estimators together
- Preprocess data appropriately before parameter search
- Use appropriate scoring metrics for your problem
- Consider using :class:`~sklearn.model_selection.cross_val_score` to evaluate model performance
- Monitor computation time and adjust search space accordingly

See Also
--------

:class:`~sklearn.model_selection.GridSearchCV` : Exhaustive search over specified parameter values.
:class:`~sklearn.model_selection.RandomizedSearchCV` : Randomized search on hyper parameters.
:class:`~sklearn.model_selection.ParameterGrid` : Grid of parameters with a discrete number of values for each.
:class:`~sklearn.model_selection.ParameterSampler` : Generator on parameters sampled from given distributions.