Cross Decomposition
==================

The :mod:`sklearn.cross_decomposition` module includes Partial Least Squares (PLS) regression and canonical correlation analysis (CCA), which are statistical methods used for modeling relations between two sets of variables.

.. currentmodule:: sklearn.cross_decomposition

Available Classes
----------------

.. autosummary::
   :toctree: generated/
   :template: class.rst

   PLSCanonical
   PLSRegression
   PLSSVD

Mathematical Formulation
------------------------

PLS finds a linear model by projecting both the predictors (X) and the targets (Y) to a new shared latent space. The goal is to maximize the covariance between the latent scores of X and Y.

The model can be expressed as:

X = T Pᵀ + E  
Y = U Qᵀ + F  

Where T and U are the score matrices, P and Q are the loading matrices, and E and F are the error terms.

For regression tasks, PLS predicts Y from X using the relationship:

Ŷ = X W (Pᵀ W)⁻¹ Qᵀ

Where W contains the weight vectors.

Key Parameters
--------------

- **n_components**: Number of components to keep
- **scale**: Whether to scale the data (default: True)
- **max_iter**: Maximum number of iterations for power method
- **tol**: Tolerance for convergence

Implementation Details
----------------------

The implementation provides two algorithms for computing singular vectors:
1. Full SVD decomposition (default for most cases)
2. Power method (used when specific conditions are met or requested)

The power method implementation follows the algorithm described in Wegelin's review of Partial Least Squares methods and includes special handling for different operational modes (A and B).

Examples
--------

>>> from sklearn.cross_decomposition import PLSRegression
>>> X = [[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]]
>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
>>> pls = PLSRegression(n_components=2)
>>> pls.fit(X, Y)
PLSRegression()
>>> Y_pred = pls.predict(X)

Notes
-----

- PLS is particularly useful when predictors are highly collinear
- The method can handle multiple response variables simultaneously
- Different variants (PLSCanonical, PLSRegression) optimize for different objectives
- The implementation includes proper convergence checking and warning systems

References
----------

[1] Wegelin, J.A. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case.
[2] Abdi, H. Partial least squares regression and projection on latent structure regression.