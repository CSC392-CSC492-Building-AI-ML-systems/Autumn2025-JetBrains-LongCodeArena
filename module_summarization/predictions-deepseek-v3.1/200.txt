Novelty and Outlier Detection
=============================

Many applications require being able to decide whether a new observation
belongs to the same distribution as existing observations (it is an *inlier*),
or should be considered as different (it is an *outlier*). Often, this ability
is used to clean real data sets. Two important distinctions must be made:

**Novelty Detection:**
  The training data is not polluted by outliers, and we are interested in
  detecting anomalies in new observations.

**Outlier Detection:**
  The training data contains outliers, and we need to fit the central
  mode of the training data, ignoring the deviant observations.

Scikit-learn provides a set of machine learning tools that can be used
for both novelty and outlier detection. This documentation section
covers the available algorithms and their specific use cases.

Available Algorithms
-------------------

- **Isolation Forest:** An efficient algorithm for outlier detection especially
  in high-dimensional datasets. It works by randomly selecting a feature and
  then randomly selecting a split value between the maximum and minimum values
  of the selected feature.

- **Local Outlier Factor (LOF):** An algorithm for outlier detection that
  computes the local density deviation of a given data point with respect to
  its neighbors. It considers as outliers the samples that have a substantially
  lower density than their neighbors.

- **One-Class SVM:** A novelty detection algorithm that learns a decision
  function for novelty detection: classifying new data as similar or different
  to the training set.

- **Elliptic Envelope:** Assumes the data is Gaussian and learns an ellipse.
  It thus fits a robust covariance estimate to the data, and thus fits an
  ellipse to the central data points, ignoring points outside the central mode.

- **DBSCAN:** A density-based clustering algorithm that can be used for outlier
  detection. Points in low-density regions are considered outliers.

Usage Examples
--------------

All estimators in scikit-learn implement a consistent interface for outlier
detection. The general approach is to instantiate an estimator, fit it on
the training data, and then use the `predict` or `decision_function` methods
to classify new data points.

For algorithms that support it, the `score_samples` method can be used to
obtain an inlierness score for each sample.

Model Selection and Evaluation
------------------------------

Evaluating outlier detection algorithms can be challenging due to the
inherently unsupervised nature of the problem. Common approaches include:

- Using labeled data when available (treated as a classification problem)
- Visual inspection of results (e.g., using dimensionality reduction)
- Domain-specific evaluation metrics

It's important to note that the choice of algorithm and its parameters
should be guided by the specific characteristics of your dataset and
the requirements of your application.

See the individual algorithm documentation for specific implementation
details and examples.