Working with Text Data
=====================

This tutorial covers the main tools available in scikit-learn for working with text data, including feature extraction, preprocessing, and vectorization techniques.

Introduction
------------

Text data requires special preprocessing and feature extraction before it can be used with machine learning algorithms. The :mod:`sklearn.feature_extraction.text` submodule provides utilities to convert text documents into numerical feature vectors.

Preprocessing Functions
-----------------------

The module includes several text preprocessing utilities:

.. autofunction:: sklearn.feature_extraction.text.strip_accents_unicode
   :noindex:

.. autofunction:: sklearn.feature_extraction.text.strip_accents_ascii
   :noindex:

.. autofunction:: sklearn.feature_extraction.text.strip_tags
   :noindex:

These functions help normalize text by:
- Removing accents and diacritics from Unicode characters
- Converting accented characters to ASCII equivalents
- Stripping HTML/XML tags from text

Core Vectorization Classes
--------------------------

CountVectorizer
~~~~~~~~~~~~~~~
Converts a collection of text documents to a matrix of token counts.

TfidfVectorizer
~~~~~~~~~~~~~~~
Converts a collection of text documents to a matrix of TF-IDF features.

HashingVectorizer
~~~~~~~~~~~~~~~~~
Converts a collection of text documents to a matrix of token occurrences using feature hashing.

TfidfTransformer
~~~~~~~~~~~~~~~~
Transforms a count matrix to a normalized TF or TF-IDF representation.

Basic Usage Example
-------------------

Here's a simple example of text vectorization:

.. code-block:: python

    from sklearn.feature_extraction.text import CountVectorizer
    
    # Sample documents
    corpus = [
        'This is the first document.',
        'This document is the second document.',
        'And this is the third one.',
        'Is this the first document?',
    ]
    
    # Create vectorizer
    vectorizer = CountVectorizer()
    
    # Fit and transform the documents
    X = vectorizer.fit_transform(corpus)
    
    # Get feature names
    print(vectorizer.get_feature_names_out())
    
    # View the document-term matrix
    print(X.toarray())

Advanced Features
-----------------

The vectorizers support several advanced features:

- **N-grams**: Capture phrases and word sequences using `ngram_range` parameter
- **Stop words**: Remove common words using built-in or custom stop word lists
- **Token patterns**: Customize token extraction using regular expressions
- **Text normalization**: Apply lowercase conversion and accent removal
- **Vocabulary control**: Specify custom vocabulary or let it be learned from data

Working with Different Input Types
----------------------------------

The vectorizers can handle various input types:

- Raw text strings
- File objects
- Filenames (automatically read from disk)

Best Practices
--------------

1. Always preprocess text consistently across training and test data
2. Consider using TF-IDF weighting for better performance in many applications
3. Use stop word removal for tasks where common words don't carry meaningful information
4. Experiment with different n-gram ranges to capture relevant context
5. For large datasets, consider using HashingVectorizer to save memory

See Also
--------

- :ref:`Feature extraction <feature_extraction>`
- :ref:`Preprocessing data <preprocessing>`
- :ref:`Working with categorical data <preprocessing_categorical_features>`