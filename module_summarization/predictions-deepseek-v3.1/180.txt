Cross-validation: evaluating estimator performance
====================================================

Learning curves and validation curves are essential tools for diagnosing model performance and understanding the behavior of machine learning estimators during the training process. The `LearningCurveDisplay` class provides a visualization API to create informative plots showing the relationship between training size and model performance.

LearningCurveDisplay class
--------------------------

The `LearningCurveDisplay` class creates visualizations of learning curves, which show how the training and validation scores change as the number of training samples increases.

### Key Features

- **Flexible visualization**: Supports different display styles for standard deviation representation including error bars and filled regions
- **Automatic scale detection**: Automatically determines whether to use linear or logarithmic scaling for the x-axis based on data distribution
- **Score customization**: Handles both regular and negated scores with proper labeling

### Parameters

- **train_sizes** (ndarray): Array of training set sizes used to generate the curve
- **train_scores** (ndarray): Scores on training sets across different cross-validation folds
- **test_scores** (ndarray): Scores on test sets across different cross-validation folds  
- **score_name** (str): Name of the scoring metric used for labeling

### Visualization Options

The class supports three display styles for variability representation:

1. **fill_between**: Shows mean scores as lines with filled regions representing Â±1 standard deviation
2. **errorbar**: Displays mean scores with error bars indicating standard deviation
3. **None**: Shows only mean scores without variability indicators

### Usage Example

The recommended way to create learning curve visualizations is through the `from_estimator` class method, which computes the learning curve and creates the display in a single step.

### Interpretation

Learning curves help identify:
- Underfitting (both training and validation scores are low)
- Overfitting (large gap between training and validation scores)
- Sufficient training data (scores plateau as training size increases)

The visualization provides insights into whether collecting more training data would improve model performance and helps optimize the trade-off between bias and variance.

See Also
--------
- :func:`sklearn.model_selection.learning_curve`: Function to compute learning curve data
- :class:`sklearn.model_selection.ValidationCurveDisplay`: For validation curve visualizations

Note: The visualization API requires matplotlib to be installed. The `log_scale` parameter is deprecated as of version 1.3 and will be removed in 1.5. Use `ax_.set_xscale()` and `ax_.set_yscale()` methods instead for scale control.