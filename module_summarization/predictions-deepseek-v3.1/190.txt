Linear and Quadratic Discriminant Analysis
==========================================

.. currentmodule:: sklearn.discriminant_analysis

Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are two classic classifiers that model class conditional densities using Gaussian distributions.

Mathematical Formulation
------------------------

Both LDA and QDA assume the data comes from a Gaussian distribution. For LDA, all classes share the same covariance matrix, while QDA allows each class to have its own covariance matrix.

The class conditional density for a sample *x* in class *k* is given by:

.. math::

   P(x|y=k) = \frac{1}{(2\pi)^{d/2}|\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right)

where:
- :math:`d` is the number of features
- :math:`\mu_k` is the mean of class *k*
- :math:`\Sigma_k` is the covariance matrix of class *k*

For LDA: :math:`\Sigma_k = \Sigma` for all *k*  
For QDA: :math:`\Sigma_k` varies per class

The prediction is made using Bayes' rule:

.. math::

   \hat{y} = \arg\max_k P(y=k|x) = \arg\max_k \left[ \log P(x|y=k) + \log P(y=k) \right]

Linear Discriminant Analysis
----------------------------

LDA assumes equal covariance matrices across classes, which leads to linear decision boundaries. The classifier finds a linear combination of features that best separates the classes.

Quadratic Discriminant Analysis
-------------------------------

QDA allows different covariance matrices for each class, resulting in quadratic decision boundaries. This provides more flexibility but requires estimating more parameters.

Usage Examples
--------------

.. code-block:: python

   from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
   from sklearn.datasets import make_classification
   from sklearn.model_selection import train_test_split

   # Generate sample data
   X, y = make_classification(n_samples=1000, n_features=4, random_state=0)
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

   # LDA classifier
   lda = LinearDiscriminantAnalysis()
   lda.fit(X_train, y_train)
   lda_score = lda.score(X_test, y_test)

   # QDA classifier
   qda = QuadraticDiscriminantAnalysis()
   qda.fit(X_train, y_train)
   qda_score = qda.score(X_test, y_test)

Advantages and Limitations
--------------------------

**LDA Advantages:**
- Fewer parameters to estimate than QDA
- More robust with limited training data
- Can be used for dimensionality reduction

**QDA Advantages:**
- More flexible decision boundaries
- Can model more complex class distributions

**Limitations:**
- Both assume Gaussian distributed data
- Performance degrades with violations of Gaussian assumption
- QDA requires sufficient data to estimate class covariance matrices

See Also
--------

:class:`LinearDiscriminantAnalysis`  
:class:`QuadraticDiscriminantAnalysis`