.. _pipeline_tutorial:

Pipeline Tutorial
=================

The :mod:`sklearn.pipeline` module provides utilities for building composite estimators as chains of transforms and estimators. This tutorial demonstrates how to create and use pipelines effectively in scikit-learn.

Overview
--------

A Pipeline sequentially applies a list of transforms followed by a final estimator. Intermediate steps must implement both `fit` and `transform` methods, while the final estimator only needs to implement `fit`. Pipelines enable:

- Combining multiple processing steps into a single estimator
- Cross-validating the entire process together
- Setting parameters for individual steps using double-underscore syntax
- Caching transformers to avoid repeated computation

Basic Usage
-----------

Here's a simple example creating and using a pipeline:

.. code-block:: python

    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import Pipeline
    
    # Create a pipeline with scaling and classification
    pipe = Pipeline([
        ('scaler', StandardScaler()), 
        ('svc', SVC())
    ])
    
    # Use like any other estimator
    pipe.fit(X_train, y_train)
    predictions = pipe.predict(X_test)

Parameter Tuning
----------------

You can set parameters for individual steps using the `__` syntax:

.. code-block:: python

    # Set the C parameter of the SVC step
    pipe.set_params(svc__C=10)
    
    # Or during grid search
    param_grid = {'svc__C': [0.1, 1, 10]}
    grid_search = GridSearchCV(pipe, param_grid)

Caching Transformers
--------------------

Use the `memory` parameter to cache fitted transformers:

.. code-block:: python

    from sklearn.pipeline import Pipeline
    from tempfile import mkdtemp
    
    # Cache transformers to a temporary directory
    cachedir = mkdtemp()
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('svc', SVC())
    ], memory=cachedir)

Convenience Functions
---------------------

Use `make_pipeline` for simplified pipeline creation:

.. code-block:: python

    from sklearn.pipeline import make_pipeline
    
    # Automatically names steps
    pipe = make_pipeline(StandardScaler(), SVC())

Additional Features
-------------------

- **Metadata routing**: Pipelines support metadata routing for advanced use cases
- **Output configuration**: Control output format (pandas DataFrame or default) using `set_output()`
- **Inspection**: Access individual steps via `named_steps` attribute

See the :ref:`API reference <pipeline_api>` for complete details on parameters and methods.