Neural Network Models (Supervised)
==================================

Activation Functions
--------------------

The module provides several in-place activation functions commonly used in neural networks:

.. autofunction:: inplace_identity
.. autofunction:: inplace_logistic
.. autofunction:: inplace_tanh
.. autofunction:: inplace_relu
.. autofunction:: inplace_softmax

Available activation functions are stored in the ``ACTIVATIONS`` dictionary:

- ``'identity'``: Identity activation (no transformation)
- ``'logistic'``: Logistic sigmoid activation
- ``'tanh'``: Hyperbolic tangent activation
- ``'relu'``: Rectified Linear Unit activation
- ``'softmax'``: Softmax activation for multi-class classification

Activation Function Derivatives
-------------------------------

Corresponding derivative functions for backpropagation:

.. autofunction:: inplace_identity_derivative
.. autofunction:: inplace_logistic_derivative
.. autofunction:: inplace_tanh_derivative
.. autofunction:: inplace_relu_derivative

Available derivatives are stored in the ``DERIVATIVES`` dictionary:

- ``'identity'``: Derivative of identity function
- ``'logistic'``: Derivative of logistic sigmoid function
- ``'tanh'``: Derivative of hyperbolic tangent function
- ``'relu'``: Derivative of ReLU function

Loss Functions
--------------

The module provides several loss functions for supervised learning:

.. autofunction:: squared_loss
   :noindex:

.. autofunction:: log_loss
   :noindex:

.. autofunction:: binary_log_loss
   :noindex:

Available loss functions are stored in the ``LOSS_FUNCTIONS`` dictionary:

- ``'squared_loss'``: Squared loss for regression tasks
- Additional loss functions available for classification tasks

Note: All activation functions and their derivatives operate in-place for memory efficiency, modifying the input arrays directly rather than creating new arrays.