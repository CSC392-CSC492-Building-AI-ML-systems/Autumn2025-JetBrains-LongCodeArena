# Scale Schedule Module Documentation

## Overview

The `scale_schedule` module provides stateless learning rate schedulers designed to interface with Composer's time abstraction system. These schedulers allow for flexible configuration using explicit time units and can be easily integrated with the Composer training framework.

## Key Features

- **Stateless Design**: Unlike PyTorch's built-in schedulers, these are stateless and work directly with Composer's time abstraction
- **Time Unit Flexibility**: Schedulers can be configured using various time units (epochs, batches, tokens, etc.)
- **SSR Support**: Built-in support for Scale Schedule Ratio (SSR) to stretch or compress learning rate schedules
- **Protocol-Based**: Implemented using Python's Protocol system for easy extension

## Core Components

### ComposerScheduler Protocol

The `ComposerScheduler` protocol defines the interface for all stateless scheduler functions:

```python
class ComposerScheduler(Protocol):
    def __call__(self, state: State, ssr: float = 1.0) -> float:
```

**Parameters:**
- `state`: The current Composer Trainer state containing timing information
- `ssr`: Scale Schedule Ratio (default: 1.0) for stretching the schedule

**Returns:**
- A multiplier (α) to apply to the optimizer's base learning rate

### Available Schedulers

The module includes several built-in scheduler implementations:

- `StepScheduler`: Step decay scheduler
- `MultiStepScheduler`: Multi-step decay scheduler
- `ConstantScheduler`: Constant learning rate
- `LinearScheduler`: Linear decay scheduler
- `ExponentialScheduler`: Exponential decay scheduler
- `CosineAnnealingScheduler`: Cosine annealing scheduler
- `CosineAnnealingWarmRestartsScheduler`: Cosine annealing with warm restarts
- `PolynomialScheduler`: Polynomial decay scheduler
- Various warmup variants: `MultiStepWithWarmupScheduler`, `ConstantWithWarmupScheduler`, etc.

### Utility Functions

- `compile_composer_scheduler()`: Converts stateless schedulers to PyTorch scheduler objects
- `_convert_time()`: Internal helper for time unit conversion with SSR support

## Usage Examples

**Simple function-based scheduler:**
```python
def custom_scheduler(state: State) -> float:
    if state.timestamp.epoch < 10:
        return 1.0
    return 0.5
```

**Class-based configurable scheduler:**
```python
class ConfigurableScheduler:
    def __init__(self, threshold_epoch: int):
        self.threshold_epoch = threshold_epoch
        
    def __call__(self, state: State) -> float:
        if state.timestamp.epoch < self.threshold_epoch:
            return 1.0
        return 0.5
```

## Mathematical Foundation

The learning rate at time t is calculated as:
η(t) = η_i × α(t)

Where:
- η_i: Initial learning rate (from optimizer)
- α(t): Multiplier from scheduler at time t

With SSR scaling:
α_σ(t) = α(t/σ)

## Integration with Composer Trainer

Schedulers can be easily integrated with the Composer Trainer:

```python
from composer import Trainer

trainer = Trainer(
    schedulers=[custom_scheduler],
    # other training parameters...
)
```

## Time Unit Handling

The module supports conversion between different time units (epoch, batch, token, duration) and properly handles SSR scaling for consistent behavior across different training durations.