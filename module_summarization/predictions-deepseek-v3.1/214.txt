Optimization Algorithms
======================

The estimation of model parameters is a fundamental task in statistical modeling and machine learning. This module provides a unified interface to several optimization algorithms available through `scipy.optimize`. These algorithms can be broadly categorized into three types: gradient-based methods, derivative-free methods, and global optimization methods.

Gradient-Based Methods
----------------------

Gradient-based optimization algorithms utilize information from the gradient (first derivative) of the objective function to efficiently locate minima or maxima. These methods are typically fast and efficient when derivatives are available and the objective function is smooth.

Available gradient-based methods include:

- **Newton-Raphson ('newton')**: Uses first and second derivatives (Hessian matrix) for quadratic convergence near the optimum. Requires the Hessian to be positive definite.
- **BFGS ('bfgs')**: A quasi-Newton method that approximates the Hessian matrix. Well-suited for medium-sized problems.
- **L-BFGS ('lbfgs')**: A limited-memory version of BFGS suitable for large-scale optimization problems with many parameters.
- **Conjugate Gradient ('cg')**: Iterative method suitable for large-scale problems that doesn't require storage of matrices.
- **Newton-CG ('ncg')**: Newton's method using conjugate gradient to solve the Newton system.

These methods typically require fewer function evaluations than derivative-free methods but depend on accurate gradient information.

Derivative-Free Methods
-----------------------

Derivative-free optimization algorithms do not require gradient information, making them suitable for problems where derivatives are unavailable, expensive to compute, or noisy.

Available derivative-free methods include:

- **Nelder-Mead ('nm')**: A simplex-based method that is robust but can be slow to converge. Suitable for small problems.
- **Powell's Method ('powell')**: A conjugate direction method that doesn't require derivatives. Effective for low-dimensional problems.

These methods are generally more robust to noise and non-smooth objective functions but may require more function evaluations to converge.

Global Optimization Methods
---------------------------

Global optimization methods attempt to find the global optimum rather than getting trapped in local optima. These are particularly useful for problems with multiple local minima.

Available global optimization methods include:

- **Basin-Hopping ('basinhopping')**: A stochastic algorithm that combines local minimization with random steps to explore the parameter space. It uses a Monte Carlo algorithm with local minimization at each step.

This method is more computationally intensive but can escape local minima and find better solutions for complex, multi-modal objective functions.

Algorithm Selection Considerations
----------------------------------

When choosing an optimization algorithm, consider:

1. **Problem size**: Gradient methods scale better for large problems
2. **Derivative availability**: Use derivative-free methods when gradients are unavailable
3. **Noise tolerance**: Derivative-free methods handle noisy objectives better
4. **Convergence speed**: Gradient methods typically converge faster
5. **Local vs global optima**: Use global methods for multi-modal problems

All methods support common optimization parameters including maximum iterations, convergence tolerances, and callback functions for monitoring progress.