Gaussian Mixture Models
=======================

.. currentmodule:: sklearn.mixture

A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. It is a popular clustering algorithm that also provides probabilistic cluster assignments.

Base Class
----------

The :class:`BaseMixture` class provides a common abstract base class for all mixture models in scikit-learn, implementing the core Expectation-Maximization (EM) algorithm and common initialization methods.

.. autoclass:: BaseMixture
   :members:
   :inherited-members:
   :exclude-members: fit, fit_predict

   .. rubric:: Methods

   .. automethod:: fit
   .. automethod:: fit_predict

Initialization Methods
----------------------

The mixture models support several initialization methods:

- **kmeans**: Initialize responsibilities using k-means clustering
- **random**: Initialize responsibilities randomly from a uniform distribution  
- **random_from_data**: Initialize by randomly selecting data points as initial components
- **k-means++**: Initialize using the k-means++ algorithm for better initial centroids

Model Fitting
-------------

The EM algorithm fits the model through these steps:

1. **Initialization**: Initialize parameters using one of the available methods
2. **Expectation Step (E-step)**: Calculate the posterior probabilities of each component given the data
3. **Maximization Step (M-step)**: Update parameters to maximize the expected log-likelihood
4. **Convergence Check**: Repeat E and M steps until convergence or maximum iterations

The algorithm runs multiple initializations (controlled by `n_init`) and selects the best solution based on log-likelihood.

Parameters
----------

Key parameters for controlling the fitting process:

- `n_components`: Number of mixture components
- `tol`: Convergence tolerance
- `reg_covar`: Regularization added to covariance matrices
- `max_iter`: Maximum number of EM iterations
- `n_init`: Number of initializations to perform
- `init_params`: Initialization method
- `random_state`: Random number generator for reproducibility

Examples
--------

Typically, users will work with concrete implementations like :class:`GaussianMixture` rather than the base class directly. See the specific mixture model implementations for usage examples.

References
----------

For theoretical background on Gaussian mixture models and the EM algorithm, see:

- Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
- Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society.