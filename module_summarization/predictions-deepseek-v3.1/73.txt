sklearn.naive_bayes
==================

.. currentmodule:: sklearn.naive_bayes

The :mod:`sklearn.naive_bayes` module implements Naive Bayes algorithms. These
are supervised learning methods based on applying Bayes' theorem with strong
(naive) feature independence assumptions.

Base Class
----------

.. autoclass:: _BaseNB
   :members: predict_joint_log_proba, predict, predict_log_proba, predict_proba
   :inherited-members:
   :exclude-members: _joint_log_likelihood, _check_X

Implemented Classifiers
-----------------------

GaussianNB
~~~~~~~~~~

.. autoclass:: GaussianNB
   :members:
   :inherited-members:
   :exclude-members: _joint_log_likelihood, _check_X

BernoulliNB
~~~~~~~~~~~

.. autoclass:: BernoulliNB
   :members:
   :inherited-members:
   :exclude-members: _joint_log_likelihood, _check_X

MultinomialNB
~~~~~~~~~~~~~

.. autoclass:: MultinomialNB
   :members:
   :inherited-members:
   :exclude-members: _joint_log_likelihood, _check_X

ComplementNB
~~~~~~~~~~~~

.. autoclass:: ComplementNB
   :members:
   :inherited-members:
   :exclude-members: _joint_log_likelihood, _check_X

CategoricalNB
~~~~~~~~~~~~~

.. autoclass:: CategoricalNB
   :members:
   :inherited-members:
   :exclude-members: _joint_log_likelihood, _check_X

Notes
-----

All naive Bayes classifiers support sample weighting through the `sample_weight`
parameter in their `fit` methods. The implemented models assume that the
features follow specific distributions (Gaussian, Bernoulli, multinomial, etc.),
and the choice of classifier should be based on the nature of the features.

References
----------

For GaussianNB, the online update algorithm is based on Stanford CS tech report
STAN-CS-79-773 by Chan, Golub, and LeVeque:
http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf

Examples
--------

>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
>>> gnb = GaussianNB()
>>> y_pred = gnb.fit(X_train, y_train).predict(X_test)
>>> print("Number of mislabeled points out of a total %d points : %d"
...       % (X_test.shape[0], (y_test != y_pred).sum()))
Number of mislabeled points out of a total 75 points : 4