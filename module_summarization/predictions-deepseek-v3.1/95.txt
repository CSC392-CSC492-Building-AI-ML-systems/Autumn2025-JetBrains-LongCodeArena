.. _schedulers:

Stateless Learning Rate Schedulers
==================================

The ``composer.schedulers`` module provides stateless learning rate schedulers designed to work with Composer's time abstraction system. These schedulers solve several limitations of PyTorch's built-in schedulers by allowing configuration using explicit time units through Composer's :mod:`~composer.core.time` abstraction.

Overview
--------

Stateless schedulers implement the :class:`ComposerScheduler` protocol, which defines a callable interface that returns a learning rate multiplier based on the current training state. These schedulers are pure functions without internal state, making them compatible with Composer's training loop and time units.

Key Features
------------

- **Time Unit Flexibility**: Schedulers can be configured using various time units (epochs, batches, tokens, etc.)
- **Stateless Design**: No internal state management required
- **Multiplicative Stacking**: Multiple schedulers can be combined with multiplicative effects
- **SSR Support**: Scale Schedule Ratio (SSR) allows for dynamic schedule stretching

Core Components
---------------

.. autoclass:: composer.schedulers.ComposerScheduler
   :members:
   :special-members: __call__

.. autofunction:: composer.schedulers.compile_composer_scheduler

.. autofunction:: composer.schedulers._convert_time

Available Schedulers
--------------------

The module provides several built-in scheduler implementations:

- :class:`StepScheduler`
- :class:`MultiStepScheduler`
- :class:`ConstantScheduler`
- :class:`LinearScheduler`
- :class:`ExponentialScheduler`
- :class:`CosineAnnealingScheduler`
- :class:`CosineAnnealingWarmRestartsScheduler`
- :class:`PolynomialScheduler`
- :class:`MultiStepWithWarmupScheduler`
- :class:`ConstantWithWarmupScheduler`
- :class:`LinearWithWarmupScheduler`
- :class:`CosineAnnealingWithWarmupScheduler`
- :class:`PolynomialWithWarmupScheduler`

Usage Examples
--------------

Basic function-based scheduler:

.. code-block:: python

   def custom_scheduler(state: State) -> float:
       if state.timestamp.epoch < 10:
           return 1.0
       return 0.5

Class-based scheduler with configuration:

.. code-block:: python

   class ConfigurableScheduler:
       def __init__(self, threshold_epoch: int):
           self.threshold_epoch = threshold_epoch
       
       def __call__(self, state: State) -> float:
           if state.timestamp.epoch < self.threshold_epoch:
               return 1.0
           return 0.5

Mathematical Representation
---------------------------

The learning rate :math:`\eta` at time :math:`t` is calculated as:

.. math::
   \eta(t) = \eta_i \times \alpha(t)

Where:
- :math:`\eta_i` is the initial learning rate
- :math:`\alpha(t)` is the multiplier returned by the scheduler

With Scale Schedule Ratio (SSR) :math:`\sigma`:

.. math::
   \alpha_{\sigma}(t) = \alpha(t / \sigma)

Notes
-----

- Schedulers return multipliers rather than absolute learning rates
- Multiple schedulers stack multiplicatively when used together
- Time conversion handles various time units and SSR scaling automatically
- The module requires ``state.max_duration`` and ``state.dataloader_len`` to be set for proper operation