.. _calibration:

Probability Calibration
======================

.. currentmodule:: sklearn.calibration

Probability calibration is the process of adjusting the output probabilities of a classifier to better match the expected distribution of probabilities. This is particularly useful for classifiers that may output poorly calibrated probabilities (e.g., Support Vector Machines) or when accurate probability estimates are required for downstream tasks.

Overview
--------

The :class:`CalibratedClassifierCV` class provides two common methods for probability calibration:

- **Sigmoid (Platt's method)**: Fits a logistic regression model to the classifier's outputs. This method is efficient and works well with large datasets but assumes a sigmoid shape in the calibration curve.
- **Isotonic regression**: A non-parametric approach that fits a piecewise constant function. It is more flexible but can overfit with small datasets (typically < 1000 samples).

The class supports calibration via cross-validation to avoid overfitting and can operate in two modes:

- **Ensemble mode (ensemble=True)**: Fits multiple classifier-calibrator pairs on different cross-validation folds and averages their predictions.
- **Non-ensemble mode (ensemble=False)**: Uses cross-validation to generate unbiased predictions for calibration, then trains a single classifier on all data.

Basic Usage
-----------

Here is a simple example of calibrating a LinearSVC classifier using Platt's method:

.. code-block:: python

    from sklearn.calibration import CalibratedClassifierCV
    from sklearn.svm import LinearSVC
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split

    # Generate a sample dataset
    X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize and fit the calibrator
    clf = LinearSVC()
    calibrated_clf = CalibratedClassifierCV(estimator=clf, method='sigmoid', cv=5)
    calibrated_clf.fit(X_train, y_train)

    # Predict probabilities
    proba = calibrated_clf.predict_proba(X_test)

Parameters
----------

- **estimator**: The base classifier to calibrate. Defaults to :class:`~sklearn.svm.LinearSVC`.
- **method**: The calibration method ('sigmoid' or 'isotonic').
- **cv**: Cross-validation strategy. Use "prefit" if the estimator is already fitted.
- **n_jobs**: Number of parallel jobs (only for cv != "prefit").
- **ensemble**: Whether to use an ensemble of calibrated classifiers.

Attributes
----------

After fitting, the following attributes are available:

- **classes_**: Array of class labels.
- **n_features_in_**: Number of features seen during fit.
- **feature_names_in_**: Names of features seen during fit (if available).
- **calibrated_classifiers_**: List of calibrated classifier instances.

Advanced Usage
--------------

Pre-fit Calibration
~~~~~~~~~~~~~~~~~~~

If you have a pre-trained classifier, you can calibrate it using the "prefit" option:

.. code-block:: python

    # Assume clf is already fitted
    calibrated_clf = CalibratedClassifierCV(estimator=clf, method='sigmoid', cv='prefit')
    calibrated_clf.fit(X_calib, y_calib)  # Use separate calibration data

Handling Multiclass Problems
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`CalibratedClassifierCV` supports multiclass calibration using a one-vs-rest approach internally.

.. note::
   Calibration is most beneficial for models whose raw probability estimates are poorly calibrated. Always evaluate calibration using proper metrics (e.g., Brier score, reliability diagrams) before and after calibration.

See Also
--------

- :ref:`User Guide <calibration>`
- :class:`sklearn.svm.LinearSVC`
- :class:`sklearn.isotonic.IsotonicRegression`