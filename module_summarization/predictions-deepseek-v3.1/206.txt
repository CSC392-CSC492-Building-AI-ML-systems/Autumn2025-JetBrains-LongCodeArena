Linear Support Vector Classification
=====================================

.. currentmodule:: sklearn.svm

The :class:`LinearSVC` class implements a linear support vector machine (SVM)
for classification tasks. Unlike the standard :class:`SVC` with a linear kernel,
this implementation is based on liblinear rather than libsvm, offering greater
flexibility in penalty and loss function choices, and better scalability to
large datasets.

Key differences from :class:`SVC` include the default loss function and how
intercept regularization is handled. This implementation supports both dense
and sparse input arrays and handles multiclass problems using a one-vs-the-rest
scheme.

Parameters
----------
penalty : {'l1', 'l2'}, default='l2'
    Norm used for regularization. 'l2' is standard for SVMs, while 'l1'
    produces sparse coefficient vectors.

loss : {'hinge', 'squared_hinge'}, default='squared_hinge'
    Loss function specification. 'hinge' represents the standard SVM loss,
    while 'squared_hinge' uses the squared hinge loss. Note that the
    combination ``penalty='l1'`` and ``loss='hinge'`` is not supported.

dual : "auto" or bool, default=True
    Algorithm selection for solving either the dual or primal optimization
    problem. Prefer ``dual=False`` when n_samples > n_features. The "auto"
    option automatically chooses based on dataset characteristics and
    parameter combinations.

tol : float, default=1e-4
    Tolerance threshold for stopping criteria.

C : float, default=1.0
    Regularization parameter controlling constraint violation penalty.
    The strength of regularization is inversely proportional to C.
    Must be strictly positive.

multi_class : {'ovr', 'crammer_singer'}, default='ovr'
    Multiclass handling strategy. "ovr" trains separate one-vs-rest classifiers
    for each class, while "crammer_singer" optimizes a joint objective across
    all classes (theoretical but computationally expensive).

fit_intercept : bool, default=True
    Whether to calculate the intercept term. If False, data is assumed to be
    already centered.

intercept_scaling : float, default=1.0
    When fit_intercept is True, this scales the synthetic feature added to
    the instance vector. Helps reduce regularization impact on the intercept.

Notes
-----
The underlying implementation uses BLAS operations through optimized Cython
routines for efficient linear algebra computations. The library interfaces
with liblinear for core SVM optimization routines.

For large-scale problems, consider using the primal formulation (dual=False)
when the number of samples exceeds the number of features, as this typically
provides better computational performance.

References
----------
.. [1] `LIBLINEAR: A Library for Large Linear Classification
       <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`_

.. [2] `Support-Vector Networks
       <https://link.springer.com/article/10.1007/BF00994018>`_
       Machine Learning, 1995.

Examples
--------
>>> from sklearn.svm import LinearSVC
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_features=4, random_state=0)
>>> clf = LinearSVC(random_state=0)
>>> clf.fit(X, y)
LinearSVC(random_state=0)
>>> print(clf.coef_)
[[0.141... 0.526... 0.679... 0.493...]]
>>> print(clf.intercept_)
[0.1693...]
>>> print(clf.predict([[0, 0, 0, 0]]))
[1]