Learn Gaussian Mixture Models
=============================

Gaussian Mixture Models (GMMs) are a probabilistic approach to clustering and density estimation. A GMM models the data as a mixture of K Gaussian components, each with its own mean and covariance. The model is learned via the Expectation-Maximization (EM) algorithm, which iteratively refines the component responsibilities (E-step) and the component parameters (M-step) until convergence.

This documentation describes the base interface for mixture models and the common configuration used to learn Gaussian Mixture Models with EM.

Base interface
--------------

The base class for mixture models provides the common API and EM-based fitting routine used by Gaussian Mixture implementations.

Key concepts
- Number of components: The model uses n_components Gaussian components to explain the data.
- Convergence: The algorithm stops when the change in likelihood (or lower bound) is less than tol or when max_iter iterations are reached.
- Initialization: The initial responsibilities (soft assignments) are crucial for EM convergence and can be initialized in several ways (see Initialization strategies).
- Regularization: reg_covar adds a small constant to the diagonal of covariance matrices to ensure numerical stability.
- Randomness: random_state controls the randomness of initialization and any stochastic steps.
- Reproducibility: warm_start allows continuing training from a previous fit without reinitialization.

Configuration and parameters
----------------------------

- n_components
  - Type: int
  - Description: Number of Gaussian components in the mixture.

- tol
  - Type: float
  - Description: Convergence threshold for the log-likelihood / lower bound improvement. Lower values lead to more precise convergence.

- reg_covar
  - Type: float
  - Description: Regularization added to the diagonal of covariance matrices to improve numerical stability.

- max_iter
  - Type: int
  - Description: Maximum number of EM iterations per initialization.

- n_init
  - Type: int
  - Description: Number of initializations to perform. The best result (highest likelihood) is kept.

- init_params
  - Type: str
  - Options:
    - "kmeans": use KMeans to initialize responsibilities
    - "random": initialize with random soft assignments
    - "random_from_data": select random data points as initial centers and create corresponding one-hot assignments
    - "k-means++": initialize using a k-means++-style strategy
  - Description: Method used to initialize the EM algorithm.

- random_state
  - Type: int / RandomState / None
  - Description: Seed or RNG to ensure reproducible initialization.

- warm_start
  - Type: bool
  - Description: If True, reuse the solution of the previous call to fit and add more iterations or initializations without reinitializing.

- verbose
  - Type: int
  - Description: Controls the verbosity of the fitting process.

- verbose_interval
  - Type: int
  - Description: Interval (in iterations) at which to print progress when verbose > 0.

Initialization strategies
-------------------------

The initial responsibilities determine the starting point for EM and can impact convergence speed and final solutions.

- kmeans
  - Initialize with a hard assignment based on a KMeans clustering result. Each sample is assigned to its closest component, forming a one-hot responsibility matrix.

- random
  - Initialize with random soft responsibilities that sum to 1 for each sample.

- random_from_data
  - Select a subset of data points as initial centers and create corresponding one-hot assignments.

- k-means++
  - Use a k-means++-style initial center selection to form a good starting set of component assignments.

The EM algorithm
----------------

- E-step (Expectation)
  - Compute the responsibilities (soft assignments) of each data point to each Gaussian component given current parameters.

- M-step (Maximization)
  - Update the component parameters (means, covariances, and mixing proportions) to maximize the expected complete-data log-likelihood given the current responsibilities.

- Convergence and checks
  - The algorithm iterates between E-step and M-step until:
    - The improvement in log-likelihood / lower bound is less than tol, or
    - max_iter iterations are reached.
  - If no satisfactory convergence is achieved within max_iter, a ConvergenceWarning is raised.
  - Validation of inputs and shapes is performed throughout fitting.

Usage example
-------------

.. code-block:: python

   # Assuming GaussianMixture is a concrete implementation deriving from the base class
   from your_ml_lib import GaussianMixture

   # Create a GMM with 3 components and kmeans++ initialization
   gmm = GaussianMixture(n_components=3, init_params="k-means++", random_state=0)

   # Learn the parameters and predict labels for the data
   labels = gmm.fit_predict(X)

Notes
-----

- The base interface expects concrete implementations to define _check_parameters and _initialize, which validate and initialize model parameters for a specific Gaussian mixture variant.
- The design follows a fit_predict workflow, where fitting is performed and the most likely component labels for the data are returned.
- Covariance regularization (reg_covar) helps prevent singular covariance matrices during learning.

See also
--------

- KMeans initialization (for clustering-based initialization)
- Random and k-means++ initialization strategies
- Convergence warnings and tolerance handling in iterative optimization

This documentation provides the high-level concept of learning Gaussian Mixture Models using the base mixture interface and EM-based parameter estimation.