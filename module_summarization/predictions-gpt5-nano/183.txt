Feature selection and dimensionality reduction on sample sets
==============================================================

Overview
--------
High-dimensional datasets (many features) can lead to overfitting, increased computational cost, and degraded model performance. Feature selection and dimensionality reduction aim to reduce the number of features while preserving the information needed for modeling. Feature selection selects a subset of existing features, while dimensionality reduction creates new features that summarize the original information.

Key distinctions
- Feature selection (selecting features): Keeps a subset of the original features, often based on their relationship to the target or their variability.
- Dimensionality reduction (creating features): Transforms the data into a lower-dimensional space, generating new features that may combine information from multiple original features.

When to use
- When n_features is much larger than n_samples (high risk of overfitting).
- When computation or storage constraints are tight.
- When interpretability of selected features is important (feature selection).
- When discovering latent structure or data geometry is helpful (dimensionality reduction).

Common approaches (scikit-learn style terminology)
1) Feature selection
   - Filter methods
     - VarianceThreshold: remove features with low variance.
     - SelectKBest: select top k features based on univariate scores (e.g., f_classif, f_regression, chi2, mutual_info_classif, mutual_info_regression).
   - Embedded methods
     - SelectFromModel: select features based on model-specific feature importance (e.g., L1-penalized models, tree-based models).
   - Wrapper methods
     - Recursive Feature Elimination (RFE) and RFECV: iteratively remove least-important features using a base estimator.
   Notes:
   - Filter methods are fast and model-agnostic but may ignore feature interactions.
   - Embedded methods integrate feature selection into model training.
   - Wrapper methods can be computationally intensive but may yield strong predictive performance.

2) Dimensionality reduction
   - Linear methods
     - Principal Component Analysis (PCA): projects data to orthogonal components capturing maximum variance.
     - IncrementalPCA: batch-friendly variant for large datasets.
     - TruncatedSVD: suitable for sparse data (often used with TF-IDF-like representations).
     - Independent Component Analysis (ICA): seeks statistically independent components.
     - FactorAnalysis: probabilistic factor analysis approach.
   - Nonlinear methods
     - KernelPCA: nonlinear feature extraction using kernel functions.
     - Isomap, Locally Linear Embedding (LLE), t-SNE: capture nonlinear structure; commonly used for visualization, but with caveats for subsequent modeling.
   - Non-negativity and parts-based approaches
     - Non-negative Matrix Factorization (NMF): learns additive parts-based components; requires non-negative data.
   Notes:
   - PCA is a common first step for noise reduction and visualization, and often improves downstream learning when features are highly correlated.
   - For sparse data, TruncatedSVD is typically preferred.
   - Nonlinear methods can reveal complex structure but may be less suitable for downstream predictive modeling without careful handling.

Usage patterns
- Supervised vs unsupervised
  - Supervised feature selection uses the target y to rank features (e.g., ANOVA F-tests, mutual information).
  - Unsupervised dimensionality reduction relies only on X (e.g., PCA, NMF) to capture structure.
- Pipelines
  - Combine scaling, selection/reduction, and learning in a pipeline to ensure consistent preprocessing in training and evaluation.
  - Use cross-validation to select the number of components or features (e.g., via explained_variance_ratio_, or cross-validated performance with RFECV).
- Data characteristics
  - Standardize/scale features when using methods sensitive to scale (e.g., PCA, SVM-based selectors, L1/L2 regularized models).
  - For sparse data, prefer methods that handle sparsity (e.g., TruncatedSVD, some feature selectors that work with sparse matrices).

Choosing the number of components or features
- Variance-based criteria: for PCA, retain components explaining a desired fraction of variance (e.g., n_components=0.95).
- Scree/elbow criterion: inspect explained_variance_ratio_ to find a point of diminishing returns.
- Cross-validated performance: select the number of features/components that optimizes predictive performance on held-out data.
- Domain constraints: preserve interpretability or comply with resource limits.

Practical tips
- Always standardize features before applying PCA or other scale-sensitive methods.
- For very large datasets, use incremental or partial fitting variants (IncrementalPCA, partial_fit patterns).
- Be mindful of data leakage in feature selection; perform selectors within cross-validation folds when evaluating model performance.
- Visualize reduced-dimensional representations (where appropriate) to gain intuition about structure and potential clusters or separability.

Workflow example (high-level)
- Given X (n_samples, n_features) and y (optional for supervised tasks):
  - Option A (feature selection for supervised tasks):
    - Choose a selector (e.g., SelectKBest with f_classif for classification).
    - Fit on training data and transform both train and test.
    - Train a classifier on the reduced feature set.
  - Option B (dimensionality reduction for visualization or preprocessing):
    - Choose a reducer (e.g., PCA with n_components=0.95).
    - Fit on training data and transform both train and test.
    - Train a classifier on the reduced feature set (if supervised).

Notes
- Some methods are better suited for certain data types (e.g., chi-squared tests for non-negative features; SVD for sparse data).
- Nonlinear methods can reveal structure but may complicate downstream modeling and interpretation.
- Always validate choices with cross-validation and consider the end-task requirements (interpretability, performance, latency, memory).

Further reading
- scikit-learn feature selection and dimensionality reduction modules and documentation
- Best practices for preprocessing, model evaluation, and pipeline design in high-dimensional settings