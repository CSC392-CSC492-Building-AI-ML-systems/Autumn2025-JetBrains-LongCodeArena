feature_extraction.text
=======================

Utilities to build feature vectors from text documents.

Overview
--------
This module provides components to convert raw text into numeric feature
vectors suitable for machine learning. It includes several vectorizers that
tokenize text, apply preprocessing, and produce sparse matrices, as well as
helpers for common text normalization tasks and a built-in stop-words list.

Public API
----------

Vectorizers
- HashingVectorizer
  - A hashing-based text vectorizer that maps tokens to a fixed-dimensional
    feature space using the hashing trick. Suitable for very large vocabularies
    or streaming data since it does not require building a vocabulary.

- CountVectorizer
  - Builds a sparse matrix of token counts from text documents. Supports
    customizable tokenization, preprocessing, and optional stop-word removal.

- TfidfVectorizer
  - Combines a CountVectorizer with a TF-IDF transformer to produce TF-IDF
    weighted features. Useful for text classification and information retrieval
    tasks.

- TfidfTransformer
  - Transforms a count matrix to a TF-IDF representation. Typically used in
    conjunction with CountVectorizer, or on top of any compatible count matrix.

Supporting utilities
- ENGLISH_STOP_WORDS
  - Built-in set of English stop words suitable for English-language text
    preprocessing.

- strip_accents_ascii
  - Remove accented characters by transliterating to ASCII where possible.

- strip_accents_unicode
  - Transform accented Unicode symbols to their closest non-accented form.

- strip_tags
  - Basic HTML/XML tag stripper using a regular expression (not intended as a
    full HTML parser).

Internal helpers (not part of the public API)
- _preprocess(doc, accent_function=None, lower=False)
  - Optional, chainable preprocessing steps for a document (lowercasing,
    accent handling, etc.).

- _analyze(doc, analyzer=None, tokenizer=None, ngrams=None, preprocessor=None,
  decoder=None, stop_words=None)
  - Chains together optional preprocessing, tokenization, and n-gram generation
    for a single document.

- _check_stop_list(stop)
  - Resolve a stop-word specification (e.g., "english", a custom list, or None)
    into a suitable stop-word collection.

Usage examples
--------------

Count-based vectorization
- from sklearn.feature_extraction.text import CountVectorizer
- documents = ["This is a sample document.", "Another document to vectorize."]
- vec = CountVectorizer(stop_words="english")
- X = vec.fit_transform(documents)

TF-IDF vectorization
- from sklearn.feature_extraction.text import TfidfVectorizer
- documents = ["This is a sample document.", "Another document to vectorize."]
- tfidf = TfidfVectorizer(stop_words="english", ngram_range=(1, 2))
- X = tfidf.fit_transform(documents)

Hashing-based vectorization (large-scale)
- from sklearn.feature_extraction.text import HashingVectorizer
- documents = ["This is a sample document.", "Another document to vectorize."]
- hv = HashingVectorizer(n_features=2**20)
- X = hv.transform(documents)

Notes
- Input handling supports strings, file-like objects, or filenames depending on
  the vectorizer configuration.
- Text normalization and tokenization behavior can be customized via preprocessor,
  tokenizer, analyzer, and n-gram settings.
- The output of vectorizers is typically a scipy.sparse matrix suitable for
  machine learning workflows.