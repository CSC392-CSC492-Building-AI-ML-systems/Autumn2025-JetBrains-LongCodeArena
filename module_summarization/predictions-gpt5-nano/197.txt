Naive Bayes classifiers
=======================

The Naive Bayes family of classifiers applies Bayesâ€™ theorem with strong (naive) feature independence assumptions. They are supervised learning methods that can be fast and effective for a variety of problems, especially with high-dimensional data.

Overview
--------
- Implemented estimators:
  - GaussianNB
  - MultinomialNB
  - BernoulliNB
  - ComplementNB
  - CategoricalNB
- All estimators inherit common prediction capabilities from the base class, including:
  - predict: classify samples
  - predict_proba: class probabilities
  - predict_log_proba: log-probabilities
  - predict_joint_log_proba: joint log-probabilities log P(x, y)
- Common API features:
  - fit and partial_fit for online updating
  - class priors and per-class statistics learned during training
  - strong independence assumptions between features given the class

Gaussian Naive Bayes (GaussianNB)
---------------------------------
- Use when features are continuous and can be modeled by Gaussian distributions per class.
- Class-conditional densities are Gaussian for each feature.
- Key learned parameters (attributes) typically include:
  - class_count_: number of training samples per class
  - class_prior_: prior probability of each class
  - theta_: per-class feature means
  - var_: per-class feature variances
  - epsilon_ (var_smoothing): added to variances for numerical stability
  - n_features_in_, feature_names_in_: feature metadata from fitting
- Online updates supported via partial_fit.

Multinomial Naive Bayes (MultinomialNB)
---------------------------------------
- Designed for discrete count features (e.g., word counts in text classification).
- Models P(x|y) with a multinomial distribution; uses smoothing to handle unseen features.
- Key learned parameters (attributes) typically include:
  - class_count_: per-class counts
  - class_log_prior_: log prior probabilities of classes
  - feature_count_ and feature_log_prob_: per-feature statistics and their log-probabilities
  - n_features_in_, feature_names_in_: feature metadata from fitting

Bernoulli Naive Bayes (BernoulliNB)
-----------------------------------
- Suitable for binary/boolean features or boolean-indicator features.
- Models P(x|y) assuming each feature is Bernoulli distributed given the class.
- Key learned parameters (attributes) typically include:
  - class_count_, class_log_prior_
  - feature_count_, feature_log_prob_ (log-probability of feature being 1 given the class)
  - n_features_in_, feature_names_in_

Complement Naive Bayes (ComplementNB)
-------------------------------------
- Aims to improve performance on imbalanced datasets.
- Uses complementary (opposite-class) statistics to compute class scores.
- Shares a similar interface with MultinomialNB and provides the same prediction methods.

Categorical Naive Bayes (CategoricalNB)
---------------------------------------
- Handles categorical features directly by modeling the probability of each category per class.
- Suitable when features take on a finite number of categories.
- Learns per-category statistics per class and provides the standard prediction API.

Common API and behavior
-----------------------
- All estimators implement:
  - fit(X, y, sample_weight=None)
  - partial_fit(X, y, classes=None)
  - predict(X)
  - predict_proba(X)
  - predict_log_proba(X)
  - predict_joint_log_proba(X)
- Input validation and internal checks are performed to ensure proper usage.
- Under the hood, predictions rely on log-probabilities for numerical stability, often combining priors with class-conditional likelihoods.

Notes
-----
- The priors (class probabilities) can be specified explicitly via priors; otherwise, they are estimated from data.
- Smoothing parameters (e.g., var_smoothing for GaussianNB, alpha for some implementations) help with numerical stability and unseen features.
- For more detailed guidance, refer to the user guide sections corresponding to each estimator.