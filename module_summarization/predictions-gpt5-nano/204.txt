Random Projection transformers

A simple and computationally efficient way to reduce the dimensionality of the data

Introduction
- Random Projection provides a lightweight approach to dimensionality reduction by projecting data onto a lower-dimensional subspace using a random matrix. This reduces computational costs and model size while controlling the trade-off with a small, bounded loss of information.

Key ideas
- Distortion control: The projection preserves pairwise distances with high probability. Distances between samples are preserved up to a small multiplicative factor (1 ± eps).
- Random projection matrices: The projection uses random matrices with entries drawn from simple distributions. Two common choices are dense Gaussian matrices and sparse matrices (e.g., Achlioptas-type sparsity). The distribution and density of the matrix are chosen to balance accuracy and speed.

The Johnson-Lindenstrauss lemma
- The theoretical foundation states that a small set of points in high-dimensional space can be embedded into a much lower-dimensional space while approximately preserving pairwise distances.
- The embedding can be achieved via a random projection, often effectively realized as an orthogonal projection or a matrix with roughly orthogonal rows.

Projection matrices
- Gaussian Random Projection: A dense matrix with entries drawn from N(0, 1/n_components). This yields an approximately distance-preserving embedding with good properties for many datasets.
- Sparse Random Projection (Achlioptas-style): A sparse matrix where most entries are zero and the non-zeros follow a simple distribution. This yields faster matrix multiplications and reduced memory usage, often with a small loss in precision.
- Density control: The density parameter regulates how many non-zero entries appear in the projection matrix. A default auto-density can be used, typically defined as 1 / sqrt(n_features).

Choosing the number of components
- The target dimensionality n_components determines the trade-off between accuracy and speed.
- A key result (the JL bound) relates n_components to the number of samples and the acceptable distortion eps:
  n_components ≥ 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3)
- The required dimensionality grows with the dataset size and the desired accuracy but is independent of the original feature count.

Estimating a safe number of components
- A helper function is available to compute a safe n_components given the number of samples and an allowed distortion eps. This can be computed for scalar or array-like inputs to obtain per-sample safe dimensions.

Usage and practical considerations
- Suitable for very high-dimensional data where exact dimensionality reduction methods (e.g., PCA) are too costly.
- Computationally efficient due to simple matrix multiplications and potential sparsity.
- The projection is randomized; results can vary between runs unless a fixed random state is used.

Examples
- Gaussian Random Projection
  from sklearn.random_projection import GaussianRandomProjection
  rp = GaussianRandomProjection(n_components=100, random_state=42)
  X_rp = rp.fit_transform(X)

- Sparse Random Projection
  from sklearn.random_projection import SparseRandomProjection
  rp = SparseRandomProjection(n_components=100, density='auto', random_state=42)
  X_rp = rp.fit_transform(X)

- Estimating a safe dimension
  from sklearn.random_projection import johnson_lindenstrauss_min_dim
  n_components = johnson_lindenstrauss_min_dim(n_samples=X.shape[0], eps=0.1)

Notes
- The density can be set explicitly or left as auto. Auto density yields a principled default based on the number of features.
- The approach is data-agnostic with respect to the feature distributions and is particularly advantageous when a fast, approximate reduction suffices.
- This method is complementary to, and often used before, other learning algorithms to improve scalability.

References
- Johnson-Lindenstrauss lemma
  https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma
- Dasgupta, Gupta, 1999, "An elementary proof of the Johnson-Lindenstrauss Lemma"
  https://citeseerx.ist.psu.edu/doc_view/pid/95cd464d27c25c9c8690b378b894d337cdf021f9

See Also
- johnson_lindenstrauss_min_dim(n_samples, eps) for computing a safe number of components.