Decision Trees
================

Overview
--------
A decision tree partitions data recursively by selecting a feature and a threshold that best separates the target variable. At each node, the algorithm evaluates potential splits and chooses the one that yields the largest decrease in impurity. The process continues until a stopping criterion is met (e.g., maximum depth, minimum samples, or minimum impurity improvement). Trees can be used for classification (discrete targets) or regression (continuous targets).

Impurity criteria interface
---------------------------
The core of the tree-building logic is the impurity criterion interface. A Criterion object encapsulates how to measure and manipulate impurity for a node and its potential children as a split is explored.

Key responsibilities
- Initialize statistics for a node using the target values, sample weights, and the subset of samples under consideration.
- Handle missing values gracefully by allocating and updating necessary statistics.
- Reset and reverse-reset impurity state when evaluating different split points.
- Update statistics incrementally as the split position moves (e.g., when samples move from the right to the left side of the split).
- Compute impurity for the current node (node_impurity) and for the left and right children (children_impurity).
- Produce the value of the node (node_value) which may be used for predictions in the node.
- Clip the node value when needed (clip_node_value).
- Provide a middle_value that helps with monotonicity constraints on splits (used by some criteria).
- Offer a fast proxy for impurity improvement (proxy_impurity_improvement) to accelerate the search for the best split.
- Compute the exact impurity improvement (impurity_improvement) given parent and child impurities.

Important concepts
- y: target values stored as a memory-friendly buffer.
- sample_weight: per-sample weights that influence impurity calculations.
- weighted_n_samples: total weight of samples considered at the node.
- sample_indices: indices of samples used in the current node; a portion may be devoted to handling missing values.
- start / end: bounds within sample_indices for the current node.
- EPSILON: a small constant used by certain impurity criteria (e.g., Poisson-based) to stabilize calculations.
- WeightedMedianCalculator: a helper used to compute weighted medians for splits when appropriate.

Specializations and extensibility
- The Criterion class is designed as an interface. Concrete impurity measures (e.g., Gini impurity for classification, entropy for probabilistic splits, or mean squared error for regression) are implemented in subclasses that override the abstract methods.
- Middle-value logic is provided to support monotonicity constraints in splits; concrete criteria implement this for their target task.
- The proxy impurity improvement serves to speed up the search by ranking split candidates with a computationally lighter metric before computing the exact impurity reduction.

Poisson criterion and numerical stability
- EPSILON is defined to aid numerical stability in Poisson-based or related impurity calculations within the criterion implementations.

Handling missing values
- Missing values are accounted for by a dedicated initialization path (init_missing). The design assumes that missing samples can be placed at the end of the sample_indices array, allowing the same update and impurity logic to cover both observed and missing data.

Performance considerations
- The impurity search uses a two-stage approach: a fast proxy measure to identify promising splits, followed by precise impurity improvement calculations on the top candidates.
- The incremental update model (updating statistics as the split position moves) enables efficient exploration of many potential split points without recomputing from scratch.

Typical impurity measures (used by concrete criteria)
- Classification: impurity measures such as Gini or entropy (information gain) are used to assess how well a split separates classes.
- Regression: impurity is typically based on variance reduction, such as mean squared error (MSE) or similar criteria.

Notes
- This documentation describes the interface and concepts behind the decision tree impurity criterion system. Concrete trees instantiate a Criterion subclass appropriate for the task (classification or regression) and rely on these methods to grow the tree efficiently and correctly.