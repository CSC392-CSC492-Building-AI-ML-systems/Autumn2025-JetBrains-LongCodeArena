Linear Mixed Effects Models
===========================

Overview
--------

Linear mixed effects models are regression models for dependent data that can estimate relationships involving both means and variances. They are also known as multilevel linear models or hierarchical linear models.

The MixedLM class
-----------------

The MixedLM class fits linear mixed effects models to data and provides support for common post-estimation tasks. It is a group-based implementation that is most efficient when the data can be partitioned into independent groups. Some models with crossed effects can be handled by specifying a model with a single group.

Model formulation
-----------------

The data are partitioned into disjoint groups. For group i, the probability model for the observed endog Y is

Y = X * beta + Z * gamma + epsilon

where

- n_i is the number of observations in group i
- Y is an n_i dimensional response vector (called endog in MixedLM)
- X is an n_i x k_fe design matrix for the fixed effects (exog in MixedLM)
- beta is a k_fe-dimensional vector of fixed effects parameters (fe_params in MixedLM)
- Z is a design matrix for the random effects with n_i rows (exog_re in MixedLM); the number of columns in Z can vary by group
- gamma is a random vector with mean 0. The covariance matrix for the first k_re elements of gamma (cov_re in MixedLM) is common to all groups. The remaining elements are variance components. Each group receives its own independent realization of gamma.
- epsilon is an n_i dimensional vector of iid normal errors with mean 0 and variance sigma^2; the epsilon values are independent both within and between groups

Notes on data, parameters, and estimation
-----------------------------------------

- Y, X, and Z (endog, exog, exog_re) must be observed. beta, cov_re, and sigma^2 are estimated using ML or REML; gamma and epsilon are random and define the probability model.

- The marginal mean is E[Y | X, Z] = X * beta. If only the mean structure is of interest, generalized estimating equations (GEE) offer an alternative.

Random effects and variance components
--------------------------------------

Two types of random effects are supported:

- Standard random effects: correlated with each other in arbitrary ways. Each group has the same number (k_re) of standard random effects, with the same joint distribution, but independent realizations across groups.

- Variance components: uncorrelated with each other and with the standard random effects. Each variance component has mean zero, with the same variance parameter across all realizations of a given variance component. The number of realized variance components per variance parameter can differ across groups.

Parameterizations
-----------------

Three parameterizations are used in different places. The fixed effects (fe_params) are identical in all three, but the variance parameters differ. The parameterizations are:

- User parameterization: cov(endog) = scale * I + Z * cov_re * Z'
  This is the main parameterization visible to the user.

- Profile parameterization: cov(endog) = I + Z * cov_re1 * Z'
  This is the parameterization of the profiled likelihood maximized to produce parameter estimates. The user cov_re equals cov_re1 scaled by the scale.

- Square root parameterization: operating with the Cholesky factor of cov_re1 instead of cov_re directly. This is hidden from the user.

These parameterizations can be packed into a single vector by concatenating fe_params with either the lower-triangular (or Cholesky) factor of the random effects covariance, followed by the variance parameters for the variance components. The packing may store the square roots when the random effects covariance is stored via its Cholesky factor. When unpacking, one must square or reflect the dependence structure depending on the parameterization used.

Score methods
--------------

Two score methods are implemented:

- Score with respect to the elements of the random effects covariance matrix (used for inference once the MLE is reached).

- Score with respect to the parameters of the Cholesky square root of the random effects covariance matrix (used for optimization).

Estimation and optimization
---------------------------

Numerical optimization uses GLS to avoid explicitly optimizing over the fixed effects parameters. The likelihood is profiled over both the scale parameter (a scalar) and the fixed effects parameters (if any). As a result, it is difficult and unnecessary to calculate the Hessian of the profiled log-likelihood, so Hessian-based optimization such as Newton-Raphson cannot be used for model fitting.

References
----------

- Lindstrom, MJ; Bates, DM (1988). "Newton Raphson and EM algorithms for linear mixed effects models for repeated measures data." Journal of the American Statistical Association, 83(404), 1014â€“1022.

- See also: Mixed effects implementation discussions and related materials:
  - http://econ.ucsb.edu/~doug/245a/Papers/Mixed%20Effects%20Implement.pdf
  - http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf
  - http://lme4.r-forge.r-project.org/slides/2009-07-07-Rennes/3Longitudinal-4.pdf

Notation glossary
-----------------

- cov_re: random effects covariance matrix (also referred to as Psi)
- scale: scalar error variance
- endog: Y (response)
- exog: X (fixed-effects design)
- exog_re: Z (random-effects design)
- fe_params: fixed effects parameters
- gamma: random effects vector
- vcomp: vector of variance parameters corresponding to variance components

Notes
-----

- All likelihood, gradient, and Hessian calculations follow Lindstrom and Bates (1988), adapted to support variance components.
- The MixedLM framework is designed for group-wise modeling but can accommodate some crossed structures via a single-group specification.
- For mean-structure-only inferences, alternative approaches such as GEE may be considered.