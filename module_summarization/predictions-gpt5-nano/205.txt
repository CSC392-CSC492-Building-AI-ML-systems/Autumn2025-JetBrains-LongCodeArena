Stochastic Gradient Descent (SGD)
===================================

Overview
--------
Stochastic Gradient Descent is an optimization method used to minimize differentiable loss functions
by iteratively updating model parameters using the gradient computed from a single example (or a
small mini-batch). At each iteration, the parameters are adjusted in the direction that reduces the
loss on the current sample, which often leads to faster convergence on large datasets compared to
batch gradient descent.

Key ideas
- Replace the full-batch gradient with a gradient computed from one (or a few) samples, which
  reduces per-iteration cost.
- Use a learning rate (step size) to control the update magnitude. A diminishing learning rate
  schedule often improves convergence and avoids oscillations.
- Combine SGD with appropriate loss functions to train classifiers or regressors.

Loss functions and targets
--------------------------
The SGD implementation described here supports a multinomial logistic loss (softmax cross-entropy)
for multiclass classification. For a single example with n_classes, the loss for the correct class y is

    loss = logsumexp(prediction) - prediction[y]    (optionally scaled by sample_weight)

where:
- prediction is the vector of raw scores (logits) for each class, typically the dot product of the
  input features with the weight matrix plus a bias term.
- logsumexp is a numerically stable function that computes log(sum(exp(prediction))).

Concretely, for a single sample, the gradient with respect to the logits is computed as

    grad[c] = exp(prediction[c] - logsumexp(prediction))  for all classes c
    if c == y, grad[y] is decremented by 1

and then each component is scaled by the sample weight. This gradient is then used to update the model
parameters (weights and bias) according to the chosen learning rate.

Numerical stability: logsumexp
-------------------------------
To avoid overflow/underflow when dealing with large logits, the logsumexp trick is used:
- Identify the maximum logit vmax in the vector.
- Compute sum_i exp(prediction[i] - vmax).
- Return log(sum_i exp(prediction[i] - vmax)) + vmax.

This keeps intermediate values in a safe numerical range while producing the correct result.

Type system and implementation notes
-----------------------------------
- The implementation uses fused types to support both 32-bit and 64-bit floating point data
  (e.g., float32 and float64). In the documented codebase, this is reflected by separate
  type specializations (e.g., double/np.float64 and float/np.float32).
- The multinomial loss and its gradient are implemented for both type variants to enable
  efficient, type-stable code paths.
- The core computations are performed with performance-oriented constructs (e.g., nogil),
  enabling faster SGD iterations.

Algorithm outline
---------------
- Initialize model parameters (weights and bias) depending on the feature dimension and
  the number of classes.
- For each training sample (or mini-batch):
  - Compute the prediction vector (logits) for the current sample.
  - Compute logsumexp(prediction) for numerical stability.
  - Compute the gradient with respect to the logits as described above.
  - Update model parameters using the learning rate:
    w <- w - eta_t * gradient
- Optionally apply regularization or a weight decay term as part of the update.
- Repeat for the desired number of epochs or until convergence criteria are met.

Convergence and practical tips
------------------------------
- Choose an appropriate learning rate schedule (e.g., eta_t = eta0 / (1 + decay * t)) to ensure
  convergence and to reduce oscillations as training progresses.
- Consider mini-batch SGD (small batch size) to reduce gradient variance and improve stability.
- Apply regularization (e.g., L2) to prevent overfitting and improve generalization.
- Normalize features to have similar scales to improve SGD performance.

Use cases
---------
- Multiclass classification with softmax cross-entropy loss.
- Large-scale datasets where full-batch optimization is impractical.
- Scenarios requiring fast, incremental updates as data arrives.

References
----------
- Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer. (Chapter 4.3.4)

See also
--------
- Softmax function and cross-entropy loss
- Log-sum-exp trick
- Learning rate schedules and regularization in SGD

Notes
-----
The material above reflects the SGD implementation supporting 32-bit and 64-bit floating point
types and a multinomial logistic loss with a numerically stable logsumexp computation.