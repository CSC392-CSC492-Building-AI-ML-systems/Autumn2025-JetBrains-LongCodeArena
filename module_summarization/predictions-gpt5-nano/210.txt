Empirical likelihood for descriptive statistics
===============================================

Overview
--------

This documentation describes the empirical likelihood (EL) machinery implemented for descriptive statistics. The module provides inference for descriptive statistics such as the mean, variance, skewness, kurtosis and correlation. When matplotlib is available, it can also produce multivariate confidence region plots and mean–variance contour plots.

The EL approach here builds a likelihood under moment (estimating equation) constraints. Observations are weighted with probabilities that maximize the empirical likelihood subject to the estimating equations. Lagrange multipliers are found by solving an optimization problem, typically via a modified Newton method.

References
----------

- Owen, A. (2001). Empirical Likelihood. Chapman and Hall.

API and Key Concepts
--------------------

DescStat(endog)
    Factory that returns an inference object for descriptive statistics using empirical likelihood.

    - If endog is one-dimensional, returns a univariate instance (DescStatUV).
    - If endog has more than one column, returns a multivariate instance (DescStatMV).

_OptFuncts (internal optimization utilities)
---------------------------------------------

A helper class that contains functions used during optimization of the empirical likelihood. Public users should treat this as an internal utility; the class and its methods are documented here for completeness.

- _log_star(self, eta, est_vect, weights, nobs)
    Transforms the log of observation probabilities in terms of the Lagrange multiplier (eta) to the log-star representation used in the optimization.

    Parameters
    - eta: Lagrange multiplier
    - est_vect: estimating equations vector (n x k)
    - weights: observation weights (nx1)
    - nobs: number of observations

    Returns
    - data_star: weighted log-star of the estimating equations

    Notes
    - This function is a placeholder for _fit_newton; its value is not directly used in optimization output, but it is used to form the objective.

- _hess(self, eta, est_vect, weights, nobs)
    Hessian of the weighted empirical likelihood with respect to eta.

    Returns
    - hess: (m x m) array, where m is the dimension of eta

- _grad(self, eta, est_vect, weights, nobs)
    Gradient of the weighted empirical likelihood with respect to eta.

    Returns
    - gradient: (m, 1) array

- _modif_newton(self, eta, est_vect, weights)
    Modified Newton method to maximize the log-star objective. Uses scipy’s _fit_newton.

    Parameters
    - eta: initial Lagrange multiplier (1 x m)
    - est_vect: estimating equations vector (n x k)
    - weights: observation weights (nx1)

    Returns
    - params: optimal eta that maximizes the log-likelihood

- _find_eta(self, eta)
    Placeholder for an additional optimization helper (implementation-specific).

Notes
-----

- The estimating equations vector est_vect and the observation weights drive the construction of the empirical likelihood.
- The optimization seeks the Lagrange multipliers that satisfy the estimating equations through the EL constraints.
- The implementation relies on numpy, scipy.optimize, and compatible optimization utilities from Statsmodels.

Dependencies
------------

- NumPy
- SciPy
- Statsmodels (for optimization helper _fit_newton)
- Optional: matplotlib (for plotting multivariate confidence regions and mean–variance contours)

See Also
--------

- DescStatUV / DescStatMV (public interfaces for univariate and multivariate descriptive statistics)
- Owen (2001) Empirical Likelihood (reference for theory)

Example usage (conceptual)
--------------------------

- Prepare a data array endog.
- Call DescStat(endog) to obtain an EL-based inference object.
- Use the returned object to perform hypothesis tests and construct confidence intervals for the desired descriptive statistics.

This documentation summarizes the public-facing behavior and the main internal optimization components of the empirical likelihood implementation for descriptive statistics.