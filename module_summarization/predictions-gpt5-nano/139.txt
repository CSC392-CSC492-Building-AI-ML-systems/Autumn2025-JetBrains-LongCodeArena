Controlling chunking
====================

Dask array is composed of many smaller blocks called chunks. The chunking determines parallelism, memory usage, and performance. Most operations are implemented to run blockwise over chunks, and some operations require inputs to have compatible chunking along the axes involved.

Specifying chunk sizes
----------------------

When creating a Dask array, you can specify the chunk sizes per axis with the chunks argument. This controls how the array is partitioned into blocks.

- Example: 2D array with square chunks
  - import dask.array as da
  - import numpy as np
  - x = da.from_array(np.arange(1000000).reshape(1000, 1000), chunks=(100, 100))

- Example: 3D or higher-dimensional arrays
  - y = da.from_array(np.zeros((100, 200, 300)), chunks=(10, 20, 30))

Rechunking
----------

You can change the chunking after creation to suit different workloads or memory constraints.

- Rechunk to a different per-axis layout
  - z = x.rechunk((200, 200))

- Use rechunk to align inputs for operations that require compatible chunking (such as concatenation, tensordot, or einsum)

Chunking and operations
-----------------------

- Most elementwise and reduction operations work blockwise and respect the existing chunk structure.
- Some operations require inputs to have compatible chunking along specific axes. When necessary, rechunking can be used to achieve alignment.
- The dispatch system (e.g., concatenate_lookup, tensordot_lookup, einsum_lookup, empty_lookup, divide_lookup, percentile_lookup, numel_lookup, nannumel_lookup, to_numpy_dispatch, to_cupy_dispatch) selects backend-specific implementations while preserving chunking semantics across backends (NumPy, CuPy, etc.).

Masked arrays and chunking
--------------------------

- For numpy.ma-based operations, Dask maps per-block computations using map_blocks or blockwise, preserving chunking across masked operations such as masked_equal, masked_invalid, masked_where, and related helpers.

Practical guidance
------------------

- Choose chunk sizes that balance parallelism with memory usage; common targets are in the range of a few MB per chunk, adjusted for available resources.
- Align chunking when combining arrays (e.g., concatenation, stacking) to minimize the need for rechunking.
- Use rechunk when changing workload characteristics (e.g., from many small blocks to fewer large blocks, or vice versa) to optimize performance.