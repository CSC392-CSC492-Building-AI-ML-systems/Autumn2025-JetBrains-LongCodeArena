Working with Text Data (Tutorial)

Overview
The sklearn.feature_extraction.text submodule provides utilities to convert text documents into numeric feature vectors. It supports several approaches:

- Count-based representations (CountVectorizer)
- TF–IDF representations (TfidfVectorizer, together with TfidfTransformer)
- Hash-based representations (HashingVectorizer)
- Text preprocessing helpers (strip_accents_unicode, strip_accents_ascii, strip_tags)
- Flexible tokenization and analysis via preprocessor, tokenizer, and n-gram configurations
- Stop word handling via ENGLISH_STOP_WORDS or custom lists

This tutorial demonstrates how to create features from text data, customize preprocessing, and build simple end-to-end pipelines.

Preprocessing and analysis
Text processing is composed of a sequence of optional steps that transform a document into tokens or n-grams:

- Preprocessor: a callable applied to the raw document before tokenization
- Tokenizer: a callable that splits text into tokens
- N-grams: a callable that builds n-grams from tokens
- Analyzer: a single callable that replaces the preprocessor/tokenizer/ngrams sequence

You can also apply a decoder when input is bytes, and you may choose to lower-case text and strip accents or HTML/XML tags.

Example usage:
- Lowercase and accent-stripping:
  - Using a simple preprocessor dictating the desired transformations
- Stripping HTML-like tags:
  - Using the strip_tags function to remove tags before tokenization

Code example (conceptual):
.. code-block:: python

    from sklearn.feature_extraction.text import CountVectorizer

    docs = [
        "This is the first document.",
        "This document is the second document.",
        "And this is the third one."
    ]

    vec = CountVectorizer(lowercase=True, strip_accents='unicode')
    X = vec.fit_transform(docs)
    print(X.shape)  # (3, number_of_features)

Vectorizers

CountVectorizer
- Produces a matrix of token counts
- Optional parameters:
  - analyzer: 'word' or 'character' (or a custom callable)
  - stop_words: list or string specifying a built-in stop list (e.g., 'english')
  - ngram_range: tuple (min_n, max_n) for word n-grams
  - strip_accents: {'ascii', 'unicode', None}
  - strip_tags: apply tag stripping to each document
- Common use: baseline text representation for classification, clustering, or retrieval

Code example:
.. code-block:: python

    from sklearn.feature_extraction.text import CountVectorizer

    docs = ["the brown fox", "the quick brown fox"]
    vec = CountVectorizer(stop_words='english', ngram_range=(1, 2))
    X = vec.fit_transform(docs)
    print(X.toarray())
    print(vec.get_feature_names_out())

TfidfVectorizer + TfidfTransformer
- TF–IDF weighting to downscale frequently occurring terms
- TfidfVectorizer combines tokenization, feature extraction, and TF–IDF weighting
- Alternatively, use TfidfTransformer on top of CountVectorizer
- Useful for text classification and information retrieval

Code example:
.. code-block:: python

    from sklearn.feature_extraction.text import TfidfVectorizer

    docs = ["this is a sample", "this is another example example"]
    tfidf = TfidfVectorizer(stop_words='english')
    X = tfidf.fit_transform(docs)
    print(X.shape)

HashingVectorizer
- Uses the hashing trick to map tokens to a fixed-dimensional feature space
- No vocabulary is built; memory usage is predictable and scalable
- Useful for very large corpora or streaming data

Code example:
.. code-block:: python

    from sklearn.feature_extraction.text import HashingVectorizer

    docs = ["sample document", "another document"]
    hv = HashingVectorizer(n_features=2**20, alternate_sign=False)
    X = hv.fit_transform(docs)
    print(X.shape)

Stop words
- ENGLISH_STOP_WORDS provides a built-in list of common English words
- You can supply stop_words as a parameter to vectorizers
- You can also pass a custom iterable to _check_stop_list (used internally)

Code example:
.. code-block:: python

    from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS

    docs = ["this is a sample", "this is another example"]
    vec = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)
    X = vec.fit_transform(docs)
    print(vec.get_feature_names_out())

Accent stripping and tag stripping helpers
- strip_accents_unicode: convert accented characters to non-accented equivalents (slow but
  unicode-aware)
- strip_accents_ascii: transliterate to ASCII where possible
- strip_tags: remove HTML/XML tags via a regex-based stripper

Code example:
.. code-block:: python

    from sklearn.feature_extraction.text import strip_accents_unicode, strip_accents_ascii, strip_tags

    s = "Café naïve example <b>bold</b>"
    print(strip_accents_unicode(s))
    print(strip_accents_ascii(s))
    print(strip_tags(s))

End-to-end tutorial: simple text classification workflow
- Step 1: Prepare a small corpus of text documents
- Step 2: Choose a vectorizer (CountVectorizer or TfidfVectorizer)
- Step 3: Create a train/test split (optional)
- Step 4: Train a classifier on the vectorized features
- Step 5: Evaluate and iterate on preprocessing and model choice

Code example:
.. code-block:: python

    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import LogisticRegression
    from sklearn.pipeline import Pipeline
    from sklearn.metrics import accuracy_score

    docs = [
        "I love this movie, it is fantastic",
        "This film was terrible and boring",
        "What a great performance",
        "I did not like this movie"
    ]
    labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative

    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(stop_words='english')),
        ('clf', LogisticRegression(max_iter=1000))
    ])

    pipeline.fit(docs, labels)
    preds = pipeline.predict(["a wonderful movie"])
    print("Prediction:", preds[0])

Public API (highlights)
- CountVectorizer: converts text to a matrix of token counts
- TfidfVectorizer: similar to CountVectorizer but with TF–IDF weighting
- HashingVectorizer: fixed-size hashing-based representation
- ENGLISH_STOP_WORDS: built-in stop word set
- strip_accents_unicode, strip_accents_ascii, strip_tags: text preprocessing helpers

Notes
- Tokenization, normalization, and n-gram generation are configurable via the analyzer chain.
- For large-scale text data, HashingVectorizer offers memory efficiency without a vocabulary.

See also
- The _preprocess and _analyze helpers for chaining preprocessing steps
- ENGLISH_STOP_WORDS for stop word filtering
- The underlying vectorizer classes in the sklearn.feature_extraction.text module

This documentation provides a concise, practical guide to generating and working with text features using scikit-learn’s text feature extraction utilities.