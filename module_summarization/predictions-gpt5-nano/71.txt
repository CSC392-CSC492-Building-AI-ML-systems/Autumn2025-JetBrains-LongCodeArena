Kernel Ridge regression
=======================

Module
------
sklearn.kernel_ridge

Overview
--------
Kernel ridge regression (KRR) combines ridge regression (linear least-squares with L2 regularization) with the kernel trick. It learns a linear function in the space induced by the selected kernel. For non-linear kernels, this corresponds to a non-linear function in the original input space.

The form of the model learned by KRR is identical to Support Vector Regression (SVR). However, KRR uses squared error loss with L2 regularization and admits a closed-form solution, which is typically faster for medium-sized datasets. The learned model is non-sparse, unlike SVR (which can produce a sparse model when epsilon > 0). This estimator supports multi-target regression (i.e., y with shape [n_samples, n_targets]).

Read more in the User Guide kernel_ridge.

Class
-----
KernelRidge
    MultiOutputMixin, RegressorMixin, BaseEstimator

Description
-----------
Kernel ridge regression learns a function in a reproducing kernel Hilbert space defined by the chosen kernel. The modelâ€™s dual representation is expressed in terms of kernel evaluations between training samples. This class exposes a simple interface analogous to linear Ridge regression, but enables nonlinear relationships through kernelization.

Parameters
----------
alpha : float or array-like of shape (n_targets,), default=1.0
    Regularization strength; must be a positive float. Regularization improves conditioning and reduces variance. If an array is provided, penalties are assumed to be target-specific. The number of elements must match the number of targets.

kernel : str or callable, default="linear"
    Kernel mapping used internally. Passed directly to pairwise_kernels. If a string, it must be one of the kernels in pairwise.PAIRWISE_KERNEL_FUNCTIONS or "precomputed". If "precomputed", X is treated as a precomputed kernel matrix. If a callable, it must be a function that takes two 1-D arrays (samples) and returns a scalar kernel value. Callables from sklearn.metrics.pairwise are not allowed since they operate on matrices.

gamma : float, default=None
    Gamma parameter for the RBF, laplacian, polynomial, exponential chi2, and sigmoid kernels. Its interpretation depends on the kernel. Ignored for other kernels.

degree : int, default=3
    Degree of the polynomial kernel. Ignored by other kernels.

coef0 : float, default=1
    Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.

kernel_params : dict, default=None
    Additional parameters to pass to the kernel function.

Attributes
----------
dual_coef_ : ndarray of shape (n_samples,) or (n_samples, n_targets)
    Representation of the weight vector(s) in kernel space.

X_fit_ : {ndarray, sparse matrix} of shape (n_samples, n_features)
    Training data used for fitting. If kernel == "precomputed", this stores the training kernel matrix of shape (n_samples, n_samples).

n_features_in_ : int
    Number of features observed during fit.

feature_names_in_ : ndarray of shape (n_features_in_,)
    Names of features observed during fit. Defined only when X has feature names that are all strings.

    versionadded:: 1.0

    See Also
    --------
    sklearn.gaussian_process.GaussianProcessRegressor : Gaussian Process Regressor with automatic kernel hyperparameter tuning and uncertainty estimates.
    sklearn.linear_model.Ridge : Linear ridge regression.
    sklearn.linear_model.RidgeCV : Ridge regression with built-in cross-validation.
    sklearn.svm.SVR : Support Vector Regression with a variety of kernels.

Notes
-----
- Read-only attributes after fitting: dual_coef_, X_fit_, n_features_in_, feature_names_in_.
- The unit of regularization is controlled by alpha; larger values imply stronger regularization.

Methods
-------
__init__(self, alpha=1, *, kernel="linear", gamma=None, degree=3, coef0=1, kernel_params=None)
    Create a KernelRidge instance with the specified hyperparameters.

_get_kernel(self, X, Y=None)
    Compute the kernel matrix between X and Y. If kernel is callable, use kernel_params to configure it; otherwise use the standard kernel parameters (gamma, degree, coef0).

Examples
--------
Basic usage with a linear kernel:

.. code-block:: python

    from sklearn.kernel_ridge import KernelRidge
    import numpy as np

    n_samples, n_features = 10, 5
    rng = np.random.RandomState(0)
    y = rng.randn(n_samples)
    X = rng.randn(n_samples, n_features)

    krr = KernelRidge(alpha=1.0, kernel="linear")
    krr.fit(X, y)
    preds = krr.predict(X)

Kernel with RBF kernel:

.. code-block:: python

    krr_rbf = KernelRidge(alpha=1.0, kernel="rbf", gamma=0.1)
    krr_rbf.fit(X, y)
    preds_rbf = krr_rbf.predict(X)

See Also
--------
Ridge : Linear ridge regression.
RidgeCV : Ridge regression with built-in cross-validation.
SVR : Support Vector Regression with various kernels.
GaussianProcessRegressor : Gaussian process regression with kernel choices.

References
----------
Kevin P. Murphy, Machine Learning: A Probabilistic Perspective, The MIT Press, ch. 14.4.3.

Notes
-----
- The kernel parameter controls the feature space in which linear ridge regression is applied.
- For precomputed kernels, the input X to fit/predict should be a kernel matrix.
- This estimator supports multi-target regression.