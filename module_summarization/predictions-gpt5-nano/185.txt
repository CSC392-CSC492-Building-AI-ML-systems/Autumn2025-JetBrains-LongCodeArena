Grid Search: Tuning the hyper-parameters of an estimator

Grid search performs an exhaustive search over a specified parameter grid to find the best combination of hyper-parameters for an estimator. It uses cross-validation to evaluate each candidate and can refit the best estimator on the full dataset.

Overview
- Purpose: Find the best hyper-parameters for an estimator by evaluating all combinations in a parameter grid.
- How it works: For each combination of parameters, GridSearchCV trains the estimator using cross-validation and records a scoring metric (by default, the estimator’s default scoring). The best combination is selected and, if requested, the estimator is refit on the full training data using those parameters.
- Typical use cases: Tuning models like SVMs, random forests, gradient boosting, or any estimator that exposes get_params / set_params for parameter tuning.

Key concepts
- estimator: An object implementing fit, predict, and get_params/set_params.
- param_grid: A dict (or list of dicts) mapping parameter names to sequences of values to test.
- scoring: A string or callable that defines the scoring metric. If None, the estimator’s default scoring is used.
- cv: Cross-validation splitting strategy (integer for k-fold, or a CV splitter object).
- refit: When True, or a string naming a scoring metric, GridSearchCV refits the best parameters on the full dataset.
- n_jobs: Number of jobs to run in parallel. -1 means using all processors.
- verbose: Controls the verbosity of the output during fitting.
- return_train_score: If True, train scores are included in cv_results_.

Parameters
- estimator: The base estimator to optimize.
- param_grid: dict or list of dicts. Each dict specifies parameter names and the values to try.
- scoring: String or callable, or None. Optional. If multiple metrics are provided (via a dict), a single metric is used for selecting the best model unless refit is a string naming one of them.
- n_jobs: int, default=None. Number of jobs to run in parallel.
- cv: int, cross-validation generator, or an iterable/iterator. Default is 5-fold cross-validation.
- refit: bool, string, or None. If True, the best parameters are used to refit on the full data. If a string, it names the scoring metric to use for selecting the best estimator to refit.
- verbose: int, default=0. Control the verbosity of the fitting process.
- return_train_score: bool, default=False. If True, train scores are available in cv_results_.

Parameter grid format
- The param_grid should be a dict (or a list of dicts) where keys are parameter names (as they appear in the estimator’s get_params/set_params) and values are lists/arrays of values to try.
- You can test multiple disjoint grids by passing a list of dictionaries to param_grid.

Example
.. code-block:: python

   from sklearn.model_selection import GridSearchCV
   from sklearn.svm import SVC
   from sklearn.datasets import load_iris

   X, y = load_iris(return_X_y=True)

   param_grid = {
       'C': [0.1, 1, 10],
       'gamma': ['scale', 'auto'],
       'kernel': ['rbf', 'linear']
   }

   grid_search = GridSearchCV(
       estimator=SVC(),
       param_grid=param_grid,
       cv=5,
       scoring='accuracy',
       verbose=2,
       n_jobs=-1
   )

   grid_search.fit(X, y)

   print("Best parameters:", grid_search.best_params_)
   print("Best score:", grid_search.best_score_)
   print("Best estimator:", grid_search.best_estimator_)

Output and results
- best_params_: Parameter values of the best found combination.
- best_score_: Mean cross-validated score of the best combination.
- best_estimator_: Estimator refit on the full training data with the best parameters (if refit is True).
- cv_results_: Dict with detailed results for all parameter combinations, including:
  - mean_test_score, std_test_score
  - mean_fit_time, std_fit_time
  - mean_score_time, std_score_time
  - params: parameter settings for each candidate

Notes
- The exhaustive nature of grid search can be computationally expensive. For large parameter spaces, consider:
  - Reducing the grid size or using a coarser grid.
  - Using RandomizedSearchCV to sample parameter settings randomly.
  - Using more efficient search strategies (e.g., successive halving variants where available).
- Ensure your estimator supports get_params and set_params for GridSearchCV to modify hyper-parameters.

See Also
- RandomizedSearchCV: sampling parameter settings from distributions rather than exhaustively testing a grid.
- HalvingGridSearchCV: progressive resource allocation to fewer candidates (where available).
- Cross-validation docs: guidelines on choosing cv strategies.
- Scoring docs: details on how scoring metrics are specified and used.