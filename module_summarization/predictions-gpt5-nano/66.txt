Generate Tutorial: Pipelines in scikit-learn
=============================================

Overview
--------
The Pipeline utility in scikit-learn lets you chain together multiple data
transforms with a final estimator to form a single composite estimator.
Intermediate steps must implement fit and transform; the final estimator only
needs to implement fit. Pipelines can be configured and tuned as a unit, with
parameters addressed using a double-underscore (``'__'``) syntax.

What is a Pipeline?
-------------------
- A pipeline is a sequence of steps: each step is a (name, transform) pair,
  where transformers implement fit/transform and the final step is an estimator
  that implements fit.
- Parameters can be set for any step by prefixing the parameter name with the
  step’s name, separated by ``'__'``.
- Pipelines support memory caching of intermediate results to speed up repeated
  fits.

Basic usage
-----------
Example: chain a standard scaler with a classifier.

.. code-block:: python

   from sklearn.pipeline import Pipeline
   from sklearn.preprocessing import StandardScaler
   from sklearn.svm import SVC

   pipe = Pipeline([
       ('scaler', StandardScaler()),
       ('svc', SVC())
   ])

   # Fit and evaluate as a single estimator
   pipe.fit(X_train, y_train)
   score = pipe.score(X_test, y_test)

Parameter setting
-----------------
You can tune parameters for any step using the ``__`` syntax:

.. code-block:: python

   pipe.set_params(svc__C=10)

Caching with memory
-------------------
To speed up expensive transformations, you can enable caching of intermediate
steps (the last step is never cached). Provide a path to a caching directory or a
memory object.

.. code-block:: python

   pipe = Pipeline([...], memory='/tmp/cache_dir')

Accessing steps
---------------
You can inspect or modify individual steps via the pipeline’s attributes:

.. code-block:: python

   first_step = pipe.named_steps['scaler']
   # or access using pipe.steps to inspect all steps

Output configuration
--------------------
You can configure how outputs are handled for transform and fit_transform
via ``set_output``. This propagates the chosen output format to all steps.

See also
--------
- make_pipeline: Convenience function for quick pipeline construction.
- FeatureUnion / make_union: Parallel feature extraction and combination utilities.
- The Pipeline class exposes attributes such as:
  - named_steps: access to each step by name
  - classes_, n_features_in_, feature_names_in_: populated after fitting (where available)

Examples
--------
Grid search with pipelines is a common pattern, enabling joint optimization of
preprocessing and model parameters. See the user guide’s pipeline section for a
hands-on example.

Notes
-----
- Intermediate steps must be transformers (fit and transform); the final step is an estimator.
- Parameter names follow the ``step__param`` convention to set parameters for
  specific steps.

API reference (highlights)
--------------------------
- Pipeline(steps, memory=None, verbose=False): construct a pipeline.
- set_output(transform=None): configure transform output across all steps.
- get_params(deep=True): retrieve parameters for the pipeline and contained steps.
- named_steps: dictionary-like access to step objects by name.

This document provides a practical guide to building and using pipelines to
streamline preprocessing, feature extraction, and modeling in a single,
reusable workflow.