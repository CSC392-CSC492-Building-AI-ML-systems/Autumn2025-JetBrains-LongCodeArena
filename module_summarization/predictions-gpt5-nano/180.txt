Cross-validation: evaluating estimator performance
=============================================

Cross-validation is a resampling procedure used to evaluate how well an estimator will generalize to an independent dataset. It involves splitting the data into training and test sets multiple times, training the model on the training portion, and evaluating it on the held-out test portion. The results are aggregated to provide a more reliable estimate of the model’s generalization performance, and to aid in model selection and hyperparameter tuning.

Key ideas
- Partition data into train/test folds and rotate the test fold across repetitions.
- Use aggregated scores (e.g., mean and standard deviation) to summarize performance.
- Helps detect overfitting and provides a more robust estimate of generalization error than a single train/test split.

Common strategies
- KFold: Split the data into k consecutive folds. Each fold is used once as the test set.
- StratifiedKFold: Like KFold, but preserves class distribution in each fold (recommended for classification).
- LeaveOneOut (LOO): Each sample forms a test set once; k equals the number of samples.
- ShuffleSplit: Random train/test splits are generated by shuffling data; can be used with any k.
- TimeSeriesSplit: Sequential train/test splits appropriate for time-series data.

Scoring
- The scoring parameter controls which metric is optimized. Examples include:
  - Classification: 'accuracy', 'f1', 'precision', 'recall', 'roc_auc'
  - Regression: 'r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'
- Scorers can be strings, callables, or None (use the estimator’s score method by default).
- Negative metrics (e.g., 'neg_mean_squared_error') are returned so that higher is better.

Using cross_val_score
- Returns scores for each fold (array-like). Use the mean and spread to summarize performance.

Example
- Classification with StratifiedKFold and accuracy:
 1) 
   from sklearn.model_selection import cross_val_score, StratifiedKFold
   from sklearn.datasets import load_iris
   from sklearn.svm import SVC

   X, y = load_iris(return_X_y=True)
   clf = SVC(kernel='linear', C=1)

   cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)
   scores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy')
   print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

Using cross_validate
- Returns a dict with test scores and optional train scores, fit/score times.

Example
- Getting train and test scores:
 2)
   from sklearn.model_selection import cross_validate
   scores = cross_validate(clf, X, y, cv=cv, scoring='accuracy', return_train_score=True)
   print("Train accuracy: %0.3f, Test accuracy: %0.3f" % (scores['train_score'].mean(), scores['test_score'].mean()))

Validation curves
- Validation curves evaluate how a model’s performance varies with a hyperparameter.
- Use validation_curve to compute training and test scores across different values of a parameter.

Example
- Varying C for an SVM:
 3)
   from sklearn.model_selection import validation_curve
   import numpy as np
   param_range = np.logspace(-3, 2, 6)
   train_scores, test_scores = validation_curve(
       clf, X, y, param_name="C", param_range=param_range, cv=5, scoring="accuracy"
   )

Learning curves (related)
- Learning curves study how performance improves with the size of the training set.
- Use learning_curve to obtain training and test scores for different training set sizes.

See also
- LearningCurveDisplay and ValidationCurveDisplay for visualizations.
- cross_val_predict for obtaining cross-validated predictions.
- The individual estimators and scoring functions used in cross-validation (KFold/StratifiedKFold, TimeSeriesSplit, make_scorer, etc.).

Notes
- For classification, prefer StratifiedKFold to maintain class distribution across folds.
- The default scoring uses the estimator’s score method; provide a scoring metric for more control or when comparing different metrics.

See also
- cross_val_score, cross_validate
- validation_curve, learning_curve
- StratifiedKFold, KFold, TimeSeriesSplit, ShuffleSplit
- make_scorer (for custom metrics)