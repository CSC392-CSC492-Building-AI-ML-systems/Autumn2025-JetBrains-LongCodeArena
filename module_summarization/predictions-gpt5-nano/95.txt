Stateless Learning Rate Schedulers
===================================

This module provides stateless learning rate schedulers that interface directly with Composer's Time abstraction. Schedulers operate with arbitrary, explicit time units and return a multiplier α to apply to the optimizer’s base learning rate (η). The effective learning rate at time t is η(t) = η_i × α(t), where η_i is the optimizer’s initial learning rate. When multiple schedulers are used, their effects stack multiplicatively. Schedulers support a scale schedule ratio (SSR), enabling consistent behavior under time scaling.

.. automodule:: composer.schedulers
   :members:
   :undoc-members:
   :show-inheritance:

Overview
--------

- Stateless schedulers implement the ComposerScheduler specification (a protocol) and can be provided as simple functions or callable classes.
- They are designed to work with Composer’s Time abstraction (Time, TimeUnit) to express progress in terms of epochs, batches, or other units.
- The module includes a number of concrete schedulers (listed below) and utilities for converting times and wrapping schedulers for use with PyTorch’s LR schedulers.

ComposerScheduler protocol
--------------------------

The primary interface for stateless schedulers. A scheduler is a pure function that, given the current trainer state and an optional scale schedule ratio (SSR), returns a multiplier α ∈ (0, ∞). By convention, α is applied to the learning rate, not the learning rate itself.

Examples are provided in the in-code documentation:

.. code-block:: python

    def ten_epoch_decay_scheduler(state: State) -> float:
        if state.timestamp.epoch < 10:
            return 1.0
        return 0.5

    # ten_epoch_decay_scheduler is a valid ComposerScheduler
    trainer = Trainer(
        schedulers=[ten_epoch_decay_scheduler],
       ...
    )

    class VariableEpochDecayScheduler(ComposerScheduler):
        def __init__(self, num_epochs: int):
            self.num_epochs = num_epochs

        def __call__(self, state: State) -> float:
            if state.time.epoch < self.num_epochs:
                return 1.0
            return 0.5

    ten_epoch_decay_scheduler = VariableEpochDecayScheduler(num_epochs=10)

    trainer = Trainer(
        schedulers=[ten_epoch_decay_scheduler],
       ...
    )

Notes
- Schedulers output a multiplier, not a learning rate directly.
- Schedulers can be stacked multiplicatively.
- The SSR applies a time-scaling to the scheduler output.

Time conversion utilities
-------------------------

-_convert_time(time, state, ssr=1.0)_ converts a specified time (either a Time object or a string) into a concrete Time value in an appropriate unit, taking SSR into account.

Key behavior:
- If time is a string, it is parsed via Time.from_timestring.
- A RuntimeError is raised if state.max_duration is not set.
- If time.unit is DURATION:
  - If max_duration.unit is EPOCH, requires state.dataloader_len and converts to BATCH units.
  - Otherwise, scales by max_duration.value and returns in max_duration.unit.
- If time.unit is EPOCH:
  - Converts to BATCH using state.dataloader_len, then applies SSR.
- The result is a Time with value=int(time.value * ssr) and the corresponding unit.

Wrapping stateless schedulers for PyTorch
-----------------------------------------

- _compile_composer_scheduler(scheduler, state, ssr=1.0)_ converts a stateless scheduler into a PyTorch-compatible scheduler object.
- The resulting scheduler mirrors .step-like behavior and binds a reference to the current State, so internal state updates from .step() can be ignored if not needed.
- This enables integration with PyTorch LR schedulers while keeping the stateless design.

Available schedulers
--------------------

The module exposes a set of concrete stateless schedulers (as indicated in __all__):

- StepScheduler
- MultiStepScheduler
- ConstantScheduler
- LinearScheduler
- ExponentialScheduler
- CosineAnnealingScheduler
- CosineAnnealingWarmRestartsScheduler
- PolynomialScheduler
- MultiStepWithWarmupScheduler
- ConstantWithWarmupScheduler
- LinearWithWarmupScheduler
- CosineAnnealingWithWarmupScheduler
- PolynomialWithWarmupScheduler

All of these are stateless and operate by returning a multiplier α to apply to the optimizer’s base LR, using Composer’s Time abstraction and the SSR concept. Warmup variants provide a staged ramp-up period before applying their primary schedule.

Usage notes
-----------

- Schedulers can be composed; their effects multiply.
- Time units are flexible (epochs, batches, or custom durations) as long as they can be expressed via the Time abstraction.
- SSR enables consistent scheduling behavior when the overall training duration is scaled.
- The internal utilities (_convert_time, compile_composer_scheduler) support converting various time specifications and wrapping schedulers for PyTorch compatibility.

Example
-------

The following demonstrates a simple usage pattern described in the in-code documentation:

.. code-block:: python

    def ten_epoch_decay_scheduler(state: State) -> float:
        if state.timestamp.epoch < 10:
            return 1.0
        return 0.5

    trainer = Trainer(
        schedulers=[ten_epoch_decay_scheduler],
        ...
    )

See Also
--------

- Time, TimeUnit, and State classes in composer.core for understanding available fields.
- PyTorch LR schedulers for integration via compile_composer_scheduler.