# Scale Schedule

Stateless learning rate schedulers that integrate with Composer’s time abstraction and support a scalable time model via a scale schedule ratio (SSR).

Overview
- Stateless schedulers implement a function that returns a multiplier alpha to apply to the optimizer’s learning rate.
- Schedulers operate on a time abstraction provided by Composer (State, Time, TimeUnit) rather than directly on PyTorch’s scheduling primitives.
- The scale schedule ratio (SSR) enables “stretching” or “compressing” the schedule in time: alpha_sigma(t) = alpha(t / sigma).
- Multiple schedulers can be composed by multiplying their multipliers.

Key concepts
- ComposerScheduler: A Protocol describing the required interface. A scheduler is valid if it implements:
  - __call__(state: State, ssr: float = 1.0) -> float
- Time and State:
  - State contains progression information (e.g., timestamp, max_duration, dataloader_len) used by schedulers to determine the current multiplier.
  - Time and TimeUnit describe units of schedule progression (e.g., epoch, batch, duration).
- Output semantics:
  - Schedulers output a multiplier alpha, not a direct learning rate.
  - The effective learning rate is eta(t) = eta_initial * alpha(t).
  - If multiple schedulers are used, their effects are applied multiplicatively.
- SSR (scale schedule ratio):
  - The scheduler’s time axis can be scaled by ssr, so that alpha_sigma(t) = alpha(t / sigma).

Public API (high level)
- ComposerScheduler (Protocol)
  - An interface for stateless schedulers. Implementations may be plain functions or callable classes.
- _convert_time(time, state, ssr=1.0) -> Time[int]
  - Converts a time specification (string or Time) into an absolute Time value, accounting for the current State and SSR.
  - Handles conversions between DURATION, EPOCH, and BATCH units with necessary state information (e.g., max_duration, dataloader_len).
- compile_composer_scheduler(scheduler: ComposerScheduler, state: State, ssr: float = 1.0) -> PyTorchScheduler
  - Converts a stateless ComposerScheduler into a PyTorch scheduler compatible with PyTorch’s .step() interface.
  - The resulting scheduler is bound to the current State, allowing schedulers to react to trainer progression while remaining stateless.
- Concrete schedulers (examples of the kinds available):
  - StepScheduler
  - MultiStepScheduler
  - ConstantScheduler
  - LinearScheduler
  - ExponentialScheduler
  - CosineAnnealingScheduler
  - CosineAnnealingWarmRestartsScheduler
  - PolynomialScheduler
  - MultiStepWithWarmupScheduler
  - ConstantWithWarmupScheduler
  - LinearWithWarmupScheduler
  - CosineAnnealingWithWarmupScheduler
  - PolynomialWithWarmupScheduler

Usage notes and examples
- Basic stateless scheduler (illustrative example from the documentation):
  - A scheduler that halves the learning rate after 10 epochs:
    def ten_epoch_decay_scheduler(state: State) -> float:
        if state.timestamp.epoch < 10:
            return 1.0
        return 0.5
  - Use in a trainer:
    trainer = Trainer(schedulers=[ten_epoch_decay_scheduler], ...)
- Callable class example (also valid as a ComposerScheduler):
    class VariableEpochDecayScheduler:
        def __init__(self, num_epochs: int):
            self.num_epochs = num_epochs
        def __call__(self, state: State) -> float:
            if state.time.epoch < self.num_epochs:
                return 1.0
            return 0.5
- SSR usage:
  - If you have multiple schedulers, their multipliers multiply together.
  - The ssr parameter scales the effective time axis when computing the multiplier.

Notes on time conversion
- The internal _convert_time helper supports:
  - time as a string (e.g., "10ep" or "2.0dur"), or as a Time object.
  - Conversion between DURATION, EPOCH, and BATCH units based on state.max_duration and state.dataloader_len.
  - SSR scaling by adjusting the Time value accordingly.
- Certain conversions require state.dataloader_len to be known; otherwise a runtime error is raised.

Reference example (from the module docstring)
- Function-based scheduler:
  def ten_epoch_decay_scheduler(state: State) -> float:
      if state.timestamp.epoch < 10:
          return 1.0
      return 0.5
  trainer = Trainer(schedulers=[ten_epoch_decay_scheduler], ...)
- Callable-class-based scheduler:
  class VariableEpochDecayScheduler(ComposerScheduler):
      def __init__(num_epochs: int):
          self.num_epochs = num_epochs
      def __call__(self, state: State) -> float:
          if state.time.epoch < self.num_epochs:
              return 1.0
          return 0.5
  ten_epoch_decay_scheduler = VariableEpochDecayScheduler(num_epochs=10)
  trainer = Trainer(schedulers=[ten_epoch_decay_scheduler], ...)

Reference implementation points
- Stateless design: emphasis on explicit time units and SSR to maximize configurability.
- Integration path: compile_composer_scheduler wraps a stateless scheduler into a PyTorch-compatible scheduler (LambdaLR-like) while retaining access to the trainer State for dynamic behavior.
- Documentation scope: covers both functional and class-based schedulers and their interactions with time, units, and SSR.

This module provides the building blocks for scalable, time-aware learning rate scheduling in Composer, enabling flexible, unit-agnostic configuration of learning rate schedules and straightforward composition of multiple schedulers.