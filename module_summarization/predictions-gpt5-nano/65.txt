Calibration tutorial: Probability calibration with CalibratedClassifierCV

Introduction
Probability calibration aims to transform the predicted scores of a classifier into well-calibrated probabilities. Calibrated probabilities reflect true likelihoods, which is important for decision-making, ranking, and uncertainty estimation. Sklearn provides CalibratedClassifierCV to calibrate probabilistic outputs using cross-validation, with two calibration methods: sigmoid (Platt scaling) and isotonic regression.

Methods for calibration
- Sigmoid (Platt scaling): Fits a logistic regression model to the classifier’s scores. Suitable for well-behaved, monotonic relations between scores and probabilities.
- Isotonic regression: A non-parametric calibration method that can capture non-monotonicities but may overfit with limited data. Recommended only when calibration data are plentiful (typically ≥ 1000 samples).

Key features
- Cross-validated calibration: For each cross-validation split, a copy of the base estimator is trained on training data and calibrated on validation data. Predictions can be averaged across folds (ensemble=True) or computed from a single calibration (ensemble=False).
- Prefit support: If cv="prefit", the estimator is already fitted and all data are used for calibration.
- Calibration uses the estimator’s decision_function if available; otherwise it uses predict_proba.
- Base estimator compatibility: Works with classifiers that expose a probability or decision function.

Parameters (highlights)
- estimator: The base classifier to calibrate (default is a LinearSVC-like estimator in some contexts).
- method: 'sigmoid' or 'isotonic'.
- cv: Cross-validation strategy. None implies default 5-fold; "prefit" uses provided fitted estimator.
- n_jobs: Parallelism for fitting across CV folds.
- ensemble: If True, averages probabilities across CV-calibrated classifiers; if False, uses cross_val_predict to obtain unbiased probabilities for calibration and trains the final base estimator on all data.
- prefit: If True (cv="prefit"), skip cross-validation and calibrate on all data.

Usage example
This example demonstrates calibrating a linear SVM using sigmoid calibration with 5-fold cross-validation, then evaluating the calibrated probabilities.

.. code-block:: python

    import numpy as np
    from sklearn.datasets import make_classification
    from sklearn.svm import LinearSVC
    from sklearn.calibration import CalibratedClassifierCV
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import brier_score_loss
    from sklearn.calibration import calibration_curve

    # Generate synthetic binary classification data
    X, y = make_classification(n_samples=2000, n_features=20, n_classes=2,
                               n_informative=2, random_state=42)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Base estimator
    base_clf = LinearSVC()

    # Calibrate probabilities with sigmoid (Platt scaling)
    calibrated = CalibratedClassifierCV(base_clf, method='sigmoid', cv=5)

    # Fit calibration model
    calibrated.fit(X_train, y_train)

    # Predict calibrated probabilities for the positive class
    prob_pos = calibrated.predict_proba(X_test)[:, 1]

    # Evaluate calibration with Brier score
    from sklearn.metrics import brier_score_loss
    brier = brier_score_loss(y_test, prob_pos)
    print(f"Brier score: {brier:.4f}")

    # Optional: calibration curve (reliability diagram)
    # prob_true, prob_pred = calibration_curve(y_test, prob_pos, n_bins=10)
    # (Plot prob_true vs prob_pred to visualize calibration)

Interpreting results
- Lower Brier score indicates better calibration (closer to true probabilities).
- A reliability diagram (calibration curve) shows how predicted probabilities align with observed frequencies. A perfectly calibrated model lies on the diagonal.

Practical considerations
- Isotonic calibration requires sufficient calibration samples to avoid overfitting; it tends to overfit with small calibration sets.
- For binary or multiclass classification, ensure the chosen estimator supports probability estimates or decision scores.
- Prefit mode is useful when you have a separate calibration split or want to calibrate a model already trained on a different dataset.

Reference notes
- The CalibratedClassifierCV class performs probability calibration via cross-validation and can ensemble the calibrated classifiers for improved stability.
- The calibration method influences how probabilities are learned: sigmoid provides a parametric fit; isotonic is non-parametric.
- Calibration can be used in conjunction with any compatible classifier and is especially beneficial when interpretable calibrated probabilities are required for downstream decisions.