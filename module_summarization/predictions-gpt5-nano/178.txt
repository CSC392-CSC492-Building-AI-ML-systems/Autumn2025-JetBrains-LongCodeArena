Computational Performance
=========================

Overview
--------
This document summarizes key considerations that influence computational performance in this project. Performance is shaped by the choice of numerical libraries, how parallelism is employed, and the structure of estimators (notably multi-output estimators). The minimum dependency configuration used for performance-oriented builds informs reproducibility and benchmarking.

Parallelism and Concurrency
---------------------------
- Estimators expose a parameter n_jobs to control parallel execution. Acceptable values are integers or None; setting n_jobs to -1 typically uses all available cores.
- Parallelism is implemented via utilities such as Parallel and delayed, enabling data- and task-parallel execution across estimators and pipelines.
- Thread pool control and discovery are supported by threadpoolctl, which helps manage and introspect the threading behavior of underlying libraries (e.g., BLAS/LAPACK-backed operations, SciPy routines).
- Performance characteristics can vary between CPython and PyPy. The minimum NumPy version is conditioned on the Python implementation (PyPy uses a newer minimum, 1.19.2) to ensure compatible and performant numerical operations.

Numerical Libraries and Build
-----------------------------
- Core numerical performance hinges on NumPy and SciPy. Minimum versions are defined to balance compatibility and speed:
  - NumPy: as per Python implementation (1.19.2 for PyPy, otherwise 1.17.3)
  - SciPy: 1.5.0
- Cython is required for building performance-critical extensions, enabling faster execution in compiled code paths.
- Performance is also influenced by the interplay with BLAS/LAPACK backends and platform-specific optimizations.

Multi-Output Estimators
-----------------------
- Multi-output estimators are meta-estimators that wrap a base estimator to handle multiple targets. Their training time scales with the number of outputs and the cost of fitting the base estimator.
- Internal helpers (e.g., _fit_estimator, _partial_fit_estimator) clone the base estimator and fit it for each output as needed, which can introduce overhead compared to fitting a single target.
- When profiling performance, consider the base estimator’s cost and the degree of parallelism across outputs.

Profiling and Benchmarking
--------------------------
- Performance benchmarks can be guided by the minimum-dependency configuration used in CI, which includes tools for testing and profiling:
  - Memory profiling (memory_profiler) to monitor memory usage during training and inference.
  - Testing and quality tools (pytest, pytest-cov) to ensure regressions aren’t introduced in performance-critical paths.
- For reliable benchmarks, run in a controlled environment with consistent library versions, and account for warm-up and caching effects.

Practical Tips
--------------
- Prefer vectorized operations in NumPy/SciPy; minimize Python-level loops inside training loops.
- Use n_jobs to exploit parallelism where appropriate, but be mindful of multi-threading limitations and Global Interpreter Lock interactions.
- For multi-output problems, evaluate whether the overhead of wrapping and cloning multiple estimators is outweighed by gains from parallelized fitting across outputs.
- Profile with standard tools (e.g., cProfile, memory_profiler) to identify bottlenecks, then target optimizations in the affected code paths.
- Ensure compatible library versions (as defined in the minimum dependency configuration) to avoid performance regressions due to ABI or API changes.

Notes on Dependency Configuration
-------------------------------
- The repository defines a mapping of dependencies to minimum versions and their usage tags (build, install, docs, tests, etc.). This mapping helps CI enforce reproducible performance baselines across environments.