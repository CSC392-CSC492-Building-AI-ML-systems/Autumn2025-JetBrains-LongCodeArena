Three types of algorithms for the estimation of the parameters of a model
========================================================================

This module provides a unified interface to fit models by optimizing an objective
function (for example, a log-likelihood). The available optimization methods fall
into three broad families, chosen via the method argument to the optimizer.

1) Gradient-based local optimization
------------------------------------

These methods use gradient information (and, in some cases, Hessian information)
to steer the search toward a local optimum. They are typically efficient near the
solution when the objective is smooth and well-behaved.

Supported methods include:
- newton: Newton-Raphson. Uses the Hessian (or its approximation) to compute updates.
- ncg: Newton-CG. Conjugate-gradient-style updates using Hessian-vector products.
- bfgs: Broyden–Fletcher–Goldfarb–Shanno. A quasi-Newton method updating the inverse Hessian.
- lbfgs: Limited-memory BFGS (with bounds). Efficient for large problems.
- cg: Conjugate Gradient. Gradient-based method suitable for large-scale problems.
- minimize: Generic wrapper around SciPy's minimize; can select a gradient-based method.

Notes:
- These methods generally require a gradient (and often a Hessian or Hessian-vector product) of the objective.
- Convergence is typically fast near the optimum but may depend on problem conditioning and initialization.

2) Derivative-free local optimization
--------------------------------------

When gradients are unavailable, unreliable, or expensive to compute, derivative-free
methods can be used. They explore the parameter space by evaluating the objective at
different points without requiring gradient information.

Supported methods include:
- nm: Nelder-Mead. A simplex-based method that does not use derivatives.
- powell: Powell’s method. A gradient-free optimization using successive line searches.

Notes:
- These methods are more robust to non-smoothness or noisy objectives but may be slower to converge.
- Useful as robust local optimizers when gradient information is missing or difficult to obtain.

3) Global optimization (basin-hopping and friends)
--------------------------------------------------

For problems with multiple local minima, global search strategies help escape local optima
and seek better global solutions. These methods often couple a stochastic/global step with a
local refinement.

Supported method:
- basinhopping: Global basin-hopping algorithm. Combines random perturbations with local minimizers
  to explore the objective landscape. Key options include:
  - niter: number of basin-hopping iterations
  - niter_success: stop criterion based on stability of the global minimum candidate
  - T: temperature for the accept/reject criterion
  - stepsize: initial step size for random displacements
  - interval: frequency of stepsize updates
  - minimizer: specification of the local minimizer used to refine candidates (e.g., a SciPy minimize call with a chosen method)

Notes:
- Basinhopping is particularly useful when the objective has many local minima.
- The local minimizer used by basinhopping is typically a gradient-based method (e.g., those in the first category).

Additional: a flexible, generic interface
-----------------------------------------

- minimize (wrapper): Acts as a general interface to SciPy’s minimize, enabling any supported method
  available in SciPy. This provides flexibility to mix and match local/global strategies as needed.

Usage guidance (brief):
- If gradient information is available and the problem is well-behaved, prefer gradient-based local methods (Newton, BFGS, LBFGS, CG).
- If gradients are unavailable or unreliable, use derivative-free methods (Nelder-Mead, Powell).
- If the objective has multiple local minima or you suspect poor global optimality, start with basinhopping to explore globally, followed by a local method for refinement.