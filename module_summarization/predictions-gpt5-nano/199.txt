Neural Network Models (Supervised)
==================================

This module provides utilities for supervised neural network models, including in-place activation functions, their derivatives, and common loss functions used in regression and classification.

Overview
--------
The utilities are designed to operate in place on NumPy arrays (or array-like data) to minimize memory usage during forward and backward passes. The module exposes:

- In-place activation functions
- In-place activation derivatives for backpropagation
- Registries for activations and derivatives
- Loss functions for regression and classification
- A registry mapping loss function names to their implementations

Activation functions (in-place)
------------------------------
Each function applies a nonlinear activation to the input array X in place.

- inplace_identity(X)
  - Leaves X unchanged.

- inplace_logistic(X)
  - Applies the logistic sigmoid function in place.
  - Uses scipy.special.expit (aliased as logistic_sigmoid).

- inplace_tanh(X)
  - Applies the hyperbolic tangent in place.

- inplace_relu(X)
  - Applies the rectified linear unit (ReLU) in place.

- inplace_softmax(X)
  - Applies the K-way softmax function in place across rows.

Public API (activations)
------------------------
- ACTIVATIONS
  - A mapping from activation name to the corresponding in-place function, e.g.:
    - "identity" -> inplace_identity
    - "tanh" -> inplace_tanh
    - "logistic" -> inplace_logistic
    - "relu" -> inplace_relu
    - "softmax" -> inplace_softmax

Activation derivatives (in-place)
---------------------------------
Each derivative function updates the backpropagated error signal (delta) in place, given the activation output Z from the forward pass.

- inplace_identity_derivative(Z, delta)
  - No-op (identity derivative).

- inplace_logistic_derivative(Z, delta)
  - delta *= Z * (1 - Z)
  - Exploits the derivative of the logistic function as a simple function of the activation value Z.

- inplace_tanh_derivative(Z, delta)
  - delta *= 1 - Z**2
  - Exploits the derivative of tanh in terms of Z.

- inplace_relu_derivative(Z, delta)
  - delta[Z == 0] = 0
  - Derivative is 1 for Z > 0 and 0 for Z <= 0 (masked at zeros).

Public API (derivatives)
------------------------
- DERIVATIVES
  - A mapping from activation name to the corresponding in-place derivative function, e.g.:
    - "identity" -> inplace_identity_derivative
    - "tanh" -> inplace_tanh_derivative
    - "logistic" -> inplace_logistic_derivative
    - "relu" -> inplace_relu_derivative

Loss functions
--------------
The module provides several loss functions used for training neural networks.

- squared_loss(y_true, y_pred)
  - Computes the squared loss for regression.
  - Returns mean((y_true - y_pred)^2) / 2.

- log_loss(y_true, y_prob)
  - Logistic loss (cross-entropy) for multi-class classification.
  - y_prob contains predicted probabilities with shape (n_samples, n_classes).
  - Clips probabilities to a small epsilon to avoid log(0).
  - If needed, expands y_prob and y_true to a two-class representation when there is a single class.
  - Returns the average cross-entropy over samples.

- binary_log_loss(y_true, y_prob)
  - Binary logistic loss for classification.
  - Identical to log_loss in the binary case but kept for multilabel usage.
  - Clips probabilities and returns the average cross-entropy over samples.

Public API (loss functions)
---------------------------
- LOSS_FUNCTIONS
  - A registry mapping loss function names to their implementations.
  - Example keys (illustrative): "squared", "log", "binary_log_loss" (actual keys depend on usage in the surrounding code).

Notes
-----
- Dependencies: NumPy and SciPy (for the logistic sigmoid and log-sum computations).
- Input shapes: activations operate on array-like inputs shaped as (n_samples, n_features). Backpropagation derivatives expect Z (activation output from forward pass) and delta (the error signal) with the same shape.

For reference, the module provides a cohesive set of utilities to perform in-place activations and their derivatives, along with common loss functions used in supervised neural networks.