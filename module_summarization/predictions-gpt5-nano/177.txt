Pipelines and Composite Estimators
===================================

The sklearn.pipeline module implements utilities to build composite estimators, as a chain of transforms and estimators. It provides the Pipeline class, as well as utilities like FeatureUnion and convenience constructors make_pipeline and make_union to compose complex estimators from simpler parts.

Pipeline
--------

A Pipeline is a sequence of steps that are applied in order, ending with a final estimator. Intermediate steps must implement fit and transform, while the final estimator must implement fit. Pipelines can cache transformed data to speed up repeated fits via the memory parameter. Parameters of steps can be set using the "__" separator, enabling convenient hyperparameter tuning with tools like GridSearchCV.

Key concepts
- Steps are a list of (name, transform_or_estimator) tuples. The last step is typically an estimator; earlier steps are transformers.
- Parameters are accessed and set using the step name followed by a double underscore and the parameter name, e.g., "scaler__with_mean".
- The pipeline can be inspected via named_steps (a dict-like object mapping step names to the corresponding objects).
- Memory caching is supported for the transformers; the final estimator is never cached.

Parameters
- steps: list of (name, transform) tuples (transforms implementing fit/transform and the final estimator).
- memory: caching mechanism (str path to a directory or an object implementing the Joblib Memory interface), or None for no caching.
- verbose: whether to print progress information during fitting.

Attributes
- named_steps: dictionary-like object with one entry per step; keys are step names, values are the steps.
- classes_: the classes seen if the last step is a classifier.
- n_features_in_: number of features seen during fit (if exposed by the first estimator in steps).
- feature_names_in_: names of features seen during fit (if exposed by the estimator).

Notes
- The final estimator must be fit; earlier transformers are responsible for transforming the data.
- The set_output mechanism can be used to configure the output format of transformers in the pipeline.

Examples
- Basic usage:
  .. code-block:: python

     from sklearn.pipeline import Pipeline
     from sklearn.preprocessing import StandardScaler
     from sklearn.svm import SVC

     pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])
     pipe.fit(X_train, y_train)
     score = pipe.score(X_test, y_test)

- Setting parameters across steps:
  .. code-block:: python

     pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)

See Also
- make_pipeline: Convenience function to create a Pipeline with less boilerplate.
- FeatureUnion: Combines transformer outputs in parallel and concatenates their features.
- make_union: Convenience constructor for FeatureUnion.

Version added
- Pipeline: 0.5

FeatureUnion
------------

FeatureUnion allows parallel feature extraction by applying several transformers to the input data and concatenating their outputs. This is useful when you want to combine features produced by different representations (e.g., text features, image features, and numeric features) before passing them to a final estimator.

Parameters, usage, and examples
- Similar to Pipeline, but with parallel transformers whose outputs are concatenated.
- Can be used inside a Pipeline as a single step to enrich the feature space before the final estimator.

make_pipeline
-------------

make_pipeline is a convenience function that constructs a Pipeline from a sequence of transformers and a final estimator without needing to name each step explicitly.

make_union
----------

make_union is a convenience function that constructs a FeatureUnion from a sequence of transformers without explicitly naming each transformer.

Notes
- Both make_pipeline and make_union are useful for quickly building composite estimators with readable, short code.