Working with Large Data Sets
============================

Overview
--------

This documentation describes the workflow for handling large data sets with distributed estimation routines. The framework supports several estimation strategies and backends to scale beyond a single process.

Key ideas
- Distributed estimation with partitions: data are split into chunks (partitions) that can be processed independently and then combined.
- Estimation strategies:
  - Regularized debiased estimation (default)
  - Simple coefficient averaging (naive), with regularized and unregularized variants
- Parallel execution
  - Sequential (no special dependencies)
  - Parallel via joblib, supporting multiple backends and clusters
    - dask.distributed
    - yarn
    - ipyparallel

Framework capabilities
----------------------

- The default method is regularized estimation with debiasing, following contemporary literature on efficient distributed sparse regression.
- The approach forms an approximate inverse covariance matrix to enable debiasing and improve estimator accuracy when combining results across partitions.
- The framework is designed to be general, allowing for different estimation methods and backends.

Literature and references
-------------------------

- Jason D. Lee, Qiang Liu, Yuekai Sun and Jonathan E. Taylor. "Communication-Efficient Sparse Regression: A One-Shot Approach." arXiv:1503.04337 (2015).

Variables used in the debiasing procedure
-----------------------------------------

These variables are used to help form the approximate inverse covariance matrix and to implement the debiasing procedure.

- wexog: A weighted design matrix used to perform the node-wise regression procedure.
- nodewise_row: Result from the node-wise regression procedure for each variable.
- nodewise_weight: Weights derived from gamma_hat values for reweighting the nodewise coefficients.
- approx_inv_cov: The estimate of the approximate inverse covariance matrix, used to debias the coefficient average and the average gradient. For the OLS case, approx_inv_cov approximates n * (X^T X)^{-1}, formed by node-wise regression.

Helper routines and their purposes
----------------------------------

The following routines support distributed estimation and debiasing across partitions.

_est_regularized_naive(mod, pnum, partitions, fit_kwds=None)
- Purpose: Estimate regularized fitted parameters for a single partition.
- Parameters:
  - mod: Model instance for the current partition
  - pnum: Index of the current partition
  - partitions: Total number of partitions
  - fit_kwds: Keyword arguments for fit_regularized
- Returns: Array of parameters for the regularized fit

_est_unregularized_naive(mod, pnum, partitions, fit_kwds=None)
- Purpose: Estimate unregularized fitted parameters for a single partition.
- Parameters:
  - mod: Model instance for the current partition
  - pnum: Index of the current partition
  - partitions: Total number of partitions
  - fit_kwds: Keyword arguments for fit
- Returns: Array of parameters for the fit

_join_naive(params_l, threshold=0)
- Purpose: Combine results from multiple naive fits by averaging across partitions.
- Parameters:
  - params_l: List of arrays of coefficients from each partition
  - threshold: Coefficient threshold below which values are set to zero
- Returns: Averaged parameter vector with thresholding applied

_calc_grad(mod, params, alpha, L1_wt, score_kwds)
- Purpose: Compute the gradient used in the debiasing procedure.
- Parameters:
  - mod: Model instance for the current partition
  - params: Estimated coefficients for the current partition
  - alpha: Penalty weight (scalar or vector)
  - L1_wt: Fraction of the penalty assigned to the L1 term (0 - ridge, 1 - lasso)
  - score_kwds: Keywords for the score function
- Returns: Gradient vector matching the dimension of params

_calc_wdesign_mat(mod, params, hess_kwds)
- Purpose: Calculate the weighted design matrix for the approximate inverse covariance.
- Parameters:
  - mod: Model instance for the current partition
  - params: Estimated coefficients for the current partition
  - hess_kwds: Keywords for the Hessian factor
- Returns: Weighted design matrix with the same dimensions as mod.exog

_est_regularized_debiased(mod, mnum, partitions, fit_kwds=None,
                          score_kwds=None, hess_kwds=None)
- Purpose: Estimate regularized parameters with debiasing (default method for distributed models).
- Parameters:
  - mod: Model instance for the current partition
  - mnum: Index of the current partition
  - partitions: Total number of partitions
  - fit_kwds: Keywords for fit_regularized
  - score_kwds: Keywords for the score function
  - hess_kwds: Keywords for the Hessian function
- Returns: A tuple of parameters and additional debiasing-related outputs (depending on implementation)

Usage workflow
--------------

- Partition the data and assign each partition to a worker.
- Choose an estimation strategy:
  - Regularized debiased estimation (default)
  - Naive averaging (regularized or unregularized)
- If using parallel execution via joblib, configure the backend (e.g., dask.distributed, yarn, ipyparallel) to match your cluster setup.
- After fitting each partition, combine results using the provided joiners (e.g., _join_naive) and apply debiasing using the gradient and weighted design matrix calculations.
- Use the approx_inv_cov for debiasing to obtain improved coefficient estimates and consistent inference across large data splits.

Notes
-----

- The framework emphasizes scalability and modularity, enabling experimentation with different backends and estimation strategies without changing the core model definitions.
- Debiasing relies on node-wise procedures to form an approximate inverse covariance, which is central to achieving accurate, scalable inference on large data sets.

References
----------

- Jason D. Lee, Qiang Liu, Yuekai Sun and Jonathan E. Taylor. "Communication-Efficient Sparse Regression: A One-Shot Approach." arXiv:1503.04337. 2015. https://arxiv.org/abs/1503.04337

This documentation reflects the distributed estimation routines and supporting components available for working with large data sets. Use this as a guide when configuring, executing, and interpreting results from large-scale analyses.