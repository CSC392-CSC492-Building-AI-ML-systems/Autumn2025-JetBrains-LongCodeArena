Analyzing a collection of text documents
========================================

This section documents how scikit-learn analyzes a collection of text documents and converts them into numerical feature vectors suitable for machine learning models. It covers the preprocessing, tokenization, stop-word handling, and vectorization steps provided by the text feature extraction utilities.

Overview
--------

Text data is converted into numeric features to be used by machine learning algorithms. The primary building blocks are:

- Decoding input to a unified Unicode text representation
- Optional text preprocessing (lowercasing, accent handling, etc.)
- Tokenization and optional n-gram expansion
- Stop-word filtering
- Vectorization into sparse numeric matrices (Counts or TF–IDF)

Key components
--------------

The text feature extraction module exposes several vectorizers and transformers:

- CountVectorizer: converts a collection of text documents to a matrix of token counts.
- TfidfVectorizer: builds on the count matrix and applies TF–IDF normalization.
- HashingVectorizer: projects text into a fixed-dimensional feature space using the hashing trick.
- TfidfTransformer: converts a count matrix to a TF–IDF representation (used with CountVectorizer or other count sources).

Common preprocessing and tokenization concepts
----------------------------------------------

Input decoding
- The system accepts different input forms (e.g., filenames, file-like objects) and decodes them into Unicode text. Each document is transformed into a string of Unicode characters before further processing.

Preprocessing
- Optional preprocessing steps can be chained to transform the raw document. Typical options include:
  - Lowercasing text
  - Accent handling (see accent stripping below)
  - Custom accent_normalization functions

Analyzer, preprocessor, tokenizer, and n-grams
- The text processing pipeline can be configured via:
  - analyzer: a callable that replaces the preprocessor, tokenizer, and n-grams steps
  - preprocessor: a callable applied to each document before tokenization
  - tokenizer: a callable that turns text into tokens
  - ngrams: a callable that expands tokens into n-grams (e.g., bigrams, trigrams)
- If an analyzer is provided, it takes responsibility for the preprocessing/tokenization/ngrams steps; otherwise, each step can be configured independently and chained together.

Stop words
- Stop-word lists can be supplied or selected from built-in options (e.g., ENGLISH_STOP_WORDS). Stop words are removed from the token stream prior to vectorization to reduce noise.

Accent stripping
- Accent handling can be performed in different ways:
  - strip_accents_unicode: uses Unicode normalization and removes combining characters. Slower, but more general.
  - strip_accents_ascii: transliterates accented characters to ASCII where possible. Faster, but language-specific.
- You can choose the approach that best fits your language and performance requirements.

Practical usage patterns
------------------------

Basic CountVectorizer usage
- Convert documents to a matrix of token counts.
- Optional: enable lowercase, specify stop words, and extract basic or custom n-grams.

Example:
  - from sklearn.feature_extraction.text import CountVectorizer
  - vectorizer = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 2))
  - X = vectorizer.fit_transform(documents)

TF–IDF-based features
- Use TfidfVectorizer to obtain TF–IDF features directly, combining token counts with inverse document frequency weighting.

Example:
  - from sklearn.feature_extraction.text import TfidfVectorizer
  - vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 2))
  - X = vectorizer.fit_transform(documents)

Hashing-based features for streaming or large-scale data
- HashingVectorizer provides a memory-friendly, stateless alternative suitable for streaming data.

Example:
  - from sklearn.feature_extraction.text import HashingVectorizer
  - vect = HashingVectorizer(n_features=2**20, alternate_sign=False)
  - X = vect.transform(documents)

TF–IDF transformation on an existing count matrix
- If you already have a count matrix, TfidfTransformer can compute TF–IDF features.

Example:
  - from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
  - count_vect = CountVectorizer(...)
  - X_counts = count_vect.fit_transform(documents)
  - tfidf_transformer = TfidfTransformer()
  - X_tfidf = tfidf_transformer.fit_transform(X_counts)

Input handling and decoding
- Documents can be provided as raw strings, filenames, or file-like objects. The decoding step ensures uniform Unicode text for downstream processing.

Combined workflow example
- Use CountVectorizer with English stop words and bigrams, then apply TfidfTransformer to obtain TF–IDF features.

Example:
  - from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
  - documents = ["text of document one", "text of document two", ...]
  - count_vect = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 2))
  - X_counts = count_vect.fit_transform(documents)
  - tfidf_transformer = TfidfTransformer()
  - X_tfidf = tfidf_transformer.fit_transform(X_counts)

See also
--------

- ENGLISH_STOP_WORDS: built-in stop word list for English
- The scalar/vectorizer APIs: fit, transform, and fit_transform methods

Notes
-----

- The choice of lowercase, accent stripping, stop words, and n-gram settings can significantly impact model performance and sparsity of the resulting feature space.
- For large datasets or streaming data, consider HashingVectorizer to avoid building a large vocabulary.