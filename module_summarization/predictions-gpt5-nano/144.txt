Scheduling policies
====================

The distributed scheduler in Dask uses explicit policies to determine:
- the preference for which tasks to execute next, and
- the preference for which workers should execute a given task.

These policies are embodied in the task lifecycle, messaging between clients and workers, and a set of scheduler extensions that influence task placement and work distribution.

Task-preference policy
----------------------

What determines which task to run next?

- Task states and lifecycle
  - Each task has a lifecycle tracked by TaskStateState with states such as released, waiting, no-worker, queued, processing, memory, erred, and forgotten.
  - The global set ALL_TASK_STATES encompasses all possible task states, helping the scheduler reason about progress and readiness.

- Readiness and dependencies
  - A task becomes ready when its dependencies are satisfied and it can be executed on some worker. The scheduler prioritizes advancing ready tasks toward execution based on current conditions (data availability, worker capacity, and other factors).

- Preference data structures and exchange
  - The scheduler uses a recommendation mechanism to guide task ordering. Recs represents per-task finish-state recommendations, and RecsMsgs aggregates three message channels: recommendations, client messages, and worker messages.
  - This recommendation protocol supports coordinating decisions between the client, scheduler, and workers, enabling dynamic adjustments to which tasks should run when and where.

- Run specifications
  - T_runspec encodes how to run a task (a callable), along with its positional and keyword arguments. The scheduling system uses these run specifications to execute tasks once a choice is made about their placement.

- Role of extensions in task policy
  - Several scheduler extensions influence which tasks are preferred next. For example, ReplayTaskScheduler can alter execution ordering for fault tolerance, while ShuffleSchedulerPlugin and SpansSchedulerExtension can affect how work is distributed and traced, respectively.

Worker-preference policy
------------------------

What determines which worker should execute a given task?

- Data locality and memory
  - Worker selection favors data locality when possible, aiming to run tasks on workers that already host the required data in memory or on fast-access storage.
  - Memory pressure is actively managed by extensions such as ActiveMemoryManagerExtension and MemorySamplerExtension, which influence where and when tasks are moved or evicted to maintain memory health.

- Load balancing and work distribution
  - WorkStealing (WorkStealing extension) is a key mechanism for balancing load. Idle workers may steal tasks from busier workers to improve utilization and reduce stragglers.
  - The presence of multiple worker queues and queue extensions (e.g., QueueExtension) provides flexibility in how tasks are buffered and assigned, impacting which worker is most suitable for a given task.

- Extensions that affect worker policy
  - Stealing-related behavior via WorkStealing directly governs how workers choose targets for execution.
  - Other extensions (PublishExtension, PubSubSchedulerExtension, ShuffleSchedulerPlugin, SpansSchedulerExtension, etc.) can indirectly influence worker choice through data movement, event-driven scheduling cues, and specialized execution strategies.

Extensions and defaults
------------------------

The default extension set defines core scheduling capabilities and how they modify task and worker preferences:

- locks, multi_locks, publish, replays, queues, variables, pubsub, semaphores
- active memory management and memory sampling
- data-shuffling and task-stealing support
- spans (tracing) and related scheduling behavior

Key types and concepts referenced in scheduling decisions
--------------------------------------------------------

- TaskStateState: a type alias for the set of possible per-task states (e.g., "released", "waiting", "no-worker", "queued", "processing", "memory", "erred", "forgotten").
- ALL_TASK_STATES: the complete set of all TaskStateState values.
- Recs: a mapping from task keys to their recommended finish state.
- RecsMsgs: a tuple containing (Recs, client messages, worker messages), representing the three channels involved in communicating scheduling decisions.
- T_runspec: a type alias for a runnable specification, including the callable, its args, and its keyword arguments.
- Extensions: components like WorkStealing, ReplayTaskScheduler, MemorySamplerExtension, ActiveMemoryManagerExtension, and ShuffleSchedulerPlugin that shape how tasks are chosen and where they run.

This documentation summarizes how Dask's distributed scheduler uses task and worker policies to optimize execution, data locality, memory usage, and load balancing through a combination of task states, recommendations, and a suite of extensions.