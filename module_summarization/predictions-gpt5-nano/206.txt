Support Vector Machines
=======================

Overview
--------

Support Vector Machines (SVMs) are a family of supervised learning methods used for classification and regression. They work by finding a decision boundary (hyperplane) that maximizes the margin between classes in the feature space. Linear SVMs are a particular efficiency-focused variant that use a linear decision boundary, suitable for high-dimensional data.

Linear SVMs via liblinear
-------------------------

This implementation provides LinearSVC, a linear Support Vector Classifier backed by liblinear. It is designed for large-scale problems and supports both dense and sparse input, with multiclass classification handled in a one-vs-rest (ovr) scheme by default.

Key characteristics include:
- Uses liblinear for efficient optimization.
- Suitable for large feature spaces and datasets.
- Supports sparse input formats.

Class reference
---------------

LinearSVC
- Inherits from LinearClassifierMixin, SparseCoefMixin, and BaseEstimator.
- Provides a linear decision boundary with configurable loss and regularization.

Parameters
----------

penalty
  {'l1', 'l2'}, default='l2'
  Norm used in the penalization. 'l2' is the standard penalty used by SVC; 'l1' yields sparse coefficient vectors.

loss
  {'hinge', 'squared_hinge'}, default='squared_hinge'
  Specifies the loss function. 'hinge' is the standard SVM loss; 'squared_hinge' is its squared version. The combination of penalty='l1' and loss='hinge' is not supported.

dual
  "auto" or bool, default=True
  Selects whether to solve the dual or primal optimization problem.
  - Prefer dual=False when the number of samples is greater than the number of features.
  - "auto" automatically chooses the dual or primal based on n_samples, n_features, loss, multi_class, and penalty. If n_samples < n_features and the chosen configuration is supported, dual will be True; otherwise dual will be False.

tol
  float, default=1e-4
  Tolerance for stopping criteria.

C
  float, default=1.0
  Regularization parameter. The strength of regularization is inversely proportional to C. Must be strictly positive.

multi_class
  {'ovr', 'crammer_singer'}, default='ovr'
  Strategy for multiclass classification:
  - 'ovr' trains one-vs-rest classifiers for each class.
  - 'crammer_singer' solves a joint objective over all classes. If 'crammer_singer' is chosen, loss, penalty, and dual are ignored.

fit_intercept
  bool, default=True
  Whether to fit an intercept term. If True, the feature vector is extended with an intercept term.

intercept_scaling
  float, default=1.0
  If fit_intercept is True, the intercept is scaled by this value and appended as a synthetic feature. Liblinear internally penalizes the intercept like any other feature. To reduce the impact of regularization on the intercept, use an appropriate intercept_scaling.

Notes
------

- The default values and behavior reflect typical usage in large-scale linear SVM classification.
- Read more in the User Guide on SVM Classification for chosen loss and penalty combinations and their implications.

Training and predictions
------------------------

- The model is trained with a problem/parameter configuration and returns a trained linear classifier.
- Predictions are made by applying the linear decision function and thresholding to assign class labels.

Example
-------

.. code-block:: python

   from sklearn.svm import LinearSVC

   # Create a linear SVM classifier
   clf = LinearSVC(C=1.0, penalty='l2', loss='squared_hinge', dual='auto')

   # Train the model
   clf.fit(X_train, y_train)

   # Predict on new data
   y_pred = clf.predict(X_test)

See also
--------

- User Guide: SVM Classification
- LinearSVC class reference in sklearn.svm

This documentation summarizes the Linear SVC implementation details, options, and typical usage for Support Vector Machines using a linear kernel with liblinear backend.