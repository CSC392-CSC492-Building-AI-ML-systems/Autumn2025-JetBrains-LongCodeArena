Decision Trees
==============

.. currentmodule:: sklearn.tree

Decision Trees (DTs) are a non-parametric supervised learning method used for
classification and regression. The goal is to create a model that predicts
the value of a target variable by learning simple decision rules inferred
from the data features.

Introduction
------------

Decision Trees learn from data to approximate a sine curve with a set of
if-then-else decision rules. The deeper the tree, the more complex the
decision rules and the more fits the model to the data.

Decision trees are simple to understand and interpret, capable of handling
both numerical and categorical data, and require little data preparation.
They use a white box model where the results are easily explainable. However,
they can overfit the data and may not generalize well to new data.

Classification
--------------

:class:`DecisionTreeClassifier` is a class capable of performing multi-class
classification on a dataset.

As with other classifiers, :class:`DecisionTreeClassifier` takes as input two
arrays: an array X, sparse or dense, of shape ``(n_samples, n_features)``
holding the training samples, and an array Y of integer values, shape
``(n_samples,)``, holding the class labels for the training samples::

    >>> from sklearn import tree
    >>> X = [[0, 0], [1, 1]]
    >>> Y = [0, 1]
    >>> clf = tree.DecisionTreeClassifier()
    >>> clf = clf.fit(X, Y)
    >>> clf.predict([[2., 2.]])
    array([1])

After being fitted, the model can predict the class of new samples::

    >>> clf.predict([[2., 2.]])
    array([1])

In case that there are few classes with very few samples, it could be possible
that the leaves of the tree contain very few training samples. In this case,
the prediction might be unreliable.

Regression
----------

:class:`DecisionTreeRegressor` is a class capable of performing regression
on a dataset.

As with other regressors, :class:`DecisionTreeRegressor` takes as input two
arrays: an array X, sparse or dense, of shape ``(n_samples, n_features)``
holding the training samples, and an array y of real values, shape
``(n_samples,)``, holding the target values for the training samples::

    >>> from sklearn import tree
    >>> X = [[0, 0], [2, 2]]
    >>> y = [0.5, 2.5]
    >>> clf = tree.DecisionTreeRegressor()
    >>> clf = clf.fit(X, y)
    >>> clf.predict([[1, 1]])
    array([0.5])

Multi-output Problems
---------------------

A multi-output problem is a supervised learning problem with several outputs
to predict, that is to say when Y is a 2d array of shape ``(n_samples, n_outputs)``.

Multi-output Decision Tree algorithms support multi-output problems in a
natural way: there is no need to modify the underlying algorithm to handle
multi-output data.

Multi-output trees can be used for regression and classification tasks.

Complexity
----------

In general, the run time cost to construct a balanced binary tree is
:math:`O(n_{samples}n_{features}\log(n_{samples}))` and query time
:math:`O(\log(n_{samples}))`.

Tips on Practical Use
---------------------

* Decision trees tend to overfit on data with a large number of features.
  It is recommended to plot the tree and check the number of samples at
  the leaf nodes. If the number of samples is small, it might be overfitting.

* When predicting using a decision tree, the prediction is made by traversing
  the tree from the root to a leaf node. The prediction is the average of the
  target values of the training samples that reach that leaf.

* Pruning the tree can help to reduce overfitting. This can be done by
  setting the ``max_depth`` parameter.

* For data including categorical variables, it is recommended to use
  :class:`sklearn.preprocessing.OrdinalEncoder` or
  :class:`sklearn.preprocessing.OneHotEncoder` to convert them into numerical
  values.

Mathematical Formulation
------------------------

Given training vectors :math:`x_i \in R^n`, i=1,..., l and a label vector
:math:`y \in R^l` where :math:`R` is the set of real numbers, a decision tree
recursively partitions the feature space such that the samples with the same
labels are grouped together.

Let the data at node :math:`m` be represented by :math:`Q_m` with :math:`n_m`
samples. For each candidate split :math:`\theta = (j, t_m)` consisting of a
feature :math:`j` and threshold :math:`t_m`, partition the data into
:math:`Q_m^{left}(\theta)` and :math:`Q_m^{right}(\theta)` subsets:

.. math::

    Q_m^{left}(\theta) = \{(x, y) | x_j \leq t_m\}

    Q_m^{right}(\theta) = Q_m \setminus Q_m^{left}(\theta)

The impurity at :math:`m` is computed using an impurity function :math:`H()`:

.. math::

    G(Q_m, \theta) = \frac{n_m^{left}}{n_m} H(Q_m^{left}(\theta)) + \frac{n_m^{right}}{n_m} H(Q_m^{right}(\theta))

Select the parameters that minimize the impurity:

.. math::

    \theta^* = \operatorname{argmin}_\theta G(Q_m, \theta)

Common impurity measures for classification are:

* Gini impurity:

.. math::

    H(Q_m) = \sum_k p_{mk} (1 - p_{mk})

* Entropy:

.. math::

    H(Q_m) = - \sum_k p_{mk} \log(p_{mk})

where :math:`p_{mk}` is the fraction of samples of class :math:`k` in node
:math:`m`.

For regression, the mean squared error is commonly used:

.. math::

    H(Q_m) = \frac{1}{n_m} \sum_{y \in Q_m} (y - \bar{y}_m)^2

where :math:`\bar{y}_m` is the mean of the target values in node :math:`m`.

Tree Algorithms
---------------

The module contains several algorithms for constructing decision trees:

* ID3 (Iterative Dichotomiser 3)
* C4.5 (successor of ID3)
* CART (Classification And Regression Tree)
* CHAID (CHi-squared Automatic Interaction Detector)

The scikit-learn implementation uses an optimized version of the CART
algorithm.

Tree Pruning
------------

Pruning is a technique in machine learning and search algorithms that reduces
the size of decision trees by removing sections of the tree that provide
little power to classify instances. Pruning reduces the complexity of the
final classifier, and hence improves predictive accuracy by the reduction
of overfitting.

There are two types of pruning:

* Pre-pruning: Halt tree construction early. This method is preferred because
  it is easier to implement and computationally cheaper.

* Post-pruning: Remove branches from a fully grown tree. This method is more
  accurate but computationally expensive.

In scikit-learn, pre-pruning is used by setting parameters like
``max_depth``, ``min_samples_split``, ``min_samples_leaf``, etc.

Missing Values Support
----------------------

Decision trees in scikit-learn support missing values (np.nan) in the input
data. The algorithm handles missing values by using a surrogate split method.
When a split is made, the algorithm finds the best surrogate split that
approximates the primary split. This allows the tree to make predictions even
when some feature values are missing.

Monotonic Constraints
---------------------

Monotonic constraints can be imposed on the decision tree to ensure that the
prediction is monotonic with respect to certain features. This is useful when
prior knowledge about the relationship between features and target is known.

The constraints are specified as a list of integers where each element
corresponds to a feature:

* 1 indicates that the target should increase with the feature
* -1 indicates that the target should decrease with the feature
* 0 indicates no constraint

These constraints are enforced during the tree construction process.

Feature Importance
------------------

The feature importance (or mean decrease impurity) is computed by measuring
how much each feature contributes to decreasing the impurity of the nodes
it is used in. The higher the value the more important the feature.

The importance of a feature is computed as the (normalized) total reduction
of the criterion brought by that feature. It is also known as the Gini
importance.

.. note::

    The feature importance values are normalized so that they sum to 1.

The feature importances are stored in the ``feature_importances_`` attribute
of the fitted tree.

Example
-------

.. code-block:: python

    from sklearn.datasets import load_iris
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.tree import plot_tree
    import matplotlib.pyplot as plt

    # Load data
    iris = load_iris()
    X, y = iris.data, iris.target

    # Fit classifier
    clf = DecisionTreeClassifier(random_state=0)
