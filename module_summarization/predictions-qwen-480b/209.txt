Analyzing a Collection of Text Documents
========================================

The :mod:`sklearn.feature_extraction.text` submodule provides utilities to convert a collection of text documents into numerical feature vectors that can be used with scikit-learn's machine learning algorithms. This process is known as **text vectorization**.

Overview
--------

Text data, in its raw form, is not directly usable by most machine learning models, which require numerical input. The goal of text feature extraction is to transform a corpus of documents into a matrix where:

- Rows correspond to individual documents.
- Columns correspond to features (e.g., words or n-grams).
- Values represent the importance or frequency of each feature in each document.

The main classes provided for this purpose are:

- :class:`CountVectorizer`: Converts a collection of text documents to a matrix of token counts.
- :class:`TfidfVectorizer`: Converts a collection of raw documents to a matrix of TF-IDF features.
- :class:`HashingVectorizer`: Implements the hashing trick to convert tokens into features.

Preprocessing and Tokenization
------------------------------

Before converting text into numerical features, several preprocessing steps may be applied:

- **Decoding**: If the input consists of byte sequences, they are decoded into Unicode strings.
- **Lowercasing**: Optionally convert all characters to lowercase.
- **Accent Stripping**: Remove or normalize accented characters using functions like :func:`strip_accents_unicode` or :func:`strip_accents_ascii`.
- **HTML Tag Removal**: Basic removal of HTML/XML tags using :func:`strip_tags`.
- **Tokenization**: Splitting the text into individual words or tokens.
- **Stop Words Filtering**: Removing commonly used words (like "the", "is", etc.) that may not carry much meaning.
- **N-gram Extraction**: Constructing sequences of n consecutive tokens (e.g., bigrams, trigrams).

These steps are handled internally by the vectorizers but can also be customized through parameters.

Example Usage
-------------

Here is a basic example of how to use :class:`CountVectorizer` to analyze a small collection of text documents:

.. code-block:: python

    from sklearn.feature_extraction.text import CountVectorizer

    # Sample documents
    documents = [
        "The quick brown fox jumps over the lazy dog",
        "Never jump over the lazy dog quickly",
        "A fast brown fox leaps over a sleepy dog"
    ]

    # Initialize the vectorizer
    vectorizer = CountVectorizer()

    # Fit and transform the documents
    X = vectorizer.fit_transform(documents)

    # View the feature names (vocabulary)
    print("Vocabulary:", vectorizer.get_feature_names_out())

    # View the dense matrix representation
    print("Dense matrix:\\n", X.toarray())

This will output a matrix where each row corresponds to a document and each column corresponds to a word in the vocabulary.

TF-IDF Transformation
---------------------

In addition to raw term frequencies, it's often useful to re-weight the features using Term Frequency-Inverse Document Frequency (TF-IDF). This down-scales the impact of tokens that occur frequently across many documents.

You can apply TF-IDF transformation using :class:`TfidfVectorizer`, which combines the functionality of :class:`CountVectorizer` with :class:`TfidfTransformer`.

.. code-block:: python

    from sklearn.feature_extraction.text import TfidfVectorizer

    # Initialize the TF-IDF vectorizer
    tfidf_vectorizer = TfidfVectorizer()

    # Fit and transform the documents
    X_tfidf = tfidf_vectorizer.fit_transform(documents)

    # View the TF-IDF matrix
    print("TF-IDF matrix:\\n", X_tfidf.toarray())

Advanced Options
----------------

Vectorizers offer a wide range of options to customize text processing:

- `ngram_range`: Specify the range of n-grams to extract (e.g., (1, 2) for unigrams and bigrams).
- `max_features`: Limit the number of features (vocabulary size) to the top occurring terms.
- `stop_words`: Provide a list of stop words or use built-in lists like "english".
- `analyzer`: Define the unit of analysis (e.g., 'word', 'char', or a custom function).
- `max_df` / `min_df`: Ignore terms that appear too frequently or too infrequently.

For large-scale applications where memory efficiency is important, consider using :class:`HashingVectorizer`, which does not store the vocabulary in memory and instead uses the hashing trick to map tokens to indices.

Conclusion
----------

Analyzing text data in scikit-learn involves transforming raw text into numerical representations suitable for machine learning. With flexible preprocessing and vectorization tools, you can tailor the feature extraction process to your specific needs, enabling effective modeling of textual data.