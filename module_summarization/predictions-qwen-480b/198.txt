Nearest Neighbors
=================

.. currentmodule:: sklearn.neighbors

The :mod:`sklearn.neighbors` module provides functionality for unsupervised and supervised learning using nearest neighbors. It includes implementations of various algorithms for efficient neighbor searches, such as Ball Tree and KD Tree, along with different distance metrics.

Overview
--------

Nearest neighbors is a simple yet powerful approach used in many machine learning applications including classification, regression, clustering, and dimensionality reduction. The core idea is to find the closest data points (neighbors) to a given query point based on a specified distance metric.

Key Concepts
------------

- **Distance Metrics**: Various distance metrics are supported including Euclidean, Manhattan, Chebyshev, and others.
- **Algorithms**: Efficient algorithms like Ball Tree and KD Tree are implemented for fast neighbor searches.
- **Types of Queries**: Support for finding k-nearest neighbors or neighbors within a fixed radius.

Classes
-------

BallTree
~~~~~~~~

The Ball Tree is a binary tree structure used for efficient neighbor searches in low to moderate dimensional spaces. It partitions the data into nested hyperspheres (balls), making it particularly effective for datasets with non-uniform density.

BallTree64
~~~~~~~~~~

A 64-bit precision implementation of the Ball Tree algorithm using double-precision floating-point numbers for enhanced accuracy in computations.

BallTree32
~~~~~~~~~~

A 32-bit precision implementation of the Ball Tree algorithm using single-precision floating-point numbers for memory-efficient operations.

Distance Metrics
----------------

The following distance metrics are available for use with nearest neighbor algorithms:

- Bray-Curtis Distance
- Canberra Distance
- Chebyshev Distance
- Dice Distance
- Euclidean Distance
- Hamming Distance
- Haversine Distance
- Jaccard Distance
- Mahalanobis Distance
- Manhattan Distance
- Minkowski Distance
- Rogers-Tanimoto Distance
- Russell-Rao Distance
- Standardized Euclidean Distance
- Sokal-Michener Distance
- Sokal-Sneath Distance
- Weighted Minkowski Distance

Implementation Details
----------------------

The implementation uses Cython for performance optimization. The Ball Tree construction involves:

1. Computing centroids for each node based on the data points it contains
2. Calculating node radii based on the maximum distance from centroid to any point
3. Supporting both weighted and unweighted data samples
4. Using reduced distances for computational efficiency where applicable

The algorithms support both 32-bit and 64-bit floating-point precision to balance between memory usage and computational accuracy.