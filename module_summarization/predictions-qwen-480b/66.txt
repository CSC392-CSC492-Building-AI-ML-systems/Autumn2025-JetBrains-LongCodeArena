.. _pipeline_tutorial:

Pipeline Tutorial
=================

In this tutorial, you will learn how to use pipelines in scikit-learn to
chain multiple data processing steps together. Pipelines are a powerful tool
for building machine learning workflows that are clean, readable, and easy
to tune.

What is a Pipeline?
-------------------

A pipeline in scikit-learn is a sequence of data processing steps where
each step is either a transformer (implementing `fit` and `transform`) or
an estimator (implementing `fit`). The final step in a pipeline must be
an estimator, which is used to make predictions.

Pipelines are useful for:

- **Preventing data leakage**: By ensuring that all transformations are
  applied consistently during training and testing.
- **Parameter tuning**: All steps in a pipeline can be tuned together using
  tools like :class:`~sklearn.model_selection.GridSearchCV`.
- **Code organization**: Keeping preprocessing and modeling steps together
  in a single object.

Creating a Pipeline
-------------------

To create a pipeline, you pass a list of steps to the :class:`~sklearn.pipeline.Pipeline`
constructor. Each step is a tuple containing a name and an estimator.

Here's a simple example using a scaler and a classifier:

.. code-block:: python

    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.svm import SVC
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split

    # Generate sample data
    X, y = make_classification(random_state=0)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

    # Create a pipeline
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('svc', SVC())
    ])

    # Fit the pipeline
    pipe.fit(X_train, y_train)

    # Evaluate the pipeline
    print(pipe.score(X_test, y_test))

In this example, the data is first scaled using `StandardScaler`, then
passed to the `SVC` classifier for training.

Setting Parameters in a Pipeline
--------------------------------

You can set or modify parameters of individual steps in a pipeline using
the `'__'` syntax. For example, to change the regularization parameter `C`
of the `SVC` classifier:

.. code-block:: python

    pipe.set_params(svc__C=10)
    pipe.fit(X_train, y_train)
    print(pipe.score(X_test, y_test))

This syntax allows you to tune hyperparameters of any step in the pipeline.

Caching Transformers
--------------------

If some steps in your pipeline are computationally expensive, you can
cache their results using the `memory` parameter. This is especially
useful when doing cross-validation or grid search.

.. code-block:: python

    from tempfile import mkdtemp
    from shutil import rmtree

    # Create a temporary directory for caching
    cachedir = mkdtemp()
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('svc', SVC())
    ], memory=cachedir)

    pipe.fit(X_train, y_train)

    # Clean up the cache directory
    rmtree(cachedir)

Note that caching only applies to transformers, not the final estimator.

Accessing Steps in a Pipeline
-----------------------------

You can access individual steps in a pipeline using the `named_steps`
attribute:

.. code-block:: python

    scaler = pipe.named_steps['scaler']
    svc = pipe.named_steps['svc']

Alternatively, you can access steps by index using the `steps` attribute:

.. code-block:: python

    scaler = pipe.steps[0][1]
    svc = pipe.steps[1][1]

Visualizing Pipelines
---------------------

Pipelines can be visualized in Jupyter notebooks or other environments
that support HTML rendering. This can help you understand the structure
of your pipeline:

.. code-block:: python

    from sklearn import set_config
    set_config(display='diagram')
    pipe

This will display a diagram of the pipeline structure.

Using `make_pipeline`
---------------------

For a more concise syntax, you can use the :func:`~sklearn.pipeline.make_pipeline`
function, which automatically generates step names based on the class names:

.. code-block:: python

    from sklearn.pipeline import make_pipeline

    pipe = make_pipeline(StandardScaler(), SVC())
    pipe.fit(X_train, y_train)
    print(pipe.score(X_test, y_test))

In this case, the steps are named `'standardscaler'` and `'svc'`.

Conclusion
----------

Pipelines are a fundamental tool in scikit-learn for building robust and
maintainable machine learning workflows. They help prevent data leakage,
simplify hyperparameter tuning, and make your code more readable.

For more advanced usage, see the :ref:`User Guide <pipeline>` and the
examples:

- :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`
- :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py`