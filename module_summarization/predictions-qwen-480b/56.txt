Mixed Generalized Linear Models
===============================

Bayesian inference for generalized linear mixed models.

Currently only families without additional scale or shape parameters
are supported (binomial and Poisson).

Two estimation approaches are supported: Laplace approximation
('maximum a posteriori'), and variational Bayes (mean field
approximation to the posterior distribution).

All realizations of random effects are modeled to be mutually
independent in this implementation.

The `exog_vc` matrix is the design matrix for the random effects.
Every column of `exog_vc` corresponds to an independent realization of
a random effect.  These random effects have mean zero and an unknown
standard deviation.  The standard deviation parameters are constrained
to be equal within subsets of the columns. When not using formulas,
these subsets are specified through the parameter `ident`.  `ident`
must have the same length as the number of columns of `exog_vc`, and
two columns whose `ident` values are equal have the same standard
deviation.  When formulas are used, the columns of `exog_vc` derived
from a common formula are constrained to have the same standard
deviation.

In many applications, `exog_vc` will be sparse.  A sparse matrix may
be passed when constructing a model class.  If a dense matrix is
passed, it will be converted internally to a sparse matrix.  There
currently is no way to avoid creating a temporary dense version of
`exog_vc` when using formulas.

Model and parameterization
--------------------------
The joint density of data and parameters factors as:

.. math::

    p(y | vc, fep) p(vc | vcp) p(vcp) p(fe)

The terms :math:`p(vcp)` and :math:`p(fe)` are prior distributions
that are taken to be Gaussian (the :math:`vcp` parameters are log
standard deviations so the standard deviations have log-normal
distributions).  The random effects distribution :math:`p(vc | vcp)`
is independent Gaussian (random effect realizations are independent
within and between values of the `ident` array).  The model
:math:`p(y | vc, fep)` depends on the specific GLM being fit.

Examples
--------
Fit a binomial mixed GLM using the Laplace approximation:

.. code-block:: python

    import statsmodels.api as sm
    import numpy as np

    # Generate data
    np.random.seed(1234)
    n = 200
    x = np.random.normal(size=(n, 2))
    z = np.random.normal(size=(n, 2))
    ident = np.array([0, 1])
    vcp = np.array([1.0, 2.0])
    vc = np.random.normal(size=(2,)) * vcp
    lp = x.sum(1) + np.dot(z, vc)
    pr = 1 / (1 + np.exp(-lp))
    y = np.random.binomial(1, pr, size=n)

    # Fit the model
    model = sm.MixedGLM(y, x, z, ident=ident, family=sm.families.Binomial())
    result = model.fit_map()
    print(result.summary())

References
----------
- Introduction to generalized linear mixed models:
  https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models

- SAS documentation:
  https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_intromix_a0000000215.htm

- An assessment of estimation methods for generalized linear mixed
  models with binary outcomes:
  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3866838/