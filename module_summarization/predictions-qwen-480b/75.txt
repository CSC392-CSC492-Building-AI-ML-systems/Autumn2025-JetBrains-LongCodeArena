Working with text data
======================

.. currentmodule:: sklearn.feature_extraction.text

The :mod:`sklearn.feature_extraction.text` submodule provides utilities to convert text documents into numerical feature vectors that can be used with machine learning algorithms. This tutorial will guide you through the main tools and techniques for working with text data in scikit-learn.

Preprocessing text
------------------

Before converting text to numerical features, it's often necessary to preprocess the text. Several utility functions are provided for common preprocessing tasks:

Stripping accents
^^^^^^^^^^^^^^^^^

Two functions are available for handling accented characters:

- :func:`strip_accents_unicode`: Transforms accentuated unicode symbols into their simple counterpart
- :func:`strip_accents_ascii`: Transforms accentuated unicode symbols into ASCII or nothing

Example::

    from sklearn.feature_extraction.text import strip_accents_unicode, strip_accents_ascii

    text = "Café résumé"
    print(strip_accents_unicode(text))  # Output: Cafe resume
    print(strip_accents_ascii(text))     # Output: Cafe resume

Stripping HTML tags
^^^^^^^^^^^^^^^^^^^

For basic HTML/XML tag removal, use :func:`strip_tags`::

    from sklearn.feature_extraction.text import strip_tags

    html_text = "<p>This is <b>bold</b> text</p>"
    print(strip_tags(html_text))  # Output: " This is  bold  text "

Text vectorizers
----------------

The main components for converting text to numerical features are the vectorizer classes:

CountVectorizer
^^^^^^^^^^^^^^^

:class:`CountVectorizer` converts a collection of text documents to a matrix of token counts::

    from sklearn.feature_extraction.text import CountVectorizer

    corpus = [
        'This is the first document.',
        'This document is the second document.',
        'And this is the third one.',
        'Is this the first document?',
    ]

    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(corpus)
    print(X.toarray())

The vectorizer creates a vocabulary from all unique tokens and represents each document as a vector of token frequencies.

TfidfVectorizer
^^^^^^^^^^^^^^^

:class:`TfidfVectorizer` combines the functionality of :class:`CountVectorizer` with :class:`TfidfTransformer` to convert raw documents into TF-IDF (Term Frequency-Inverse Document Frequency) vectors::

    from sklearn.feature_extraction.text import TfidfVectorizer

    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(corpus)
    print(X.shape)

HashingVectorizer
^^^^^^^^^^^^^^^^^

:class:`HashingVectorizer` converts text to a matrix of token occurrences using the hashing trick, which is memory efficient and allows for online learning::

    from sklearn.feature_extraction.text import HashingVectorizer

    vectorizer = HashingVectorizer(n_features=2**4)
    X = vectorizer.fit_transform(corpus)
    print(X.shape)

Common parameters
^^^^^^^^^^^^^^^^^

All vectorizers share common parameters:

- ``analyzer``: Specifies how to extract features from documents
- ``ngram_range``: The lower and upper boundary of the range of n-values for different n-grams
- ``stop_words``: Words to be removed from the documents
- ``lowercase``: Convert all characters to lowercase
- ``preprocessor``: Override the preprocessing step
- ``tokenizer``: Override the string tokenization step

Example with custom parameters::

    vectorizer = CountVectorizer(
        analyzer='word',
        ngram_range=(1, 2),
        stop_words='english',
        lowercase=True
    )

Stop words
----------

Stop words are common words that are often filtered out during text processing. The module provides a built-in list of English stop words::

    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

    print(list(ENGLISH_STOP_WORDS)[:10])

You can also provide your own list of stop words or use 'english' to use the built-in list.

N-grams
-------

N-grams are contiguous sequences of n items from a given sample of text. Vectorizers support n-grams through the ``ngram_range`` parameter::

    # Unigrams and bigrams
    vectorizer = CountVectorizer(ngram_range=(1, 2))

TF-IDF transformation
---------------------

:class:`TfidfTransformer` transforms a count matrix to a normalized tf or tf-idf representation::

    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

    # First get counts
    count_vectorizer = CountVectorizer()
    counts = count_vectorizer.fit_transform(corpus)

    # Then apply TF-IDF transformation
    tfidf_transformer = TfidfTransformer()
    tfidf = tfidf_transformer.fit_transform(counts)

This is equivalent to using :class:`TfidfVectorizer` directly.

Best practices
--------------

1. **Preprocessing**: Consider your specific use case when choosing preprocessing steps
2. **Vocabulary size**: Large vocabularies can lead to high memory usage and sparse matrices
3. **N-grams**: Using n-grams can capture local context but increases feature dimensionality
4. **Stop words**: Removing stop words can reduce noise but may also remove important context
5. **Memory efficiency**: For large datasets, consider using :class:`HashingVectorizer`

Example pipeline
----------------

Here's a complete example showing how to work with text data::

    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.pipeline import Pipeline

    # Sample data
    texts = [
        "I love this movie",
        "This film is terrible",
        "Great acting and plot",
        "Awful movie, waste of time"
    ]
    labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative

    # Create pipeline
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(stop_words='english')),
        ('classifier', MultinomialNB())
    ])

    # Train
    pipeline.fit(texts, labels)

    # Predict
    new_texts = ["This movie is amazing", "I hate this film"]
    predictions = pipeline.predict(new_texts)

This approach combines text vectorization with classification in a clean, reusable pipeline.