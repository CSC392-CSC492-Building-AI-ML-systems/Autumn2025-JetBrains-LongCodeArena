Gaussian Processes
==================

.. currentmodule:: sklearn.gaussian_process

Gaussian Processes (GPs) are a powerful tool for regression and classification tasks in machine learning. They provide a probabilistic framework that allows for uncertainty quantification in predictions, making them particularly useful in scenarios where understanding the confidence of predictions is important.

Introduction
------------

A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution. In the context of machine learning, GPs are used to define a distribution over functions. This distribution is characterized by a mean function and a covariance function (also known as a kernel). The kernel function encodes assumptions about the function being modeled, such as smoothness, periodicity, or other structural properties.

Gaussian Process Regression
---------------------------

Gaussian Process Regression (GPR) is a non-parametric regression method that uses a Gaussian Process as a prior distribution over functions. Given a dataset of input-output pairs, GPR provides a posterior distribution over functions that is consistent with the observed data. This posterior distribution can then be used to make predictions at new input points, along with uncertainty estimates.

The key components of GPR are:

1. **Mean Function**: Defines the expected value of the function at any point.
2. **Kernel Function**: Defines the covariance between function values at different points.
3. **Noise Model**: Accounts for noise in the observed outputs.

Gaussian Process Classification
-------------------------------

Gaussian Process Classification (GPC) extends the concept of GPR to classification tasks. Instead of modeling the output directly, GPC models the latent function that maps inputs to class probabilities. The latent function is assumed to follow a Gaussian Process, and a likelihood function is used to relate the latent function to the observed class labels.

The main difference from GPR is that GPC typically requires approximate inference methods, such as Laplace approximation or expectation propagation, due to the non-Gaussian likelihood associated with classification.

Kernels
-------

Kernels play a crucial role in Gaussian Processes by defining the covariance structure of the functions. The choice of kernel can significantly impact the performance and interpretability of the model. Scikit-learn provides a variety of kernels that can be combined to create more complex covariance structures.

All kernels in scikit-learn allow for kernel-engineering, meaning they can be combined via the "+" and "*" operators or exponentiated with a scalar via "**". These operations enable the creation of custom kernels tailored to specific problem domains.

Hyperparameter Optimization
---------------------------

Gaussian Processes allow for analytic gradient-based hyperparameter optimization. The space of hyperparameters can be specified by giving lower and upper boundaries for each hyperparameter, defining a rectangular search space. Alternatively, hyperparameters can be declared as "fixed," excluding them from optimization.

This flexibility in hyperparameter specification allows for fine-tuning of the kernel to the data, potentially leading to better model performance.

Examples
--------

For practical examples of using Gaussian Processes in scikit-learn, please refer to the following:

- :ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_noisy_targets.py`
- :ref:`sphx_glr_auto_examples_gaussian_process_plot_gpc_iris.py`

These examples demonstrate how to use Gaussian Process Regression and Classification on real datasets, including how to specify kernels and optimize hyperparameters.

API Reference
-------------

For detailed information on the classes and functions provided by the Gaussian Process module, please refer to the following sections:

- :class:`GaussianProcessRegressor`
- :class:`GaussianProcessClassifier`
- :mod:`sklearn.gaussian_process.kernels`