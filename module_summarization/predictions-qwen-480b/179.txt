Cross Decomposition
===================

.. currentmodule:: sklearn.cross_decomposition

The :mod:`sklearn.cross_decomposition` module includes algorithms that perform
cross decomposition, which is a method for finding linear relationships between
two datasets. These methods are particularly useful when dealing with high-dimensional
data or when there is multicollinearity among variables.

The main algorithms implemented in this module are Partial Least Squares (PLS)
and Canonical Correlation Analysis (CCA) variants. These techniques are used for
both regression and dimensionality reduction purposes.

Available estimators
--------------------

.. autosummary::
   :toctree: generated/
   :template: class.rst

   PLSCanonical
   PLSRegression
   PLSSVD

Partial Least Squares (PLS)
---------------------------

Partial Least Squares is a statistical method that bears some similarity to
principal components regression; instead of finding hyperplanes of maximum
variance between the response and independent variables, it finds a linear
regression model by projecting the predicted variables and the observable
variables to a new space.

The PLS approach is particularly useful when the number of variables is large
compared to the number of observations, and when there is multicollinearity
among the predictor variables.

PLS Canonical
~~~~~~~~~~~~~

:class:`PLSCanonical` implements the PLS canonical algorithm which is equivalent
to CCA (Canonical Correlation Analysis) when using the 'B' mode. It finds the
linear relationship between two multidimensional datasets by maximizing the
correlation between the projections of the datasets.

PLS Regression
~~~~~~~~~~~~~~

:class:`PLSRegression` implements the PLS regression algorithm. It is used to
predict a dependent variable or set of dependent variables from a set of
independent variables. PLS regression is particularly useful when the independent
variables are highly collinear or when there are more variables than observations.

PLS SVD
~~~~~~~

:class:`PLSSVD` implements a simplified version of PLS using Singular Value
Decomposition. It is a faster but less flexible alternative to the other PLS
methods, suitable for basic PLS applications.

Mathematical Background
-----------------------

Cross decomposition methods work by finding weight vectors that maximize the
covariance between the projections of two datasets X and Y. The basic algorithm
can be described as follows:

1. Find weight vectors w and c such that the covariance between Xw and Yc is maximized
2. Compute the scores: t = Xw and u = Yc
3. Deflate the matrices X and Y
4. Repeat for the desired number of components

The specific implementation details vary between the different algorithms,
particularly in how the weight vectors are computed and normalized.

Examples
--------

>>> from sklearn.cross_decomposition import PLSRegression
>>> import numpy as np
>>> X = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])
>>> Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])
>>> pls2 = PLSRegression(n_components=2)
>>> pls2.fit(X, Y)
PLSRegression(n_components=2)
>>> Y_pred = pls2.predict(X)

See also
--------

For more information on cross decomposition methods, see:

- Wegelin, J.A. (2000). A Survey of Partial Least Squares (PLS) Methods, with Emphasis on the Two-Block Case.
- Abdi, H. (2010). Partial least squares regression and projection on latent space regression. Wiley Interdisciplinary Reviews: Computational Statistics, 2(1), 97-106.