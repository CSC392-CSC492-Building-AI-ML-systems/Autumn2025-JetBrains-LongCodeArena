Handling Large Datasets
=======================

When working with large datasets, such as spectral cubes in radio astronomy, memory management and computational efficiency become critical. The ``spectral-cube`` package provides several strategies for computing moment maps and handling data that is too large to fit into memory.

Strategies for Moment Map Computation
-------------------------------------

The package implements three main strategies for computing moment maps, each with different trade-offs between memory usage and speed:

1. **Cubewise**: Processes the entire cube at once. This is the fastest method but requires that the entire dataset fits into memory.

2. **Slicewise**: Processes the cube one slice at a time along the specified axis. This reduces memory usage at the cost of speed.

3. **Raywise**: Processes the cube one ray (line of sight) at a time. This is the most memory-efficient method, especially useful for very large datasets.

The ``moment_auto`` function automatically selects the most appropriate strategy based on the size of the cube and available memory.

Memory-Efficient Data Access
----------------------------

For large datasets, the ``StokesSpectralCube`` class provides mechanisms to handle data that may not fit entirely in memory:

- **Lazy loading**: Data is loaded only when needed for computation.
- **Chunked processing**: Large operations are broken down into smaller chunks that can be processed individually.
- **Memory mapping**: Direct access to data on disk without loading it entirely into RAM.

The ``_get_filled_data`` method allows for efficient data access with proper handling of missing or invalid values, ensuring that computations can proceed without interruption.

Stokes Cube Handling
--------------------

The ``StokesSpectralCube`` class is designed to handle multi-component data (Stokes parameters I, Q, U, V, etc.) efficiently. It supports:

- Shared masks across all Stokes components to reduce memory overhead.
- Component-specific masks for detailed control.
- Broadcasting of masks to accommodate different spatial dimensions.

This design allows for efficient processing of polarimetric data without duplicating mask information unnecessarily.

Performance Considerations
--------------------------

When working with large datasets, consider the following:

1. Use ``moment_auto`` for automatic optimization of computation strategy.
2. For custom processing, prefer ``moment_raywise`` for memory efficiency or ``moment_cubewise`` for speed when memory allows.
3. Utilize the built-in masking system to avoid processing invalid data.
4. Take advantage of the lazy evaluation system to minimize unnecessary computations.

The package automatically handles the trade-offs between memory usage and computational speed, making it suitable for both small exploratory datasets and large production-scale data cubes.