Imputation of Missing Values
============================

.. currentmodule:: sklearn.impute

Many real-world datasets contain missing values, often encoded as blanks,
NaNs or other placeholders. Such datasets, however, are incompatible with
scikit-learn estimators which assume that all values in an array are
numerical, and hold meaning. A basic strategy to use incomplete datasets
is to discard entire rows and/or columns containing missing values.
However, this comes at the price of losing data which may be valuable
(even if incomplete). A better strategy is to impute the missing values,
i.e., to infer them from the known part of the data. See
:ref:`sphx_glr_auto_examples_impute_plot_missing_values.py` for a
comparison of different imputation strategies.

Univariate vs. Multivariate Imputation
--------------------------------------

One type of imputation algorithm is univariate, which imputes values in the
``i``-th feature dimension using only non-missing values in that feature
``i`` (e.g., :class:`SimpleImputer`). In contrast, multivariate imputation
algorithms use the entire set of available feature dimensions to estimate the
missing values (e.g., :class:`IterativeImputer`).

Univariate Imputation
---------------------

The :class:`SimpleImputer` class provides basic strategies for imputing
missing values. Missing values can be imputed with a provided constant value,
or using the statistics (mean, median, or most frequent) of each column in
which the missing values are located. This class also allows for different
missing values to be used for different features through the
``missing_values`` parameter.

The following snippet demonstrates how to replace missing values,
encoded as ``np.nan``, using the mean value of the columns (axis 0)
that contain the missing values::

    >>> import numpy as np
    >>> from sklearn.impute import SimpleImputer
    >>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')
    >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
    SimpleImputer()
    >>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
    >>> print(imp.transform(X))
    [[4.         2.        ]
     [6.         3.666...]
     [7.         6.        ]]

The :class:`SimpleImputer` class also supports sparse matrices::

    >>> import scipy.sparse as sp
    >>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])
    >>> imp = SimpleImputer(missing_values=0, strategy='mean')
    >>> imp.fit(X)
    SimpleImputer(missing_values=0)
    >>> X_test = sp.csc_matrix([[0, -1], [6, 0], [7, 0]])
    >>> print(imp.transform(X_test).toarray())
    [[6.  -1. ]
     [6.   1.5]
     [7.   1.5]]

Note that this format is not meant to be used to implicitly store missing
values in the matrix because it would densify it at transform time. Missing
values encoded by 0 must be used with sparse input.

When the :class:`SimpleImputer` is used with the ``'constant'`` strategy,
the missing values are replaced by a user-provided value::

    >>> import numpy as np
    >>> from sklearn.impute import SimpleImputer
    >>> imp = SimpleImputer(strategy="constant", fill_value=999)
    >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
    SimpleImputer(fill_value=999, strategy='constant')
    >>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
    >>> print(imp.transform(X))
    [[999.   2.]
     [  6. 999.]
     [  7.   6.]]

:class:`SimpleImputer` can also be used in conjunction with
:class:`pandas.CategoricalData`. For this, use the string ``"??"`` as
missing values, and set ``strategy`` to ``"most_frequent"``::

    >>> import pandas as pd
    >>> from sklearn.impute import SimpleImputer
    >>> df = pd.DataFrame([["a", "x"],
    ...                    [np.nan, "y"],
    ...                    ["a", np.nan],
    ...                    ["b", "y"]], dtype="category")
    >>> imp = SimpleImputer(strategy="most_frequent")
    >>> print(imp.fit_transform(df))
    [['a' 'x']
     ['a' 'y']
     ['a' 'y']
     ['b' 'y']]

:class:`SimpleImputer` can also impute categorical data represented as
strings or object dtypes. In this case, the strategy should be
``"most_frequent"`` or ``"constant"``::

    >>> import numpy as np
    >>> from sklearn.impute import SimpleImputer
    >>> df = pd.DataFrame([["a", "x"],
    ...                    [np.nan, "y"],
    ...                    ["a", np.nan],
    ...                    ["b", "y"]])
    >>> imp = SimpleImputer(strategy="most_frequent")
    >>> print(imp.fit_transform(df))
    [['a' 'x']
     ['a' 'y']
     ['a' 'y']
     ['b' 'y']]

:class:`SimpleImputer` supports pandas' nullable integer data types
(e.g., ``Int64``) and nullable boolean data types (e.g., ``boolean``).
Missing values in these dtypes can be encoded as either ``np.nan`` or
``pd.NA``::

    >>> import pandas as pd
    >>> from sklearn.impute import SimpleImputer
    >>> df = pd.DataFrame({"feature": pd.array([1, 2, None], dtype="Int64")})
    >>> imp = SimpleImputer(missing_values=pd.NA, strategy="mean")
    >>> print(imp.fit_transform(df))
    [[1. ]
     [2. ]
     [1.5]]

:class:`SimpleImputer` also supports imputing data with ``pandas.NA`` as
the missing value indicator when the data is numeric::

    >>> import pandas as pd
    >>> from sklearn.impute import SimpleImputer
    >>> df = pd.DataFrame({"feature": pd.array([1.0, 2.0, None], dtype="Float64")})
    >>> imp = SimpleImputer(missing_values=pd.NA, strategy="mean")
    >>> print(imp.fit_transform(df))
    [[1. ]
     [2. ]
     [1.5]]

:class:`SimpleImputer` can be used in a Pipeline to impute missing values
before fitting a model::

    >>> from sklearn.pipeline import Pipeline
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.impute import SimpleImputer
    >>> from sklearn.model_selection import train_test_split
    >>> from sklearn.datasets import make_classification
    >>> X, y = make_classification(n_samples=1000, n_features=4,
    ...                            n_informative=2, n_redundant=0,
    ...                            random_state=0, shuffle=False)
    >>> # Introduce missing values
    >>> X[0, 0] = np.nan
    >>> X[1, 1] = np.nan
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
    ...                                                     random_state=0)
    >>> pipe = Pipeline([('imputer', SimpleImputer(strategy='mean')),
    ...                  ('classifier', LogisticRegression())])
    >>> pipe.fit(X_train, y_train)
    Pipeline(steps=[('imputer', SimpleImputer()), ('classifier', LogisticRegression())])
    >>> print(pipe.score(X_test, y_test))
    0.9...

:class:`SimpleImputer` also supports adding a binary indicator matrix
to mark the imputed values. This can be enabled by setting
``add_indicator=True``::

    >>> import numpy as np
    >>> from sklearn.impute import SimpleImputer
    >>> X = np.array([[1, 2], [np.nan, 3], [7, 6]])
    >>> imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)
    >>> imp.fit(X)
    SimpleImputer(add_indicator=True)
    >>> X_test = np.array([[np.nan, 2], [6, np.nan], [7, 6]])
    >>> print(imp.transform(X_test))
    [[4.         2.         1.         0.        ]
     [6.         3.666... 0.         1.        ]
     [7.         6.         0.         0.        ]]

The indicator matrix is a binary matrix where 1 indicates that the value
was missing in the original dataset.

Multivariate Imputation
-----------------------

Multivariate imputation algorithms use the entire set of available feature
dimensions to estimate the missing values. Currently, scikit-learn provides
the :class:`IterativeImputer` class, which implements