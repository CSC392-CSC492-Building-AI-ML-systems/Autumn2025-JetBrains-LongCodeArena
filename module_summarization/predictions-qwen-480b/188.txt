Kernel Approximation
====================

.. currentmodule:: sklearn.kernel_approximation

This module implements several approximate kernel feature maps based on Fourier transforms and Count Sketches. These methods allow for efficient computation of kernel methods by approximating the feature mappings that correspond to certain kernels, enabling the use of linear models in place of more computationally expensive kernel methods.

PolynomialCountSketch
---------------------

.. autoclass:: PolynomialCountSketch
   :members:
   :inherited-members:
   :special-members: __init__

The `PolynomialCountSketch` class implements Tensor Sketch, which approximates the feature map of the polynomial kernel:

.. math:: K(X, Y) = (\gamma \langle X, Y \rangle + \text{coef0})^\text{degree}

by efficiently computing a Count Sketch of the outer product of a vector with itself using Fast Fourier Transforms (FFT).

Other Kernel Approximation Methods
----------------------------------

In addition to `PolynomialCountSketch`, scikit-learn provides several other classes for kernel approximation:

- :class:`AdditiveChi2Sampler`: Approximate feature map for additive chi2 kernel.
- :class:`Nystroem`: Approximate a kernel map using a subset of the training data.
- :class:`RBFSampler`: Approximate a RBF kernel feature map using random Fourier features.
- :class:`SkewedChi2Sampler`: Approximate feature map for "skewed chi-squared" kernel.

See Also
--------
- :func:`sklearn.metrics.pairwise.kernel_metrics`: List of built-in kernels.

Examples
--------

Using `PolynomialCountSketch` with a linear classifier:

.. code-block:: python

    from sklearn.kernel_approximation import PolynomialCountSketch
    from sklearn.linear_model import SGDClassifier
    
    X = [[0, 0], [1, 1], [1, 0], [0, 1]]
    y = [0, 0, 1, 1]
    
    ps = PolynomialCountSketch(degree=3, random_state=1)
    X_features = ps.fit_transform(X)
    
    clf = SGDClassifier(max_iter=10, tol=1e-3)
    clf.fit(X_features, y)
    
    print(clf.score(X_features, y))  # Output: 1.0

These kernel approximation methods are particularly useful when working with large datasets where computing the full kernel matrix would be computationally prohibitive. By approximating the kernel feature map, we can use efficient linear models while still capturing non-linear relationships in the data.