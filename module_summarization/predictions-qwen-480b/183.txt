Feature Selection and Dimensionality Reduction
===============================================

.. currentmodule:: sklearn.feature_selection

Feature selection and dimensionality reduction are essential techniques in
machine learning used to reduce the number of input variables in a dataset.
These methods help improve model performance, reduce overfitting, decrease
training time, and enhance interpretability by focusing on the most relevant
features.

Why Perform Feature Selection?
------------------------------

- **Improved Model Performance**: Removing irrelevant or redundant features
  can improve a model's ability to generalize.
- **Reduced Overfitting**: Fewer features mean less noise and fewer chances
  for the model to learn spurious patterns.
- **Faster Training**: Algorithms train faster on datasets with fewer features.
- **Enhanced Interpretability**: Simpler models are easier to understand and explain.

Types of Feature Selection
--------------------------

1. **Univariate Selection**: Statistical tests are used to select features
   that have the strongest relationship with the target variable.

2. **Recursive Feature Elimination (RFE)**: A model is trained, and the least
   important features are removed iteratively.

3. **Feature Importance**: Tree-based models provide a measure of feature
   importance which can be used for selection.

4. **Variance Threshold**: Features with low variance are removed as they
   contribute little to the model.

5. **Select From Model**: Uses a modelâ€™s coefficients or feature importance
   to select features.

Scikit-learn provides several tools for feature selection and dimensionality
reduction, including:

- :class:`SelectKBest`: Selects the top `k` features based on univariate
  statistical tests.
- :class:`SelectPercentile`: Selects a percentage of the best features.
- :class:`RFE`: Recursive feature elimination.
- :class:`RFECV`: RFE with cross-validated selection of the optimal number
  of features.
- :class:`VarianceThreshold`: Removes features with low variance.
- :class:`SelectFromModel`: Selects features based on importance weights.

Example: Univariate Feature Selection
-------------------------------------

.. code-block:: python

    from sklearn.feature_selection import SelectKBest, f_classif
    from sklearn.datasets import load_iris

    X, y = load_iris(return_X_y=True)
    selector = SelectKBest(score_func=f_classif, k=2)
    X_new = selector.fit_transform(X, y)

Example: Recursive Feature Elimination
--------------------------------------

.. code-block:: python

    from sklearn.feature_selection import RFE
    from sklearn.linear_model import LogisticRegression

    estimator = LogisticRegression()
    selector = RFE(estimator, n_features_to_select=2)
    X_new = selector.fit_transform(X, y)

Example: Feature Importance with Trees
--------------------------------------

.. code-block:: python

    from sklearn.ensemble import RandomForestClassifier
    from sklearn.feature_selection import SelectFromModel

    estimator = RandomForestClassifier()
    selector = SelectFromModel(estimator, prefit=False)
    X_new = selector.fit_transform(X, y)

Dimensionality Reduction Techniques
-----------------------------------

In addition to feature selection, scikit-learn offers dimensionality reduction
techniques such as:

- :class:`PCA`: Principal Component Analysis.
- :class:`TruncatedSVD`: Dimensionality reduction using truncated SVD.
- :class:`NMF`: Non-negative Matrix Factorization.

These methods transform the data into a lower-dimensional space while
preserving as much variance as possible.

Example: Principal Component Analysis
-------------------------------------

.. code-block:: python

    from sklearn.decomposition import PCA

    pca = PCA(n_components=2)
    X_reduced = pca.fit_transform(X)

Conclusion
----------

Feature selection and dimensionality reduction are powerful tools for
preprocessing data and improving machine learning models. By carefully
selecting or transforming features, you can build models that are both
efficient and effective. Scikit-learn provides a wide range of methods
to help you achieve this goal.