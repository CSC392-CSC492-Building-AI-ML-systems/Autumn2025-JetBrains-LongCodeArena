Transforming the Prediction Target
==================================

Many machine learning algorithms make assumptions about the format of the target variable. Often, it is necessary to transform the target variable before training a model. Scikit-learn provides utilities and transformers to facilitate such transformations.

While scikit-learn's preprocessing tools are primarily focused on feature transformation, similar principles apply when transforming the target variable (also known as the dependent variable or label). Target transformation can be essential for:

- Handling categorical targets in classification tasks.
- Normalizing or scaling regression targets.
- Encoding labels into numerical formats.

Although scikit-learn does not include a dedicated module for target transformation in the same way it does for feature preprocessing, several utilities and transformers can be used effectively for this purpose.

Label Encoding for Classification
---------------------------------

In classification problems, the target variable is often represented as strings or other non-numeric types. To make these compatible with machine learning algorithms, they need to be encoded into numerical values.

The :class:`sklearn.preprocessing.OrdinalEncoder` and :class:`sklearn.preprocessing.OneHotEncoder` classes can be used to encode categorical targets. However, for a simple label encoding (i.e., mapping each class to an integer), you can use :class:`sklearn.preprocessing.LabelEncoder`.

Example:

.. code-block:: python

    from sklearn.preprocessing import LabelEncoder

    # Original target labels
    y = ['cat', 'dog', 'cat', 'bird']

    # Encode labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    print(y_encoded)  # Output: [0 1 0 2]

Inverse transformation can also be performed to retrieve the original labels:

.. code-block:: python

    y_original = le.inverse_transform(y_encoded)
    print(y_original)  # Output: ['cat' 'dog' 'cat' 'bird']

Target Scaling for Regression
-----------------------------

In regression tasks, it may be beneficial to scale the target variable, especially when the algorithm is sensitive to the scale of the data (e.g., neural networks or support vector regression).

:class:`sklearn.preprocessing.StandardScaler` or :class:`sklearn.preprocessing.MinMaxScaler` can be applied to the target variable just as they are applied to features.

Example:

.. code-block:: python

    from sklearn.preprocessing import StandardScaler

    # Original target values
    y = [[100], [200], [300], [400]]

    # Scale target
    scaler = StandardScaler()
    y_scaled = scaler.fit_transform(y)

    print(y_scaled)

    # Inverse transform to get original scale
    y_original = scaler.inverse_transform(y_scaled)
    print(y_original)

Custom Target Transformers
--------------------------

For more complex transformations, you can create custom transformers by subclassing :class:`sklearn.base.BaseEstimator` and :class:`sklearn.base.TransformerMixin`. This approach allows you to define both `fit` and `transform` methods tailored to your specific needs.

Example:

.. code-block:: python

    from sklearn.base import BaseEstimator, TransformerMixin
    import numpy as np

    class LogTransformer(BaseEstimator, TransformerMixin):
        def fit(self, y, X=None):
            return self

        def transform(self, y):
            return np.log1p(y)

        def inverse_transform(self, y):
            return np.expm1(y)

This transformer can be used to apply a logarithmic transformation to the target variable, which is often useful in regression tasks with skewed target distributions.

Using Transformers in Pipelines
-------------------------------

To ensure that transformations are applied consistently during both training and prediction, it is recommended to use transformers within a :class:`sklearn.pipeline.Pipeline`. This ensures that the same transformation logic is applied to the target variable during training and inverse transformation is applied to predictions.

Example:

.. code-block:: python

    from sklearn.pipeline import Pipeline
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import StandardScaler

    # Define pipeline with target transformation
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', LinearRegression())
    ])

    # Fit and predict
    pipeline.fit(X, y)
    y_pred = pipeline.predict(X)

In this example, the target `y` is scaled before being passed to the regressor, and predictions are automatically inverse-transformed to the original scale.

Conclusion
----------

Transforming the prediction target is a crucial step in many machine learning workflows. Whether it's encoding categorical labels or scaling regression targets, scikit-learn provides flexible tools to handle these transformations effectively. By using encoders, scalers, and custom transformers, you can ensure that your model receives the data in the most suitable format for learning.