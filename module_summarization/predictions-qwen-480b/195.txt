Gaussian Mixture Models
=======================

Gaussian Mixture Models (GMMs) are a probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters. They are widely used for clustering, density estimation, and anomaly detection.

This module provides tools for learning Gaussian Mixture Models using the Expectation-Maximization (EM) algorithm.

Base Class for Mixture Models
-----------------------------

.. autoclass:: sklearn.mixture._base.BaseMixture
   :members:
   :undoc-members:
   :show-inheritance:

The `BaseMixture` class serves as the foundation for implementing various mixture models. It provides common functionality such as parameter initialization, convergence checking, and the EM algorithm framework.

Key Parameters
~~~~~~~~~~~~~~

- `n_components`: The number of mixture components.
- `tol`: The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.
- `reg_covar`: Non-negative regularization added to the diagonal of covariance matrices to ensure positive definiteness.
- `max_iter`: The maximum number of EM iterations to perform.
- `n_init`: The number of initializations to perform. The best results are kept based on the log-likelihood.
- `init_params`: The method used to initialize the weights, means, and precisions. Options include 'kmeans', 'random', 'random_from_data', and 'k-means++'.
- `warm_start`: If True, the solution of the last fitting is used as initialization for the next call of fit().
- `verbose`: Enable verbose output.
- `verbose_interval`: Number of iterations between each print message when verbose is True.

Initialization Methods
~~~~~~~~~~~~~~~~~~~~~~

The model parameters can be initialized using several strategies:

1. **kmeans**: Uses K-Means clustering to initialize the parameters.
2. **random**: Random initialization of responsibilities.
3. **random_from_data**: Randomly selects data points as initial component centers.
4. **k-means++**: Uses the K-Means++ algorithm for initialization.

EM Algorithm
~~~~~~~~~~~~

The Expectation-Maximization algorithm alternates between two steps:

1. **E-step (Expectation)**: Computes the posterior probabilities (responsibilities) of each component for each data point.
2. **M-step (Maximization)**: Updates the model parameters (weights, means, and covariances) to maximize the expected log-likelihood.

The algorithm iterates until convergence or until the maximum number of iterations is reached.

Methods
~~~~~~~

.. automethod:: sklearn.mixture._base.BaseMixture.fit

.. automethod:: sklearn.mixture._base.BaseMixture.fit_predict

The `fit` method estimates the model parameters using the EM algorithm, while `fit_predict` additionally returns the predicted component labels for the input data.