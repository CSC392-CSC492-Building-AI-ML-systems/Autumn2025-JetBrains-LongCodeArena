Kernel Ridge Regression
========================

.. currentmodule:: sklearn.kernel_ridge

:class:`KernelRidge` extends the linear ridge regression model by applying the kernel trick, allowing it to learn non-linear relationships in the data. It combines the principles of ridge regression (least squares with L2 regularization) with kernel methods to operate in high-dimensional feature spaces induced by the kernel.

The model learns a linear function in the kernel-induced space, which corresponds to a non-linear function in the original input space. This approach is particularly useful when dealing with complex, non-linear patterns that linear models cannot capture effectively.

The mathematical formulation of kernel ridge regression is similar to that of Support Vector Regression (SVR), but with key differences:

- KRR uses squared error loss, while SVR uses epsilon-insensitive loss.
- Both methods apply L2 regularization.
- Unlike SVR, KRR admits a closed-form solution, making it faster to train on medium-sized datasets.
- However, the resulting model is non-sparse, meaning prediction time can be slower than SVR, which learns sparse models when using epsilon > 0.

This estimator supports multi-output regression, where the target variable `y` can be a 2D array of shape `(n_samples, n_targets)`.

For more information, refer to the :ref:`User Guide <kernel_ridge>`.

.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_kernel_ridge_regression_001.png
   :target: ../auto_examples/miscellaneous/plot_kernel_ridge_regression.html
   :align: center
   :scale: 75%

   Comparison of kernel ridge regression and support vector regression.

Parameters
----------

.. list-table::
   :widths: 20 80
   :header-rows: 0

   * - ``alpha`` : float or array-like of shape (n_targets,), default=1.0
     - Regularization strength; must be a positive float. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific to each target.

   * - ``kernel`` : str or callable, default="linear"
     - Kernel mapping used internally. This parameter is passed directly to :func:`~sklearn.metrics.pairwise.pairwise_kernels`. If `kernel` is a string, it must be one of the metrics in `pairwise.PAIRWISE_KERNEL_FUNCTIONS` or "precomputed". If `kernel` is "precomputed", X is assumed to be a kernel matrix.

   * - ``gamma`` : float, default=None
     - Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Ignored by other kernels.

   * - ``degree`` : int, default=3
     - Degree of the polynomial kernel. Ignored by other kernels.

   * - ``coef0`` : float, default=1
     - Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.

   * - ``kernel_params`` : dict, default=None
     - Additional parameters (keyword arguments) for kernel function passed as callable object.

Attributes
----------

.. list-table::
   :widths: 20 80
   :header-rows: 0

   * - ``dual_coef_`` : ndarray of shape (n_samples,) or (n_samples, n_targets)
     - Representation of weight vector(s) in kernel space.

   * - ``X_fit_`` : {ndarray, sparse matrix} of shape (n_samples, n_features)
     - Training data, which is also required for prediction. If kernel == "precomputed", this is instead the precomputed training matrix, of shape (n_samples, n_samples).

   * - ``n_features_in_`` : int
     - Number of features seen during :term:`fit`.

   * - ``feature_names_in_`` : ndarray of shape (`n_features_in_`,)
     - Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.

See Also
--------

- :class:`sklearn.gaussian_process.GaussianProcessRegressor`: Gaussian Process regressor providing automatic kernel hyperparameters tuning and predictions uncertainty.
- :class:`sklearn.linear_model.Ridge`: Linear ridge regression.
- :class:`sklearn.linear_model.RidgeCV`: Ridge regression with built-in cross-validation.
- :class:`sklearn.svm.SVR`: Support Vector Regression accepting a large variety of kernels.

References
----------

* Kevin P. Murphy, "Machine Learning: A Probabilistic Perspective", The MIT Press, chapter 14.4.3, pp. 492-493

Examples
--------

.. code-block:: python

    from sklearn.kernel_ridge import KernelRidge
    import numpy as np

    n_samples, n_features = 10, 5
    rng = np.random.RandomState(0)
    y = rng.randn(n_samples)
    X = rng.randn(n_samples, n_features)

    krr = KernelRidge(alpha=1.0)
    krr.fit(X, y)
    KernelRidge(alpha=1.0)