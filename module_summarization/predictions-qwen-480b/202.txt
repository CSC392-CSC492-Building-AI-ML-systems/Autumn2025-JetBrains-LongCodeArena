Permutation Feature Importance
==============================

Permutation feature importance is a model inspection technique that measures the importance of each feature by calculating the increase in the model's prediction error after permuting the feature. A feature is considered "important" if shuffling its values increases the model error, because the model relied on the feature for the prediction. A feature is considered "unimportant" if shuffling its values leaves the model error unchanged, because the model ignored the feature for the prediction.

The permutation importance is defined to be the difference between the baseline score and the score obtained after permutation. This technique works for any scikit-learn estimator and can be computed efficiently using parallel computing.

.. note::

    Permutation importance is calculated on a specific dataset, typically a test set, and reflects the importance of features for that particular dataset. It may differ from the feature importance computed during training (e.g., tree-based feature importances).

Function Definition
-------------------

.. autofunction:: sklearn.inspection.permutation_importance

Parameters
----------

estimator : object
    An estimator object implementing `fit` method.

X : array-like of shape (n_samples, n_features)
    Data on which permutation importance will be computed.

y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None
    Target values (class labels in classification, real numbers in regression).

scoring : str, callable, list, tuple, or dict, default=None
    Scorer to use. If `None`, the estimator's default scorer (via `estimator.score`) is used.
    If a string, it should be a valid scorer name (see :ref:`scoring_parameter`).
    If callable, it should be a scorer object with signature `scorer(estimator, X, y)`.
    For multiple metric evaluation, a list, tuple, or dict of scorers can be passed.

n_repeats : int, default=5
    Number of times to permute a feature.

n_jobs : int, default=None
    Number of jobs to run in parallel. `None` means 1 unless in a :obj:`joblib.parallel_backend` context.
    `-1` means using all processors.

random_state : int, RandomState instance, default=None
    Pseudo-random number generator to control the permutations of each feature.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights used in scoring.

max_samples : int or float, default=1.0
    The number (or fraction) of samples to use for computing the permutation importance.
    If int, then it specifies the absolute number of samples.
    If float, then it specifies the fraction of samples, between 0 and 1.

Returns
-------

result : :class:`~sklearn.utils.Bunch` or dict of such instances
    Dictionary-like object, with the following attributes:

    importances_mean : ndarray of shape (n_features,)
        Mean of feature importance over `n_repeats`.

    importances_std : ndarray of shape (n_features,)
        Standard deviation over `n_repeats`.

    importances : ndarray of shape (n_features, n_repeats)
        Raw permutation importance scores.

    If `scoring` is a dict or a list of scorers, then `result` is a dict with the same keys as `scoring`,
    and each value is a :class:`~sklearn.utils.Bunch` object with the above attributes.

Examples
--------

The following example shows how to compute the permutation importance for a RandomForestClassifier:

.. code-block:: python

    from sklearn.datasets import make_classification
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.inspection import permutation_importance

    X, y = make_classification(n_samples=1000, n_features=5, n_informative=3,
                               n_redundant=2, random_state=0)
    clf = RandomForestClassifier(random_state=0)
    clf.fit(X, y)

    result = permutation_importance(clf, X, y, n_repeats=10, random_state=0)
    print("Importances (mean ± std):")
    for i in range(X.shape[1]):
        print(f"Feature {i}: {result.importances_mean[i]:.3f} ± {result.importances_std[i]:.3f}")

See Also
--------

:func:`sklearn.inspection.partial_dependence`: Compute the partial dependence of features.

Notes
-----

- The permutation importance is calculated using a subset of samples if `max_samples` is less than the total number of samples.
- The permutation is done in parallel using joblib, which can significantly speed up computation for large datasets.
- The feature names are automatically inferred from the input data if it is a pandas DataFrame. Otherwise, generic names like `x0`, `x1`, etc., are used.

References
----------

Breiman, L. (2001). Random Forests. *Machine Learning*, 45(1), 5–32. https://doi.org/10.1023/A:1010933404324

Fisher, R. A. (1936). The Use of Multiple Measurements in Taxonomic Problems. *Annals of Eugenics*, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x