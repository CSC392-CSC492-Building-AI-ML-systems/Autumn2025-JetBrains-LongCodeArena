Linear and Quadratic Discriminant Analysis
==========================================

.. currentmodule:: sklearn.discriminant_analysis

Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.

These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, and have proven to work well in practice. They make strong assumptions about the data distribution, assuming that the data for each class is drawn from a Gaussian distribution.

Linear Discriminant Analysis
----------------------------

Linear Discriminant Analysis (LDA) assumes that the data for each class is drawn from a Gaussian distribution with a common covariance matrix. This leads to linear decision boundaries between classes.

The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The decision boundary is linear because the log-likelihood ratio between any two classes is linear in :math:`x`.

Mathematically, for a given input vector :math:`x`, LDA computes the posterior probability of each class :math:`k` using Bayes' theorem:

.. math::

    P(Y=k|X=x) = \frac{P(X=x|Y=k)P(Y=k)}{P(X=x)}

Where:
- :math:`P(Y=k)` is the prior probability of class :math:`k`
- :math:`P(X=x|Y=k)` is the class-conditional density of :math:`x` given class :math:`k`
- :math:`P(X=x)` is the evidence

Under the Gaussian assumption with shared covariance matrix, the discriminant function for class :math:`k` becomes:

.. math::

    \delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log(\pi_k)

Where:
- :math:`\mu_k` is the mean of class :math:`k`
- :math:`\Sigma` is the shared covariance matrix
- :math:`\pi_k` is the prior probability of class :math:`k`

The class with the highest discriminant score is chosen as the prediction.

Quadratic Discriminant Analysis
-------------------------------

Quadratic Discriminant Analysis (QDA) is similar to LDA but assumes that each class has its own covariance matrix. This leads to quadratic decision boundaries between classes.

In QDA, the discriminant function for class :math:`k` becomes:

.. math::

    \delta_k(x) = -\frac{1}{2} \log(|\Sigma_k|) - \frac{1}{2}(x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k) + \log(\pi_k)

Where:
- :math:`\Sigma_k` is the covariance matrix for class :math:`k`

Since each class has its own covariance matrix, the decision boundaries are quadratic functions of :math:`x`.

Comparison between LDA and QDA
------------------------------

The main difference between LDA and QDA lies in their assumptions about the covariance structure:

1. **LDA** assumes all classes share the same covariance matrix, leading to linear decision boundaries. This makes LDA a more constrained model that is less prone to overfitting, especially when the training set is small or the number of features is high.

2. **QDA** allows each class to have its own covariance matrix, leading to quadratic decision boundaries. This makes QDA more flexible but also more prone to overfitting, especially with limited training data.

In practice:
- Use LDA when you have limited training data or when you believe the assumption of shared covariance is reasonable
- Use QDA when you have sufficient training data and suspect that different classes have different covariance structures

Both methods work best when the classes are well-separated and the data is approximately normally distributed within each class.

Implementation Details
----------------------

Both LDA and QDA in scikit-learn support multiple solvers and can handle multi-class classification problems naturally. They also support shrinkage estimation of the covariance matrix, which can improve performance when the number of training samples is small compared to the number of features.

The implementations provide methods for:
- Fitting the model to training data
- Making predictions on new data
- Computing class probabilities
- Accessing the learned parameters (means, covariances, priors)

These classifiers are particularly useful as baseline methods and often perform surprisingly well on many real-world datasets, especially in their simplicity and interpretability.