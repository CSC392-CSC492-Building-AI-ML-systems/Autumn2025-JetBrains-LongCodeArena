.. _outlier_detection:

=====================
Novelty and Outlier Detection
=====================

.. currentmodule:: sklearn

Many machine learning tasks require the detection of *outliers* or *novelties* in datasets. While both concepts are related, they differ in their approach and assumptions:

- **Outlier Detection** (also known as *anomaly detection*) is the task of identifying observations that deviate significantly from the rest of the data. It is typically performed in an unsupervised manner, assuming that the training data may already contain outliers.

- **Novelty Detection** assumes that the training data is free of outliers and aims to detect whether new observations are outliers (novelties) with respect to the training data.

Scikit-learn provides tools for both tasks, allowing users to build robust models that can identify abnormal behavior in data.

.. _outlier_detection_methods:

Methods
========

Scikit-learn includes several estimators for outlier and novelty detection:

- :class:`ensemble.IsolationForest`: Isolation Forest algorithm, which isolates anomalies instead of profiling normal data points.
- :class:`neighbors.LocalOutlierFactor`: Unsupervised Outlier Detection using Local Outlier Factor (LOF).
- :class:`svm.OneClassSVM`: Unsupervised Outlier Detection using One-Class SVM.
- :class:`covariance.EllipticEnvelope`: For detecting outliers in Gaussian-distributed data using robust covariance estimation.

Each method has its own assumptions and is suitable for different types of data and outlier patterns.

.. _outlier_detection_usage:

Usage
=====

Outlier detection estimators in scikit-learn implement a ``fit_predict`` method that returns ``-1`` for outliers and ``1`` for inliers:

.. code-block:: python

    from sklearn.ensemble import IsolationForest
    import numpy as np

    # Sample data
    X = np.array([[1, 2], [2, 3], [10, 10], [3, 2]])

    # Initialize and fit the model
    clf = IsolationForest(contamination=0.1)
    y_pred = clf.fit_predict(X)

    print(y_pred)  # [-1  1  1  1] - First point is detected as outlier

For novelty detection, where the training set is assumed to be clean, the ``predict`` method is used on new data after fitting on the training set:

.. code-block:: python

    # Training data (assumed clean)
    X_train = np.array([[1, 2], [2, 3], [3, 2]])

    # New data to test
    X_test = np.array([[10, 10], [2, 2]])

    # Fit on clean training data
    clf = IsolationForest(contamination=0.1)
    clf.fit(X_train)

    # Predict on new data
    y_pred = clf.predict(X_test)

    print(y_pred)  # [-1  1] - First test point is a novelty

.. _outlier_detection_metrics:

Evaluation Metrics
==================

Evaluating outlier and novelty detection models can be challenging due to the imbalanced nature of the problem. Common approaches include:

- **Precision, Recall, and F1-Score**: Especially useful when true labels are available.
- **ROC AUC**: Area under the ROC curve, useful for scoring models that output anomaly scores.
- **Confusion Matrix**: To visualize true positives, false positives, etc.

.. _outlier_detection_references:

References
==========

- Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). *Isolation forest*. In 2008 Eighth IEEE International Conference on Data Mining (pp. 413-422). IEEE.
- Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000). *LOF: identifying density-based local outliers*. ACM Sigmod Record, 29(2), 93-104.
- Sch√∂lkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., & Williamson, R. C. (2001). *Estimating the support of a high-dimensional distribution*. Neural computation, 13(7), 1443-1471.

For more information, refer to the individual estimator documentation and examples in the :ref:`user guide <outlier_detection>`.