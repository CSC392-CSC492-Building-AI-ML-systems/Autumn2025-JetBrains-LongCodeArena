Partial Dependence and Individual Conditional Expectation
=========================================================

.. currentmodule:: sklearn.inspection

The :mod:`sklearn.inspection` module provides tools to help understand the predictions of a model. Two such tools are **Partial Dependence** and **Individual Conditional Expectation (ICE)** plots. These methods show how the predictions of a model change as a function of one or more input features, while averaging out the effects of all other features.

Partial Dependence
------------------

**Partial dependence** plots (PDP) display the relationship between the target response and a set of "target" features, while marginalizing over the values of all other features (the "complement" features). Intuitively, they show how the model's predictions would change on average if we varied the values of the target features.

Mathematically, for a feature of interest :math:`x_s`, the partial dependence is defined as:

.. math::

    \text{PD}(x_s) = \mathbb{E}_{X_C}[\text{f}(x_s, X_C)]

where :math:`f` is the prediction function, :math:`X_C` represents the complement features, and the expectation is taken over the marginal distribution of :math:`X_C`.

Partial dependence plots are useful for understanding:

- The general relationship between a feature and the model's predictions.
- Whether the relationship is linear, monotonic, or more complex.
- Interactions between features (when using 2-way PDPs).

Individual Conditional Expectation (ICE)
----------------------------------------

**Individual Conditional Expectation (ICE)** plots refine the analysis by showing the dependence of the prediction on a feature for each individual sample. Instead of averaging over all samples (as in PDP), ICE plots display one line per sample.

For a feature of interest :math:`x_s`, the ICE curve for the :math:`i`-th sample is defined as:

.. math::

    \text{ICE}_{(i)}(x_s) = \text{f}(x_s, x_{C}^{(i)})

where :math:`x_{C}^{(i)}` is the value of the complement features for the :math:`i`-th sample.

ICE plots are useful for:

- Detecting heterogeneous effects (i.e., when the effect of a feature varies across samples).
- Identifying subgroups of samples with different behaviors.
- Visualizing interactions and clusters in feature effects.

Usage in scikit-learn
---------------------

In scikit-learn, both Partial Dependence and ICE plots can be computed using the :func:`partial_dependence` function, and visualized using :func:`plot_partial_dependence` or :func:`PartialDependenceDisplay`.

Example
~~~~~~~

.. code-block:: python

    from sklearn.datasets import make_friedman1
    from sklearn.ensemble import GradientBoostingRegressor
    from sklearn.inspection import PartialDependenceDisplay

    X, y = make_friedman1(n_samples=1000)
    clf = GradientBoostingRegressor(n_estimators=100).fit(X, y)

    # Plot partial dependence for the first two features
    PartialDependenceDisplay.from_estimator(clf, X, [0, 1])
    plt.show()

In this example, the partial dependence of the model's predictions on features 0 and 1 is visualized. The shaded area represents the ICE curves, and the solid line represents the average (the PDP).

See Also
~~~~~~~~

- :func:`partial_dependence`: Compute partial dependence values.
- :class:`PartialDependenceDisplay`: Visualization of partial dependence.
- :func:`permutation_importance`: Another method for model inspection based on feature permutation.