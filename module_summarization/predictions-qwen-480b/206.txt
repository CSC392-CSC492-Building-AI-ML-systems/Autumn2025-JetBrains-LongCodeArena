Support Vector Machines
=======================

.. currentmodule:: sklearn.svm

Support Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outlier detection.

The advantages of support vector machines are:

- Effective in high dimensional spaces.
- Still effective in cases where the number of dimensions is greater than the number of samples.
- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
- Versatile: different kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.

The disadvantages of support vector machines include:

- If the number of features is much greater than the number of samples, avoid over-fitting in choosing kernel functions and regularization term is crucial.
- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).

The support vector machines in scikit-learn support both dense (``numpy.ndarray`` and convertible to that by ``numpy.asarray``) and sparse (any ``scipy.sparse``) data. However, note that sparse input will be copied even if ``copy_X`` is set to ``False`` in ``fit``.

.. _svm_classification:

Classification
--------------

:class:`SVC`, :class:`NuSVC` and :class:`LinearSVC` are classes capable of performing multi-class classification on a dataset.

.. figure:: ../auto_examples/svm/images/sphx_glr_plot_iris_svc_001.png
   :target: ../auto_examples/svm/plot_iris_svc.html
   :align: center
   :scale: 75%

   SVM decision boundary for the iris dataset using a linear kernel

:class:`SVC` and :class:`NuSVC` are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section :ref:`svm_mathematical_formulation`). On the other hand, :class:`LinearSVC` is another implementation of Support Vector Classification for the case of a linear kernel. Note that :class:`LinearSVC` does not accept keyword ``kernel``, as this is handled by the class itself, and it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.

All of :class:`SVC`, :class:`NuSVC` and :class:`LinearSVC` take as input two arrays: an array ``X`` of shape ``(n_samples, n_features)`` holding the training samples, and an array ``y`` of class labels (strings or integers), of shape ``(n_samples)``::

    >>> from sklearn import svm
    >>> X = [[0, 0], [1, 1]]
    >>> y = [0, 1]
    >>> clf = svm.SVC()
    >>> clf.fit(X, y)
    SVC()

After being fitted, the model can then be used to predict new values::

    >>> clf.predict([[2., 2.]])
    array([1])

SVMs decision function (detailed in the :ref:`svm_mathematical_formulation`) depends on some subset of the training data, called the support vectors. Some properties of these support vectors are available as attributes:

    >>> # get support vectors
    >>> clf.support_vectors_
    array([[0., 0.],
           [1., 1.]])
    >>> # get indices of support vectors
    >>> clf.support_
    array([0, 1]...)
    >>> # get number of support vectors for each class
    >>> clf.n_support_
    array([1, 1]...)

.. _svm_regression:

Regression
----------

The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.

The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close enough to their target.

There are three different implementations of Support Vector Regression: :class:`SVR`, :class:`NuSVR` and :class:`LinearSVR`. :class:`LinearSVR` provides a faster implementation than :class:`SVR` but only considers the linear kernel, while :class:`NuSVR` implements a slightly different formulation than :class:`SVR` and :class:`LinearSVR`. See :ref:`svm_implementation_details` for further details.

As with classification classes, the fit method will take as argument vectors ``X``, ``y``, only that in this case ``y`` is expected to have floating point values instead of integer values::

    >>> from sklearn import svm
    >>> X = [[0, 0], [2, 2]]
    >>> y = [0.5, 2.5]
    >>> regr = svm.SVR()
    >>> regr.fit(X, y)
    SVR()
    >>> regr.predict([[1, 1]])
    array([1.5])

.. _svm_outlier_detection:

Density estimation, outlier detection
-------------------------------------

One-class SVM is used for novelty detection, that is, given a set of samples, it will detect the soft boundary of that set so as to classify new points as belonging to that set or not. The class that implements this is :class:`OneClassSVM`.

In this case, the fit method will take as input an array ``X`` and not a vector ``y``. This is because it is an unsupervised learning method::

    >>> from sklearn import svm
    >>> X = [[0], [0.44], [0.45], [0.46], [1]]
    >>> clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1)
    >>> clf.fit(X)
    OneClassSVM(gamma=0.1)
    >>> clf.predict([[0.45], [1.5]])
    array([ 1, -1])
    >>> clf.score_samples([[0.45], [1.5]])
    array([1.779..., 0.244...])

.. _svm_complexity:

Complexity
----------

Support Vector Machines are powerful tools, but their computational and memory requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by this libsvm-based implementation scales between :math:`O(n_{features} \times n_{samples}^2)` and :math:`O(n_{features} \times n_{samples}^3)` depending on the implementation.

For the linear case, the complexity is approximately :math:`O(n_{features} \times n_{samples} \times n_{classes})` for the underlying liblinear implementation.

If you are running into memory issues or need to speed up training, consider using :class:`SGDClassifier` or :class:`sklearn.linear_model.LogisticRegression` instead, possibly after a :class:`sklearn.kernel_approximation.Nystroem` transformer.

.. _svm_kernel_ridge:

Kernel ridge regression
-----------------------

Kernel ridge regression (KRR) :ref:`sklearn.kernel_ridge.KernelRidge` combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.

Both KRR and SVC rely on the kernel trick to embed their input data into a higher-dimensional space. They differ in how they achieve regularization: SVC uses the epsilon-insensitive loss, while KRR uses the squared loss. In contrast to SVC, KRR learns a non-parametric model and typically has a fit time complexity that is better than SVC at the cost of a worse prediction time complexity. For equal prediction performance, KRR and SVC are expected to have similar prediction performance.

.. _svm_parameters:

Parameters selection, validation, and tuning
--------------------------------------------

The strength of the regularization is inversely proportional to the parameter ``C``. Large values of ``C`` correspond to less regularization. For an intuitive visualization of the effect of different values of ``C`` on decision functions, see :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`.

The parameter ``gamma`` defines how much influence a single training example has. The larger ``gamma`` is, the closer other examples must be to be affected. Proper choice of ``C`` and ``gamma`` is critical to the SVMâ€™s performance. It is usually selected by grid search with cross-validation.

.. _svm_mathematical_formulation:

Mathematical formulation
------------------------

A support vector machine constructs a hyper-plane or set of hyper-planes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training-data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.

.. image:: ../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_001.png