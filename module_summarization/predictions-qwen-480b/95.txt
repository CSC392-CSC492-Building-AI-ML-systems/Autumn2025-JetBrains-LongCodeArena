# Schedulers

Stateless learning rate schedulers for Composer.

Stateless schedulers solve some of the problems associated with PyTorch's built-in schedulers. The primary design goal of the schedulers provided in this module is to allow schedulers to interface directly with Composer's time abstraction. This means that schedulers can be configured using arbitrary but explicit time units.

## ComposerScheduler

```python
class composer.schedulers.ComposerScheduler(Protocol)
```

Specification for a stateless scheduler function.

While this specification is provided as a Python class, an ordinary function can implement this interface as long as it matches the signature of this interface's `__call__` method.

For example, a scheduler that halves the learning rate after 10 epochs could be written as:

```python
def ten_epoch_decay_scheduler(state: State) -> float:
    if state.timestamp.epoch < 10:
        return 1.0
    return 0.5

# ten_epoch_decay_scheduler is a valid ComposerScheduler
trainer = Trainer(
    schedulers=[ten_epoch_decay_scheduler],
   ...
)
```

In order to allow schedulers to be configured, schedulers may also written as callable classes:

```python
class VariableEpochDecayScheduler(ComposerScheduler):

    def __init__(num_epochs: int):
        self.num_epochs = num_epochs

    def __call__(state: State) -> float:
        if state.time.epoch < self.num_epochs:
            return 1.0
        return 0.5

ten_epoch_decay_scheduler = VariableEpochDecayScheduler(num_epochs=10)
# ten_epoch_decay_scheduler is also a valid ComposerScheduler
trainer = Trainer(
    schedulers=[ten_epoch_decay_scheduler],
   ...
)
```

The constructions of `ten_epoch_decay_scheduler` in each of the examples above are equivalent. Note that neither scheduler uses the `scale_schedule_ratio` parameter. As long as this parameter is not used when initializing `Trainer`, it is not required that any schedulers implement that parameter.

### Methods

#### `__call__(self, state: State, ssr: float = 1.0) -> float`

Calculate the current learning rate multiplier α.

A scheduler function should be a pure function that returns a multiplier to apply to the optimizer's provided learning rate, given the current trainer state, and optionally a "scale schedule ratio" (SSR). A typical implementation will read `state.timestamp`, and possibly other fields like `state.max_duration`, to determine the trainer's latest temporal progress.

**Note:** All instances of `ComposerScheduler` output a `multiplier` for the learning rate, rather than the learning rate directly. By convention, we use the symbol α to refer to this multiplier. This means that the learning rate η at time t can be represented as η(t) = η_i × α(t), where η_i represents the learning rate used to initialize the optimizer.

**Note:** It is possible to use multiple schedulers, in which case their effects will stack multiplicatively.

The `ssr` param indicates that the schedule should be "stretched" accordingly. In symbolic terms, where α_σ(t) represents the scheduler output at time t using scale schedule ratio σ:

α_σ(t) = α(t / σ)

**Parameters:**
- `state` (State): The current Composer Trainer state.
- `ssr` (float): The scale schedule ratio. In general, the learning rate computed by this scheduler at time t with an SSR of 1.0 should be the same as that computed by this scheduler at time t × s with an SSR of s. Default = `1.0`.

**Returns:**
- `alpha` (float): A multiplier to apply to the optimizer's provided learning rate.

## Functions

### `compile_composer_scheduler(scheduler: ComposerScheduler, state: State, ssr: float = 1.0) -> PyTorchScheduler`

Converts a stateless scheduler into a PyTorch scheduler object.

While the resulting scheduler provides a `.step()` interface similar to other PyTorch schedulers, the scheduler is also given a bound reference to the current `State`. This means that any internal state updated by `.step()` can be ignored.

## Available Schedulers

### `StepScheduler`

```python
composer.schedulers.StepScheduler(step_size: Union[str, Time[int], Time[float]], gamma: float = 0.1, scale_schedule_ratio: float = 1.0)
```

Decays the learning rate by a multiplicative factor (`gamma`) every `step_size` units of time.

### `MultiStepScheduler`

```python
composer.schedulers.MultiStepScheduler(milestones: List[Union[str, Time[int], Time[float]]], gamma: float = 0.1, scale_schedule_ratio: float = 1.0)
```

Decays the learning rate by a multiplicative factor (`gamma`) at a sequence of specified time milestones.

### `ConstantScheduler`

```python
composer.schedulers.ConstantScheduler(alpha: float = 1.0)
```

Maintains a constant learning rate multiplier.

### `LinearScheduler`

```python
composer.schedulers.LinearScheduler(start_factor: float = 1.0, end_factor: float = 0.0, total_iters: Union[str, Time[int], Time[float]] = '1dur', scale_schedule_ratio: float = 1.0)
```

Linearly adjusts the learning rate multiplier from `start_factor` to `end_factor` over `total_iters` units of time.

### `ExponentialScheduler`

```python
composer.schedulers.ExponentialScheduler(gamma: float, scale_schedule_ratio: float = 1.0)
```

Decays the learning rate exponentially by a multiplicative factor `gamma` each unit of time.

### `CosineAnnealingScheduler`

```python
composer.schedulers.CosineAnnealingScheduler(T_max: Union[str, Time[int], Time[float]], eta_min: float = 0.0, scale_schedule_ratio: float = 1.0)
```

Decays the learning rate according to the cosine annealing formula.

### `CosineAnnealingWarmRestartsScheduler`

```python
composer.schedulers.CosineAnnealingWarmRestartsScheduler(T_0: Union[str, Time[int], Time[float]], T_mult: float = 1.0, eta_min: float = 0.0, scale_schedule_ratio: float = 1.0)
```

Cosine annealing with warm restarts.

### `PolynomialScheduler`

```python
composer.schedulers.PolynomialScheduler(power: float = 1.0, total_iters: Union[str, Time[int], Time[float]] = '1dur', zero_division: str = 'warn', scale_schedule_ratio: float = 1.0)
```

Decays the learning rate according to a polynomial function.

### `MultiStepWithWarmupScheduler`

```python
composer.schedulers.MultiStepWithWarmupScheduler(milestones: List[Union[str, Time[int], Time[float]]], gamma: float = 0.1, warmup: Union[str, Time[int], Time[float]] = '0ep', scale_schedule_ratio: float = 1.0)
```

Combines multi-step decay with an initial warmup period.

### `ConstantWithWarmupScheduler`

```python
composer.schedulers.ConstantWithWarmupScheduler(alpha: float = 1.0, warmup: Union[str, Time[int], Time[float]] = '0ep', scale_schedule_ratio: float = 1.0)
```

Maintains a constant learning rate multiplier with an initial warmup period.

### `LinearWithWarmupScheduler`

```python
composer.schedulers.LinearWithWarmupScheduler(warmup: Union[str, Time[int], Time[float]], alpha: float = 1.0, scale_schedule_ratio: float = 1.0)
```

Linearly increases the learning rate multiplier from 0 to `alpha` during warmup, then maintains `alpha`.

### `CosineAnnealingWithWarmupScheduler`

```python
composer.schedulers.CosineAnnealingWithWarmupScheduler(warmup: Union[str, Time[int], Time[float]], T_max: Union[str, Time[int], Time[float]] = '1dur', eta_min: float = 0.0, scale_schedule_ratio: float = 1.0)
```

Cosine annealing with an initial warmup period.

### `PolynomialWithWarmupScheduler`

```python
composer.schedulers.PolynomialWithWarmupScheduler(warmup: Union[str, Time[int], Time[float]], power: float = 1.0, total_iters: Union[str, Time[int], Time[float]] = '1dur', zero_division: str = 'warn', scale_schedule_ratio: float = 1.0)
```

Polynomial decay with an initial warmup period.