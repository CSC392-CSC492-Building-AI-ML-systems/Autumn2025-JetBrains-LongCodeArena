Connect to Remote Data
======================

Dask provides utilities to connect to and read data from remote storage systems such as cloud storage (e.g., S3, GCS), HDFS, and other file systems supported by `fsspec`. This allows you to work with large datasets stored remotely as if they were local files.

Reading Remote Files with ``read_bytes``
----------------------------------------

The ``read_bytes`` function is a low-level utility for reading bytes from files, including remote files. It supports chunked reading, delimiter-based splitting, and integration with various file systems.

.. code-block:: python

    from dask.bytes import read_bytes

    # Read a single remote file
    sample, blocks = read_bytes('s3://my-bucket/my-file.csv')

    # Read multiple files using a glob pattern
    sample, blocks = read_bytes('s3://my-bucket/*.csv')

    # Include file paths in the output
    sample, blocks, paths = read_bytes('s3://my-bucket/*.csv', include_path=True)

Parameters
~~~~~~~~~~

- ``urlpath`` : string or list
    Path or paths to files. Prefix with a protocol like ``s3://`` or ``hdfs://`` to specify the file system.
- ``delimiter`` : bytes, optional
    Delimiter to split blocks on, e.g., ``b'\\n'`` for line-based splitting.
- ``not_zero`` : bool, optional
    If True, skip the first delimiter to discard headers.
- ``blocksize`` : int or str, optional
    Size of chunks to read. Can be an integer (bytes) or a string like ``"128 MiB"``. Set to ``None`` for whole-file chunks.
- ``sample`` : int, str, or bool, optional
    Size of the sample header to return. Set to ``False`` for no sample.
- ``compression`` : str, optional
    Compression format (e.g., ``'gzip'``, ``'xz'``). If ``'infer'``, it is inferred from the file extension.
- ``include_path`` : bool, optional
    If True, include file paths alongside the data blocks.
- ``**kwargs`` : dict
    Additional options for the file system (e.g., ``host``, ``port``, ``username``, ``password``).

Returns
~~~~~~~

- ``sample`` : bytes
    A sample of the file header.
- ``blocks`` : list of lists of ``dask.Delayed``
    Each list corresponds to a file, and each delayed object computes to a block of bytes.
- ``paths`` : list of strings (if ``include_path=True``)
    Paths corresponding to each file in ``blocks``.

Examples
~~~~~~~~

.. code-block:: python

    # Read CSV files from S3 in chunks
    sample, blocks = read_bytes('s3://my-bucket/data/*.csv', delimiter=b'\\n')

    # Read a compressed file without chunking
    sample, blocks = read_bytes('s3://my-bucket/data.csv.gz', blocksize=None)

    # Read files and include their paths
    sample, blocks, paths = read_bytes('s3://my-bucket/data/*.csv', include_path=True)

Supported File Systems
----------------------

Dask leverages `fsspec` to support a wide range of file systems:

- Local file system (default)
- Amazon S3 (``s3://``)
- Google Cloud Storage (``gcs://`` or ``gs://``)
- HDFS (``hdfs://``)
- HTTP(S) (``http://`` or ``https://``)
- And more (e.g., Azure Blob Storage, FTP)

To use a specific file system, ensure the required libraries are installed (e.g., ``s3fs`` for S3, ``gcsfs`` for GCS).

Compression Handling
--------------------

Dask automatically handles compressed files when ``blocksize=None``. For chunked reading, compressed files are not supported due to the nature of compression algorithms. In such cases, set ``blocksize=None`` to read the entire file.

.. code-block:: python

    # Read a gzipped file
    sample, blocks = read_bytes('s3://my-bucket/data.csv.gz', blocksize=None)

Notes
-----

- For remote data, ensure proper credentials and permissions are configured for the respective file system.
- Use ``dask.delayed`` to process the blocks lazily and in parallel.
- For structured data (e.g., CSV, Parquet), consider using higher-level functions like ``dd.read_csv`` or ``dd.read_parquet`` which build on these utilities.