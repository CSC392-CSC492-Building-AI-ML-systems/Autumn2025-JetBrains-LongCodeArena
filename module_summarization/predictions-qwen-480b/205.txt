Stochastic Gradient Descent
===========================

Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to
fitting linear classifiers and regressors under convex loss functions such as
(hinge loss for linear SVM, logistic loss for logistic regression, etc.).
Even though SGD has been around for a long time, it has received a considerable
amount of attention recently in machine learning because of its efficiency and
ease of implementation.

SGD has been successfully applied to large-scale and sparse machine learning
problems often encountered in text classification and natural language
processing. The advantages of SGD are:

- Efficiency.
- Ease of implementation (lots of opportunities for code tuning).

The disadvantages of SGD include:

- SGD requires a number of hyperparameters such as the regularization
  parameter and the number of iterations.
- SGD is sensitive to feature scaling.

SGD Algorithm
-------------

SGD is an iterative method for optimizing an objective function that has a
convex loss function. At each iteration, the gradient of the loss is estimated
by computing the gradient on a single sample (or a small batch of samples),
and the model parameters are updated in the opposite direction of the gradient.

The update rule for SGD is:

.. math::

    w \leftarrow w - \eta \left[ \nabla_w L(w; x_i, y_i) + \alpha \nabla_w R(w) \right]

Where:
- :math:`w` are the model parameters.
- :math:`\eta` is the learning rate.
- :math:`L(w; x_i, y_i)` is the loss function for a single sample :math:`(x_i, y_i)`.
- :math:`R(w)` is the regularization term.
- :math:`\alpha` is the regularization strength.

Loss Functions
--------------

The following loss functions are supported by the SGD implementation:

- **Squared Loss**: Used for regression tasks.
- **Log Loss**: Used for logistic regression.
- **Hinge Loss**: Used for linear SVM.
- **Epsilon-Insensitive Loss**: Used for regression tasks with tolerance.

Multinomial Logistic Loss
-------------------------

For multi-class classification, the multinomial logistic loss (also known as
softmax loss) is used. The loss for a single sample is defined as:

.. math::

    \text{loss} = - \sum_{c} \delta_{y,c} \left( \text{prediction}[c] - \log \sum_{k} \exp(\text{prediction}[k]) \right)

Where:
- :math:`\text{prediction}[c]` is the predicted score for class :math:`c`.
- :math:`\delta_{y,c}` is 1 if :math:`y = c`, else 0.
- :math:`y` is the true class label.

The gradient of the multinomial logistic loss with respect to the prediction
for class :math:`c` is:

.. math::

    \frac{\partial \text{loss}}{\partial \text{prediction}[c]} = p[c] - \delta_{y,c}

Where:
- :math:`p[c] = \frac{\exp(\text{prediction}[c])}{\sum_{k} \exp(\text{prediction}[k])}` is the predicted probability for class :math:`c`.

Regularization
--------------

SGD supports the following regularization strategies:

- **L2 Regularization**: Adds a penalty equal to the square of the magnitude of coefficients.
- **L1 Regularization**: Adds a penalty equal to the absolute value of the magnitude of coefficients.
- **Elastic Net Regularization**: Combines L1 and L2 penalties.

Implementation Details
----------------------

The implementation uses fused types to support both 32-bit and 64-bit floating-point
numbers, ensuring numerical stability and performance across different platforms.

The implementation includes:

- Efficient computation of loss functions and their gradients.
- Support for both dense and sparse data.
- Numerically stable computation of the log-sum-exp function for multinomial logistic regression.
- Soft thresholding for L1 regularization.

References
----------

- Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent.
  In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD.
- Bishop, C. M. (2006). Pattern recognition and machine learning. Springer. (Chapter 4.3.4)