.. _calibration_tutorial:

==========================
Probability Calibration Tutorial
==========================

This tutorial provides an overview of probability calibration in machine learning, focusing on the use of the :class:`sklearn.calibration.CalibratedClassifierCV` class to improve the reliability of predicted probabilities from classifiers.

Introduction to Probability Calibration
=======================================

In many machine learning applications, it is not sufficient to simply predict class labels; we also need reliable estimates of the associated probabilities. These probabilities are used in various downstream tasks such as:

- Risk assessment
- Decision-making under uncertainty
- Ranking and thresholding
- Cost-sensitive learning

However, many classifiers produce probability estimates that are not well-calibrated. A classifier is said to be well-calibrated if the predicted probabilities match the true frequencies of the outcomes. For example, if a classifier assigns a probability of 0.8 to 100 samples, approximately 80 of those samples should actually belong to the positive class.

Methods of Calibration
======================

Two main calibration methods are commonly used:

1. **Platt Scaling (Sigmoid Calibration)**: Fits a logistic regression model to the classifier's scores. This is suitable for small datasets and assumes a sigmoid-shaped calibration curve.

2. **Isotonic Regression**: Fits a non-parametric isotonic function to the scores. This method is more flexible but requires more calibration data to avoid overfitting.

Using CalibratedClassifierCV
============================

The :class:`CalibratedClassifierCV` class provides a convenient way to calibrate classifiers using cross-validation. Here's how to use it:

Basic Usage
-----------

.. code-block:: python

    from sklearn.calibration import CalibratedClassifierCV
    from sklearn.svm import LinearSVC
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split

    # Generate sample data
    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Create and calibrate classifier
    clf = LinearSVC()
    calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=5)
    calibrated_clf.fit(X_train, y_train)

    # Get calibrated probabilities
    probabilities = calibrated_clf.predict_proba(X_test)

Parameters Explained
--------------------

- **estimator**: The base classifier to be calibrated (default: LinearSVC)
- **method**: Calibration method ('sigmoid' or 'isotonic')
- **cv**: Cross-validation strategy (integer, CV splitter, or "prefit")
- **ensemble**: Whether to use ensemble of calibrated classifiers
- **n_jobs**: Number of parallel jobs for fitting

Cross-Validation Strategies
---------------------------

The `cv` parameter determines how calibration is performed:

- **Integer**: Specifies the number of cross-validation folds
- **None**: Uses 5-fold cross-validation by default
- **"prefit"**: Calibrates an already fitted estimator
- **CV splitter**: Custom cross-validation object

Ensemble vs Non-ensemble Mode
-----------------------------

When `ensemble=True` (default):
    - For each CV fold, fits the estimator on training data and calibrates on test data
    - Final predictions are averaged across all calibrated classifiers

When `ensemble=False`:
    - Uses cross-validation to get unbiased predictions for calibration
    - Final classifier is trained on all data

Prefit Mode
-----------

When `cv="prefit"`:
    - Assumes the estimator is already fitted
    - All provided data is used for calibration
    - User must ensure training and calibration data are disjoint

Best Practices
==============

1. **Choose appropriate calibration method**:
   - Use 'sigmoid' for small datasets or when assuming parametric form
   - Use 'isotonic' for larger datasets with complex calibration curves

2. **Ensure sufficient calibration data**:
   - Isotonic calibration requires substantial data (>>1000 samples)
   - Small datasets may benefit from simpler calibration methods

3. **Validate calibration quality**:
   - Use calibration plots to visualize reliability
   - Check calibration on held-out test data

4. **Consider computational cost**:
   - Ensemble mode increases prediction time
   - Parallel processing can be enabled with n_jobs parameter

Example: Comparing Calibration Methods
=====================================

.. code-block:: python

    import matplotlib.pyplot as plt
    from sklearn.calibration import CalibratedClassifierCV, calibration_curve
    from sklearn.svm import LinearSVC
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split

    # Generate data
    X, y = make_classification(n_samples=10000, n_features=20, n_classes=2, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Base classifier
    svc = LinearSVC()
    
    # Calibrate with sigmoid
    cal_sigmoid = CalibratedClassifierCV(svc, method='sigmoid', cv=5)
    cal_sigmoid.fit(X_train, y_train)
    
    # Calibrate with isotonic
    cal_isotonic = CalibratedClassifierCV(svc, method='isotonic', cv=5)
    cal_isotonic.fit(X_train, y_train)

    # Plot calibration curves
    fig, ax = plt.subplots(figsize=(8, 6))
    
    # Base classifier
    svc.fit(X_train, y_train)
    prob_svc = svc.decision_function(X_test)
    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_svc, n_bins=10)
    ax.plot(mean_predicted_value, fraction_of_positives, "s-", label="SVC")
    
    # Sigmoid calibration
    prob_sigmoid = cal_sigmoid.predict_proba(X_test)[:, 1]
    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_sigmoid, n_bins=10)
    ax.plot(mean_predicted_value, fraction_of_positives, "s-", label="Sigmoid")
    
    # Isotonic calibration
    prob_isotonic = cal_isotonic.predict_proba(X_test)[:, 1]
    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_isotonic, n_bins=10)
    ax.plot(mean_predicted_value, fraction_of_positives, "s-", label="Isotonic")
    
    ax.set_xlabel("Mean Predicted Probability")
    ax.set_ylabel("Fraction of Positives")
    ax.legend()
    plt.show()

Conclusion
==========

Probability calibration is essential for obtaining reliable probability estimates from classifiers. The :class:`CalibratedClassifierCV` provides a robust framework for calibration using cross-validation, supporting both parametric and non-parametric calibration methods. Proper calibration improves decision-making and risk assessment in practical applications.