Cross-validation: evaluating estimator performance
==================================================

.. currentmodule:: sklearn.model_selection

Learning Curve visualization
----------------------------

.. autoclass:: LearningCurveDisplay
   :members:

.. autofunction:: learning_curve

Validation Curve visualization
------------------------------

.. autofunction:: validation_curve

Evaluating estimator performance is a critical aspect of machine learning model development. Cross-validation provides a robust approach to assess how well a model generalizes to unseen data by partitioning the dataset into multiple training and testing subsets.

Learning curves are particularly useful for diagnosing whether a model suffers from high bias (underfitting) or high variance (overfitting). By plotting the model's performance on both training and validation sets as a function of the training set size, we can gain insights into the model's behavior:

- If both training and validation scores converge to a low value, the model likely suffers from high bias
- If there's a large gap between training and validation scores, the model likely suffers from high variance
- If scores improve with more data, collecting additional training samples may be beneficial

The :class:`LearningCurveDisplay` class provides a convenient way to visualize learning curves. It is recommended to use the :meth:`LearningCurveDisplay.from_estimator` method to create instances, which automatically computes the learning curve and creates the visualization.

Validation curves help in understanding how model performance varies with respect to a specific hyperparameter. This is useful for hyperparameter tuning and understanding model sensitivity to parameter changes.

Both learning and validation curves support different display styles for showing uncertainty through standard deviation, including error bars and filled regions. The visualization automatically adapts the x-axis scale based on the data distribution to provide optimal readability.

These tools are essential for model selection, hyperparameter tuning, and diagnosing model performance issues in a principled manner.