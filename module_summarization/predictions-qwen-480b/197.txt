Naive Bayes
===========

.. currentmodule:: sklearn.naive_bayes

The :mod:`sklearn.naive_bayes` module implements Naive Bayes algorithms. These
are supervised learning methods based on applying Bayes' theorem with strong
(naive) feature independence assumptions.

Naive Bayes methods are a set of supervised learning algorithms based on
applying Bayes' theorem with the "naive" assumption of conditional independence
between every pair of features given the value of the class variable.

Bayes' theorem states the following relationship:

.. math::

    P(y \mid x_1, \ldots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}
                                     {P(x_1, \ldots, x_n)}

Using the naive conditional independence assumption that
:math:`P(x_i \mid y) = P(x_i \mid y)` for all :math:`i`, this relationship is
simplified to:

.. math::

    P(y \mid x_1, \ldots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)

Since :math:`P(x_1, \ldots, x_n)` is constant given the input, we can simply
compute:

.. math::

    \hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y)

In practice, the joint probability :math:`P(y \prod_{i=1}^{n} P(x_i \mid y)`
can be very small, leading to underflow. To avoid this, we compute the
log-probability instead:

.. math::

    \log P(y \mid x_1, \ldots, x_n) \propto \log P(y) + \sum_{i=1}^{n} \log P(x_i \mid y)

The different naive Bayes classifiers differ mainly in the assumptions they make
regarding the distribution of :math:`P(x_i \mid y)`.

In spite of their apparently over-simplified assumptions, naive Bayes classifiers
have worked quite well in many real-world situations, famously in document
classification and spam filtering. They require a small amount of training data
to estimate the necessary parameters.

Gaussian Naive Bayes
--------------------

:class:`GaussianNB` implements the Gaussian Naive Bayes algorithm for
classification. The likelihood of the features is assumed to be Gaussian:

.. math::

    P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}}
                    \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)

The parameters :math:`\sigma_y` and :math:`\mu_y` are estimated using maximum
likelihood.

>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.naive_bayes import GaussianNB
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
>>> gnb = GaussianNB()
>>> y_pred = gnb.fit(X_train, y_train).predict(X_test)
>>> print("Number of mislabeled points out of a total %d points : %d"
...       % (X_test.shape[0], (y_test != y_pred).sum()))
Number of mislabeled points out of a total 75 points : 4

:class:`GaussianNB` supports online updates via the :meth:`partial_fit` method.

Multinomial Naive Bayes
-----------------------

:class:`MultinomialNB` implements the multinomial naive Bayes algorithm.
It is suitable for classification with discrete features (e.g., word counts for
text classification). The multinomial distribution normally requires integer
feature counts. However, in practice, fractional counts such as tf-idf may also
work.

>>> from sklearn.feature_extraction.text import CountVectorizer
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn.pipeline import Pipeline
>>> 
>>> # Sample data
>>> data = ["This is a positive example", "This is a negative example"]
>>> labels = [1, 0]
>>> 
>>> # Create pipeline
>>> pipeline = Pipeline([
...     ('vectorizer', CountVectorizer()),
...     ('classifier', MultinomialNB())
... ])
>>> 
>>> # Fit and predict
>>> pipeline.fit(data, labels)
Pipeline(steps=[('vectorizer', CountVectorizer()),
                ('classifier', MultinomialNB())])
>>> pipeline.predict(["This is another positive example"])
array([1])

Bernoulli Naive Bayes
---------------------

:class:`BernoulliNB` implements the Bernoulli naive Bayes algorithm.
It is suitable for discrete data with binary (and multiple) features, which are
modeled using a Bernoulli distribution.

>>> from sklearn.naive_bayes import BernoulliNB
>>> import numpy as np
>>> X = np.random.randint(2, size=(6, 100))
>>> Y = np.array([1, 2, 3, 4, 4, 5])
>>> clf = BernoulliNB()
>>> clf.fit(X, Y)
BernoulliNB()
>>> print(clf.predict(X[2:3]))
[3]

Complement Naive Bayes
----------------------

:class:`ComplementNB` implements the complement naive Bayes algorithm.
It was designed to correct the severe assumptions made by the standard
MultinomialNB regarding imbalanced datasets. It is particularly effective for
imbalanced data.

Categorical Naive Bayes
-----------------------

:class:`CategoricalNB` implements the categorical naive Bayes algorithm for
categorical features. It assumes that each feature, which is described by the
index :math:`i`, has its own categorical distribution.

Implementation Details
----------------------

All Naive Bayes classifiers in scikit-learn support both dense and sparse input
matrices. However, the sparse input is only supported when the underlying
algorithm allows it.

All Naive Bayes classifiers expose the following methods:

- :meth:`fit`: Fit the model according to the given training data.
- :meth:`predict`: Perform classification on an array of test vectors X.
- :meth:`predict_proba`: Return probability estimates for the test vector X.
- :meth:`predict_log_proba`: Return log-probability estimates for the test vector X.
- :meth:`partial_fit`: Incremental fit on a batch of samples (for online learning).

For numerical stability, the Naive Bayes classifiers use log-probabilities
internally.

References
----------

- H. Zhang (2004). `The optimality of Naive Bayes.
  <https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf>`_
  Proc. FLAIRS.