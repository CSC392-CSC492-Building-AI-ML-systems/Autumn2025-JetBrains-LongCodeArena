Neural Network Models (Supervised)
==================================

.. currentmodule:: sklearn.neural_network

Supervised learning with neural networks involves training models to learn a mapping from input features to output targets using labeled data. The models are composed of layers of interconnected nodes (neurons) that apply transformations to the input data through weighted connections and activation functions.

Activation Functions
---------------------

Activation functions introduce non-linear properties to the neural network, allowing it to learn complex patterns in the data. The following activation functions are supported:

- **Identity**: Linear activation function that leaves the input unchanged.
- **Logistic (Sigmoid)**: Maps input values to a range between 0 and 1, suitable for probability outputs.
- **Hyperbolic Tangent (Tanh)**: Maps input values to a range between -1 and 1, often used in hidden layers.
- **Rectified Linear Unit (ReLU)**: Outputs the input directly if it is positive, otherwise outputs zero. Commonly used in deep networks.
- **Softmax**: Converts raw scores to probabilities that sum to one, typically used in the output layer for multi-class classification.

Loss Functions
--------------

Loss functions measure how well the model's predictions match the true labels. Different loss functions are used depending on the type of problem:

- **Squared Loss**: Used for regression tasks, computes the mean squared difference between true and predicted values.
- **Log Loss (Cross-Entropy)**: Used for multi-class classification, measures the performance of a classification model whose output is a probability value between 0 and 1.
- **Binary Log Loss**: A specialized version of log loss for binary classification problems.

Training Process
----------------

The training of neural networks involves optimizing the weights of the connections to minimize the chosen loss function. This is typically done using gradient-based optimization algorithms such as stochastic gradient descent (SGD) or its variants like Adam or RMSprop.

During training, the forward pass computes the output of the network given an input, and the backward pass (backpropagation) computes gradients of the loss with respect to each weight by applying the chain rule. These gradients are then used to update the weights in the direction that reduces the loss.

Implementation Details
----------------------

The implementation includes utilities for efficient computation of activation functions and their derivatives, which are essential for backpropagation. It also provides numerically stable implementations of loss functions to ensure robust training even with extreme probability values.

These components form the foundation for building more complex architectures such as multi-layer perceptrons (MLP) for both classification and regression tasks.