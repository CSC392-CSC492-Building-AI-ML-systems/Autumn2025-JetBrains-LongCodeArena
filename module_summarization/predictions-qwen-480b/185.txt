Tuning the hyper-parameters of an estimator
==========================================

Hyper-parameter tuning is a crucial step in the machine learning pipeline. It involves selecting the best set of hyper-parameters for an estimator to optimize its performance on a given dataset. Hyper-parameters are parameters that are not learned from the data but are set prior to the training process. Examples include the regularization strength in logistic regression, the number of trees in a random forest, or the learning rate in gradient boosting.

Grid Search
-----------

Grid search is one of the most common methods for hyper-parameter tuning. It works by exhaustively searching through a specified subset of hyper-parameter values and evaluating the performance of the estimator for each combination. The combination that yields the best performance is selected as the optimal set of hyper-parameters.

In scikit-learn, grid search is implemented via the :class:`~sklearn.model_selection.GridSearchCV` class. This class performs an exhaustive search over a specified parameter grid and uses cross-validation to evaluate the performance of each combination. The best parameters are stored in the `best_params_` attribute, and the best estimator is available through the `best_estimator_` attribute.

Here is an example of how to use :class:`~sklearn.model_selection.GridSearchCV`:

.. code-block:: python

    from sklearn.model_selection import GridSearchCV
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split

    # Generate a synthetic dataset
    X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

    # Split the dataset into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define the model
    model = RandomForestClassifier(random_state=42)

    # Define the parameter grid
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    }

    # Perform grid search with cross-validation
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    # Print the best parameters and score
    print("Best parameters:", grid_search.best_params_)
    print("Best cross-validation score:", grid_search.best_score_)

In this example, the grid search evaluates all combinations of the specified hyper-parameters using 5-fold cross-validation. The best combination is selected based on the accuracy score.

Randomized Search
-----------------

While grid search is effective, it can be computationally expensive, especially when the parameter space is large. Randomized search offers a more efficient alternative by sampling a fixed number of candidate parameter settings from specified distributions. This approach can often find a good set of hyper-parameters with significantly fewer evaluations.

In scikit-learn, randomized search is implemented via the :class:`~sklearn.model_selection.RandomizedSearchCV` class. This class allows you to specify a parameter distribution instead of a grid, and it samples a fixed number of combinations to evaluate.

Here is an example of how to use :class:`~sklearn.model_selection.RandomizedSearchCV`:

.. code-block:: python

    from sklearn.model_selection import RandomizedSearchCV
    from scipy.stats import randint

    # Define the parameter distribution
    param_dist = {
        'n_estimators': randint(50, 200),
        'max_depth': [None, 10, 20],
        'min_samples_split': randint(2, 11)
    }

    # Perform randomized search with cross-validation
    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, 
                                       n_iter=10, cv=5, scoring='accuracy', random_state=42)
    random_search.fit(X_train, y_train)

    # Print the best parameters and score
    print("Best parameters:", random_search.best_params_)
    print("Best cross-validation score:", random_search.best_score_)

In this example, the randomized search evaluates 10 random combinations of hyper-parameters sampled from the specified distributions.

Conclusion
----------

Hyper-parameter tuning is an essential step in building effective machine learning models. Grid search and randomized search are two powerful techniques available in scikit-learn for this purpose. While grid search provides an exhaustive search over a specified grid, randomized search offers a more efficient alternative by sampling from parameter distributions. The choice between the two depends on the size of the parameter space and the computational resources available.