Optimization Module
===================

This module provides general-purpose optimization functions designed for model fitting. The functions are implemented to be independent of specific model classes, allowing for broad reuse across different statistical and mathematical modeling contexts.

The core functionality is encapsulated within the ``Optimizer`` class, which provides a flexible framework for fitting models using various optimization algorithms from the SciPy library.

Key Features
------------

- **Multiple Optimization Methods**: Supports various optimization algorithms including Newton-Raphson, Nelder-Mead, BFGS, Powell's method, conjugate gradient, and basin-hopping solvers.
- **Flexible Interface**: Provides a unified interface for different optimization methods while preserving access to method-specific parameters.
- **Comprehensive Output Options**: Offers detailed output information including convergence diagnostics, iteration history, and solver settings.
- **Method Validation**: Includes built-in validation to ensure only supported optimization methods are used.

Optimizer Class
---------------

The ``Optimizer`` class serves as the primary interface for performing optimization tasks.

.. class:: Optimizer

   .. method:: _fit(objective, gradient, start_params, fargs, kwargs, hessian=None, method='newton', maxiter=100, full_output=True, disp=True, callback=None, retall=False)

      Performs model fitting using the specified optimization method.

      **Parameters**

      - ``start_params`` (array_like, optional): Initial parameter estimates. Defaults to an array of zeros.
      - ``method`` (str): Optimization algorithm to use. Supported methods include:
        
        * ``'newton'``: Newton-Raphson method
        * ``'nm'``: Nelder-Mead simplex algorithm
        * ``'bfgs'``: Broyden-Fletcher-Goldfarb-Shanno algorithm
        * ``'powell'``: Modified Powell's method
        * ``'cg'``: Conjugate gradient method
        * ``'ncg'``: Newton-conjugate gradient method
        * ``'basinhopping'``: Global basin-hopping solver
        * ``'minimize'``: Wrapper for scipy.optimize.minimize
      - ``maxiter`` (int): Maximum number of iterations (ignored by basinhopping).
      - ``full_output`` (bool): Whether to return detailed output information.
      - ``disp`` (bool): Whether to display convergence messages.
      - ``fargs`` (tuple): Additional arguments passed to the objective function.
      - ``callback`` (callable): Function called after each iteration.
      - ``retall`` (bool): Whether to return solutions from all iterations.

      **Returns**

      - ``xopt`` (array): Optimal parameter values that minimize the objective function.
      - ``retvals`` (dict or None): Solver-specific information when ``full_output=True``.
      - ``optim_settings`` (dict): Dictionary containing the optimization parameters used.

      **Notes**

      Different optimization methods support various optional parameters:

      **Newton ('newton')**
         - ``tol``: Relative error tolerance for convergence.

      **Nelder-Mead ('nm')**
         - ``xtol``: Relative error tolerance in parameters.
         - ``ftol``: Relative error tolerance in function values.
         - ``maxfun``: Maximum function evaluations.

      **BFGS ('bfgs')**
         - ``gtol``: Gradient norm stopping criterion.
         - ``norm``: Norm order for gradient computation.
         - ``epsilon``: Step size for gradient approximation.

      **L-BFGS ('lbfgs')**
         - ``m``: Maximum variable metric corrections.
         - ``pgtol``: Projected gradient tolerance.
         - ``factr``: Function accuracy tolerance.
         - ``maxfun``: Maximum iterations.
         - ``epsilon``: Step size for numerical gradient.
         - ``approx_grad``: Whether to approximate gradient.

      **Conjugate Gradient ('cg')**
         - ``gtol``: Gradient norm stopping criterion.
         - ``norm``: Norm order for gradient computation.
         - ``epsilon``: Step size for gradient approximation.

      **Newton-CG ('ncg')**
         - ``fhess_p``: Hessian-vector product function.
         - ``avextol``: Average relative error tolerance.
         - ``epsilon``: Step size for Hessian approximation.

      **Powell ('powell')**
         - ``xtol``: Line-search error tolerance.
         - ``ftol``: Relative error tolerance in function values.
         - ``maxfun``: Maximum function evaluations.
         - ``start_direc``: Initial direction set.

      **Basin-hopping ('basinhopping')**
         - ``niter``: Number of basin hopping iterations.
         - ``niter_success``: Stopping criterion for convergence.
         - ``T``: Temperature parameter for acceptance criterion.
         - ``stepsize``: Initial step size for random displacement.
         - ``interval``: Step size update interval.
         - ``minimizer``: Keyword arguments for the local minimizer.

      The basin-hopping solver ignores the ``maxiter``, ``retall``, and ``full_output`` parameters.