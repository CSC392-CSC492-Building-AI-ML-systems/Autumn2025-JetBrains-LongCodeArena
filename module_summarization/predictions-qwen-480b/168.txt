Feature Extraction for Model Inspection
=======================================

Feature extraction is a powerful technique for inspecting intermediate representations within a neural network. This module provides utilities to extract features from specific layers or nodes of a model, enabling tasks such as visualization, debugging, and model analysis.

Overview
--------

The feature extraction functionality allows you to create a new model that returns intermediate activations alongside (or instead of) the final output. This is particularly useful for:

- Visualizing what a model has learned at different layers
- Debugging model behavior by inspecting intermediate outputs
- Using intermediate features for downstream tasks
- Comparing model behavior between training and evaluation modes

Key Components
--------------

The feature extraction module provides two main functions:

``create_feature_extractor(model, return_nodes=None, train_return_nodes=None, eval_return_nodes=None, tracer_kwargs=None)``
    Creates a feature extractor from a given model. It returns a new model that, when called, will return both the original output and the specified intermediate features.

``get_graph_node_names(model, tracer_kwargs=None)``
    Utility function to inspect and retrieve the names of all nodes in the model's computation graph. This helps identify which nodes can be used for feature extraction.

Node Naming Convention
----------------------

Each node in the computation graph is identified by a unique name. These names follow a hierarchical structure:

- Names are constructed by traversing the module hierarchy from the top-level down to individual operations
- The top-level module name is not included in the node name
- For example, if a module applies a ReLU operation, the node name will be simply ``'relu'``
- When duplicate names occur, they are disambiguated with a suffix like ``'_1'``, ``'_2'``, etc.

Tracing Behavior
----------------

The feature extractor uses PyTorch's FX (Functional Transformations) system to trace the model:

- The tracing process records the execution order of operations
- Leaf modules (atomic building blocks) can be specified to prevent tracing into their internals
- Different behaviors can be specified for training and evaluation modes
- The system warns when differences are detected between training and evaluation graphs

Training vs Evaluation Modes
----------------------------

Models often behave differently in training and evaluation modes (e.g., due to dropout or batch normalization). The feature extractor can handle these differences:

- Separate return nodes can be specified for training and evaluation modes
- When graph differences are detected, a warning is issued to alert the user
- Users are advised to specify output nodes separately for each mode when needed

Usage Example
-------------

Here's a basic example of how to use the feature extractor:

.. code-block:: python

    import torch
    import torchvision.models as models
    from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names

    # Load a pre-trained model
    model = models.resnet18(pretrained=True)

    # Inspect available node names
    train_nodes, eval_nodes = get_graph_node_names(model)

    # Define which nodes to extract features from
    return_nodes = {
        'layer1.0.relu': 'layer1_relu',
        'layer2.0.relu': 'layer2_relu',
        'layer3.0.relu': 'layer3_relu',
        'layer4.0.relu': 'layer4_relu'
    }

    # Create the feature extractor
    feature_extractor = create_feature_extractor(model, return_nodes=return_nodes)

    # Use the feature extractor
    input_tensor = torch.randn(1, 3, 224, 224)
    features = feature_extractor(input_tensor)

    # Access extracted features
    print(features['layer1_relu'].shape)
    print(features['layer2_relu'].shape)

Advanced Configuration
----------------------

Leaf Modules
~~~~~~~~~~~~

You can specify which modules should be treated as atomic units during tracing:

.. code-block:: python

    tracer_kwargs = {'leaf_modules': [torch.nn.ReLU]}
    feature_extractor = create_feature_extractor(
        model, 
        return_nodes=return_nodes,
        tracer_kwargs=tracer_kwargs
    )

This prevents the tracer from descending into the specified modules, treating them as single operations.

Limitations
-----------

- The feature extractor works best with models that have a static computation graph
- Dynamic control flow (e.g., loops with data-dependent conditions) may not be fully supported
- Some complex models with custom forward methods might require special handling
- The tracer may not capture all operations in models with extensive use of non-standard PyTorch functions

Best Practices
--------------

1. Always inspect available node names using ``get_graph_node_names`` before specifying return nodes
2. Be aware of differences between training and evaluation modes in your model
3. Use descriptive names for extracted features to make your code more readable
4. Consider computational overhead when extracting features from many nodes
5. Validate that extracted features have the expected shapes and properties for your use case