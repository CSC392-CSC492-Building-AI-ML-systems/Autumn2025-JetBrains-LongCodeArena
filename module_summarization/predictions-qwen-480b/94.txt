# Scale Schedule Documentation

## Overview

This module provides stateless learning rate schedulers designed to work seamlessly with Composer's time abstraction. Unlike PyTorch's built-in schedulers, these schedulers can be configured using arbitrary time units and integrate directly with Composer's training loop.

## Core Concepts

### ComposerScheduler Protocol

The `ComposerScheduler` protocol defines the interface for stateless schedulers. It's a callable that takes the current training state and returns a learning rate multiplier.

```python
def scheduler_function(state: State, ssr: float = 1.0) -> float:
    # Return learning rate multiplier
    pass
```

### Learning Rate Multipliers

All schedulers return a multiplier (α) rather than absolute learning rates. The actual learning rate is calculated as:
```
η(t) = η_initial × α(t)
```

### Scale Schedule Ratio (SSR)

The SSR parameter allows stretching or compressing the schedule timeline:
```
α_σ(t) = α(t / σ)
```

## Available Schedulers

### Basic Schedulers
- **ConstantScheduler**: Maintains a constant learning rate
- **LinearScheduler**: Linear decay from start to end value
- **ExponentialScheduler**: Exponential decay
- **PolynomialScheduler**: Polynomial decay with configurable power
- **StepScheduler**: Step decay at fixed intervals
- **MultiStepScheduler**: Step decay at multiple specified points

### Warmup Variants
- **ConstantWithWarmupScheduler**: Constant schedule with linear warmup
- **LinearWithWarmupScheduler**: Linear decay with linear warmup
- **CosineAnnealingWithWarmupScheduler**: Cosine annealing with linear warmup
- **PolynomialWithWarmupScheduler**: Polynomial decay with linear warmup
- **MultiStepWithWarmupScheduler**: Step decay with linear warmup

### Advanced Schedulers
- **CosineAnnealingScheduler**: Cosine annealing to zero
- **CosineAnnealingWarmRestartsScheduler**: Cosine annealing with warm restarts

## Usage Examples

### Simple Custom Scheduler
```python
def ten_epoch_decay_scheduler(state: State) -> float:
    if state.timestamp.epoch < 10:
        return 1.0
    return 0.5

trainer = Trainer(schedulers=[ten_epoch_decay_scheduler], ...)
```

### Configurable Scheduler Class
```python
class VariableEpochDecayScheduler(ComposerScheduler):
    def __init__(self, num_epochs: int):
        self.num_epochs = num_epochs

    def __call__(self, state: State) -> float:
        if state.timestamp.epoch < self.num_epochs:
            return 1.0
        return 0.5

scheduler = VariableEpochDecayScheduler(num_epochs=10)
trainer = Trainer(schedulers=[scheduler], ...)
```

## Time Conversion

The module includes utilities for converting time representations, handling duration units, and applying scale schedule ratios appropriately across different time granularities (epochs, batches, samples, tokens).

## Multiple Schedulers

Multiple schedulers can be used simultaneously, with their effects combining multiplicatively for fine-grained learning rate control.