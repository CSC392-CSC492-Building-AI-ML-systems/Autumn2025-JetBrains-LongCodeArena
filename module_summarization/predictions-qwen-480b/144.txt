Scheduling Policies
===================

Dask's distributed scheduler uses a variety of policies to determine the order in which tasks are executed and which workers should execute them. These policies are designed to optimize for performance, resource utilization, and task dependencies.

Task Selection Policies
-----------------------

The scheduler maintains tasks in different states and uses priority queues to determine which tasks should be executed next. The main task states include:

- **released**: Task has been released and its data may be garbage collected
- **waiting**: Task is waiting for dependencies to complete
- **no-worker**: Task is ready to run but no suitable worker is available
- **queued**: Task is queued for execution
- **processing**: Task is currently being executed
- **memory**: Task has completed and its result is in memory
- **erred**: Task has failed during execution
- **forgotten**: Task has been forgotten by the scheduler

Task Prioritization
~~~~~~~~~~~~~~~~~~~

Tasks are prioritized based on several factors:

1. **User-defined priorities**: Tasks can be assigned explicit priorities by users
2. **Graph-level priorities**: Based on the structure of the computation graph
3. **Data locality**: Tasks that can run on workers where their dependencies already exist
4. **Resource constraints**: Tasks requiring specific resources are scheduled accordingly

Worker Selection Policies
-------------------------

When selecting workers for task execution, the scheduler considers multiple factors:

Resource Awareness
~~~~~~~~~~~~~~~~~~

The scheduler tracks worker resources including:
- Available memory
- CPU utilization
- Network bandwidth
- Custom resources defined by users

Workers are selected based on their ability to satisfy task requirements and their current resource availability.

Data Locality
~~~~~~~~~~~~~

The scheduler attempts to schedule tasks on workers that already have the required input data in memory. This minimizes data transfer overhead and improves performance.

Load Balancing
~~~~~~~~~~~~~~

Work is distributed across workers to ensure balanced resource utilization. The scheduler considers:
- Current task queue lengths
- Worker processing capacity
- Memory pressure on individual workers

Work Stealing
~~~~~~~~~~~~~

The scheduler implements work stealing mechanisms to redistribute tasks from busy workers to idle ones, helping maintain balanced workload distribution across the cluster.

Task Placement Strategy
~~~~~~~~~~~~~~~~~~~~~~~

The placement strategy considers:
1. **First fit**: Place task on the first worker that can execute it
2. **Best fit**: Place task on the worker that best matches its resource requirements
3. **Round-robin**: Distribute tasks evenly across available workers
4. **Affinity-based**: Prefer workers with existing data dependencies

These policies work together to optimize the overall execution of distributed computations while respecting resource constraints and minimizing communication overhead.