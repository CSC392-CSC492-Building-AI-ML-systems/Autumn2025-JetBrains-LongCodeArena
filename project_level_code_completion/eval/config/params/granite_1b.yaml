preprocess_params:
  model: "huggingface"
  composers: "none"
  tokenizer: "ibm-granite/granite-4.0-h-1b-base"

inference_params:
  model: "granite_1b"      
  seq_max_len: 4096
  input_data_path: ""
  context_max: 128

eval_params:
  device: cuda
